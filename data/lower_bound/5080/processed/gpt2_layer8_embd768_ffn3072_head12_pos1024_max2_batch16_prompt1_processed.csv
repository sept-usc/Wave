Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum,flops_log,flops_threshold,is_matmul_candidate,is_attention_candidate,elementwise_add_fma_ops,role
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",350.5,99600.0,347088.0,12288.0,0.0,0.0,359376.0,359376.0,272.0,1088.0,0.2,147456.0,50176.0,5.0600000000000005,3122.6319999999955,96448.0,63728.0,93456.0,6144.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4608.0,1568.0,12.792127255735657,14.59154337685173,False,False,0,other
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),351.5,57249792.0,115015680.0,73728.0,0.0,0.0,115089408.0,115089408.0,204480.0,77198.625,0.7259361280333769,9100748.0,412660.0,24.612000000000002,3147.2439999999956,0.0,589824.0,57212928.0,36864.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,284398.375,12895.625,18.561219853819836,14.59154337685173,True,True,0,QKV
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",352.5,221184.0,0.0,442368.0,0.0,0.0,442368.0,442368.0,0.0,1536.0,0.0,98304.0,98304.0,3.66,3150.9039999999954,0.0,0.0,0.0,221184.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0,12.99989965440235,14.59154337685173,False,False,0,add
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",353.5,221184.0,0.0,442368.0,0.0,0.0,442368.0,442368.0,0.0,1536.0,0.0,98304.0,98304.0,3.724,3154.6279999999956,0.0,0.0,0.0,221184.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0,12.99989965440235,14.59154337685173,False,False,0,add
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",354.5,18432.0,0.0,36864.0,0.0,0.0,36864.0,36864.0,0.0,768.0,0.0,49152.0,49152.0,3.088,3157.7159999999953,0.0,0.0,0.0,18432.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,10.515017870423751,14.59154337685173,False,False,0,add
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",355.5,393216.0,13860864.0,0.0,0.0,0.0,13860864.0,13860864.0,102144.0,192.0,0.99812382739212,245760.0,49152.0,20.555999999999997,3178.2719999999954,10678272.0,2396160.0,393216.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7680.0,1536.0,16.444579959591028,14.59154337685173,True,True,0,Attention
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",356.5,9486336.0,19169280.0,98304.0,0.0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,15.459999999999999,3193.7319999999954,147456.0,147456.0,9437184.0,49152.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,313728.0,1536.0,16.77393470819845,14.59154337685173,True,False,0,Wo
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",357.5,12288.0,24576.0,0.0,0.0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.6479999999999997,3196.3799999999956,0.0,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0,10.109566325223746,14.59154337685173,False,False,12288,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",358.5,99600.0,347088.0,12288.0,0.0,0.0,359376.0,359376.0,272.0,1088.0,0.2,147456.0,50176.0,5.036,3201.4159999999956,96448.0,63728.0,93456.0,6144.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4608.0,1568.0,12.792127255735657,14.59154337685173,False,False,0,other
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),359.5,75515904.0,151289856.0,36864.0,0.0,0.0,151326720.0,151326720.0,240480.0,4608.0,0.981198589894242,10616832.0,589824.0,14.228,3215.6439999999957,0.0,294912.0,75497472.0,18432.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,331776.0,18432.0,18.83495177255031,14.59154337685173,True,True,0,FFN1
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",360.5,147456.0,245760.0,294912.0,0.0,0.0,540672.0,540672.0,0.0,10752.0,0.0,602112.0,196608.0,3.848,3219.4919999999956,196608.0,49152.0,0.0,147456.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,18816.0,6144.0,13.200569938854194,14.59154337685173,False,False,0,splitKreduce
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",361.5,0.0,49152.0,0.0,0.0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,2.7199999999999998,3222.211999999995,0.0,49152.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,6144.0,10.802693161352469,14.59154337685173,False,False,0,other
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",362.5,0.0,98304.0,0.0,0.0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,2.756,3224.9679999999953,0.0,98304.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,6144.0,11.495830169541591,14.59154337685173,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",363.5,0.0,49152.0,0.0,0.0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,2.724,3227.6919999999955,0.0,49152.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,6144.0,10.802693161352469,14.59154337685173,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",364.5,49152.0,98304.0,0.0,0.0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,2.968,3230.6599999999953,0.0,0.0,49152.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12288.0,6144.0,11.495830169541591,14.59154337685173,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",365.5,0.0,49152.0,0.0,0.0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,2.668,3233.3279999999954,0.0,49152.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,6144.0,10.802693161352469,14.59154337685173,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",366.5,291944.75,719057.5,12288.0,0.0,0.0,731345.5,731345.5,0.0,768.0,0.0,196608.0,196608.0,2.812,3236.139999999995,49152.0,98304.0,285800.75,6144.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,6144.0,13.502642634633448,14.59154337685173,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",367.5,49152.0,98304.0,0.0,0.0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,2.6719999999999997,3238.8119999999954,0.0,0.0,49152.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,6144.0,11.495830169541591,14.59154337685173,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",368.5,0.0,49152.0,0.0,0.0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,2.996,3241.8079999999954,0.0,49152.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12288.0,6144.0,10.802693161352469,14.59154337685173,False,False,0,other
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),369.5,76333056.0,153354240.0,98304.0,0.0,0.0,153452544.0,153452544.0,272640.0,329512.5,0.45280949774411205,12725808.0,714480.0,60.44,3302.247999999995,0.0,786432.0,76283904.0,49152.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,397681.5,22327.5,18.848901924099394,14.59154337685173,True,False,0,FFN2
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",370.5,12288.0,24576.0,0.0,0.0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.672,3304.919999999995,0.0,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0,10.109566325223746,14.59154337685173,False,False,12288,elementwise_add
