Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum,flops_log,flops_threshold,is_matmul_candidate,is_attention_candidate,elementwise_add_fma_ops,role
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",350.5,32324.0,109812.0,3072.0,0.0,0.0,112884.0,112884.0,68.0,360.0,0.1588785046728972,49152.0,16640.0,5.112,2910.5679999999984,31280.0,16956.0,30788.0,1536.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0,11.634124880372074,13.642930424418756,False,False,0,other
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",351.5,12632064.0,25509888.0,98304.0,0.0,0.0,25608192.0,25608192.0,112512.0,105216.0,0.5167548500881834,13381632.0,49152.0,21.54,2932.1079999999984,147456.0,196608.0,12582912.0,49152.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,418176.0,1536.0,17.058422897310713,13.642930424418756,True,False,0,QKV
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",352.5,188416.0,0.0,376832.0,0.0,0.0,376832.0,376832.0,0.0,512.0,0.0,32768.0,32768.0,3.692,2935.7999999999984,0.0,0.0,0.0,188416.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0,12.839557397467308,13.642930424418756,False,False,0,add
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",353.5,188416.0,0.0,376832.0,0.0,0.0,376832.0,376832.0,0.0,512.0,0.0,32768.0,32768.0,3.6799999999999997,2939.4799999999987,0.0,0.0,0.0,188416.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0,12.839557397467308,13.642930424418756,False,False,0,add
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",354.5,6144.0,0.0,12288.0,0.0,0.0,12288.0,12288.0,0.0,256.0,0.0,16384.0,16384.0,3.032,2942.5119999999984,0.0,0.0,0.0,6144.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,512.0,512.0,9.416459832284596,13.642930424418756,False,False,0,add
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",355.5,131072.0,4620288.0,0.0,0.0,0.0,4620288.0,4620288.0,34048.0,64.0,0.99812382739212,81920.0,16384.0,10.348,2952.859999999998,3559424.0,798720.0,131072.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2560.0,512.0,15.345967815214049,13.642930424418756,True,True,0,Attention
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),356.5,33570816.0,67371008.0,32768.0,0.0,0.0,67403776.0,67403776.0,115456.0,1024.0,0.9912087912087912,4325376.0,131072.0,8.64,2961.499999999998,0.0,262144.0,33554432.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,135168.0,4096.0,18.0262116128881,13.642930424418756,True,True,0,Wo
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",357.5,24576.0,40960.0,49152.0,0.0,0.0,90112.0,90112.0,0.0,1920.0,0.0,135168.0,16384.0,3.092,2964.5919999999983,36864.0,4096.0,0.0,24576.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4224.0,512.0,11.408819717317222,13.642930424418756,False,False,0,splitKreduce
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",358.5,4096.0,8192.0,0.0,0.0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.636,2967.2279999999982,0.0,0.0,4096.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0,9.011035410141815,13.642930424418756,False,False,4096,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",359.5,32324.0,109812.0,3072.0,0.0,0.0,112884.0,112884.0,68.0,360.0,0.1588785046728972,49152.0,16640.0,5.072,2972.2999999999984,31280.0,16956.0,30788.0,1536.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0,11.634124880372074,13.642930424418756,False,False,0,other
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",360.5,16842752.0,34013184.0,131072.0,0.0,0.0,34144256.0,34144256.0,150016.0,140288.0,0.5167548500881834,17842176.0,65536.0,23.671999999999997,2995.971999999998,196608.0,262144.0,16777216.0,65536.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,557568.0,2048.0,17.346104959999995,13.642930424418756,True,False,0,FFN1
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",361.5,0.0,16384.0,0.0,0.0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.636,2998.6079999999984,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,9.704121561132915,13.642930424418756,False,False,0,other
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",362.5,0.0,32768.0,0.0,0.0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.6559999999999997,3001.2639999999983,0.0,32768.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,10.397238225511654,13.642930424418756,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",363.5,0.0,16384.0,0.0,0.0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.588,3003.851999999998,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,9.704121561132915,13.642930424418756,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",364.5,16384.0,32768.0,0.0,0.0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.684,3006.5359999999982,0.0,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0,10.397238225511654,13.642930424418756,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",365.5,0.0,16384.0,0.0,0.0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.624,3009.1599999999985,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,9.704121561132915,13.642930424418756,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",366.5,96239.625,237535.25,4096.0,0.0,0.0,241631.25,241631.25,0.0,256.0,0.0,65536.0,65536.0,2.7199999999999998,3011.8799999999983,16384.0,32768.0,94191.625,2048.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,12.395172221233791,13.642930424418756,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",367.5,16384.0,32768.0,0.0,0.0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.6159999999999997,3014.4959999999983,0.0,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,10.397238225511654,13.642930424418756,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",368.5,0.0,16384.0,0.0,0.0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.6879999999999997,3017.1839999999984,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0,9.704121561132915,13.642930424418756,False,False,0,other
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",369.5,16818176.0,33964032.0,81920.0,0.0,0.0,34045952.0,34045952.0,148096.0,139520.0,0.5149087672452158,17829888.0,16384.0,69.85999999999999,3087.0439999999985,147456.0,262144.0,16777216.0,40960.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,557184.0,512.0,17.343221728870475,13.642930424418756,True,False,0,FFN2
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",370.5,4096.0,8192.0,0.0,0.0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.624,3089.667999999998,0.0,0.0,4096.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0,9.011035410141815,13.642930424418756,False,False,4096,elementwise_add
