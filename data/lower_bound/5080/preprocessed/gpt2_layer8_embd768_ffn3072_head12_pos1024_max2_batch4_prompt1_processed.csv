Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",1,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,1.696,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",2,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.568,3.2640000000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",3,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,1.0,0.0,0.0,32.0,1.632,4.896,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",4,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,3.0,0.0,64.0,32.0,2.112,7.008,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",5,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,2.688,9.696,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",6,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,2.624,12.32,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::unrolled_elementwise_kernel<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)], std::array<char *, 2>, 8, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<1>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",7,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,3.776,16.096,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::sum_functor<long, long, long>::operator ()(at::TensorIterator &)::[lambda(long, long) (instance 1)]>, unsigned int, long, 4, 4>>(T3)",8,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,3.328,19.424,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",9,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,2.496,21.919999999999998,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",10,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.08,24.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",11,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.048,26.048000000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void at_cuda_detail::DeviceScanInitKernel<at_cuda_detail::ScanTileState<long, 1>>(T1, int)",12,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.144,28.192,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanKernel<at_cuda_detail::DeviceScanPolicy<long, std::plus<long>>::Policy900, const long *, long *, at_cuda_detail::ScanTileState<long, 1>, std::plus<long>, at_cuda_detail::NullType, unsigned int, long, 0>(T2, T3, T4, int, T5, T6, T7)",13,128.0,0.0,256.0,0,0.0,256.0,256.0,129.0,6.0,0.9555555555555556,32.0,32.0,2.88,31.072,0.0,0.0,0.0,128.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",14,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.592,33.664,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",15,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,2.4,36.064,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",16,1152.0,0.0,2304.0,0,0.0,2304.0,2304.0,56.0,4.0,0.9333333333333333,32.0,32.0,3.008,39.072,0.0,0.0,0.0,1152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",17,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.496,41.568000000000005,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",18,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,2.0,0.0,32.0,32.0,2.496,44.06400000000001,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",19,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,3.0,0.0,64.0,0.0,2.56,46.62400000000001,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,0.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",20,3840.0,0.0,7680.0,0,0.0,7680.0,7680.0,0.0,288.0,0.0,3264.0,12288.0,4.32,50.94400000000001,0.0,0.0,0.0,3840.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,102.0,384.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",21,3840.0,0.0,7680.0,0,0.0,7680.0,7680.0,0.0,288.0,0.0,3264.0,12288.0,3.872,54.81600000000001,0.0,0.0,0.0,3840.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,102.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",22,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.656,57.47200000000001,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",23,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,2.0,0.0,32.0,32.0,2.432,59.90400000000001,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",24,12.0,0.0,24.0,0,0.0,24.0,24.0,0.0,2.0,0.0,32.0,32.0,3.392,63.296000000000014,0.0,0.0,0.0,12.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",25,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,4.992,68.28800000000001,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),26,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.552,83.84,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",27,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.296,87.13600000000001,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",28,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.04,90.17600000000002,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",29,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.04,93.21600000000002,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",30,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.04,96.25600000000003,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",31,98304.0,3462144.0,0.0,0,0.0,3462144.0,3462144.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,11.328,107.58400000000003,2666496.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",32,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,16.064,123.64800000000002,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",33,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,126.24000000000002,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",34,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.024,131.264,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",35,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,17.888,149.15200000000002,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",36,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.656,151.80800000000002,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",37,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.624,154.43200000000002,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",38,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,157.02400000000003,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",39,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.624,159.64800000000002,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",40,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.72,162.36800000000002,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",41,73004.0,179800.0,3072.0,0,0.0,182872.0,182872.0,0.0,192.0,0.0,49152.0,49152.0,2.656,165.02400000000003,12288.0,24576.0,71468.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",42,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.528,167.55200000000002,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",43,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.592,170.14400000000003,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),44,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.408,187.55200000000002,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",45,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.744,191.29600000000002,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",46,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,193.88800000000003,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",47,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,4.992,198.88000000000002,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),48,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,16.032,214.91200000000003,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",49,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.296,218.20800000000003,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",50,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.232,221.44000000000003,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",51,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.04,224.48000000000002,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",52,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.104,227.58400000000003,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",53,98304.0,3462144.0,0.0,0,0.0,3462144.0,3462144.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,10.336,237.92000000000004,2666496.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",54,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,16.128,254.04800000000006,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",55,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,256.64000000000004,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",56,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,4.992,261.63200000000006,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",57,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,16.8,278.4320000000001,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",58,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.56,280.9920000000001,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",59,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.528,283.5200000000001,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",60,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.624,286.1440000000001,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",61,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.624,288.76800000000014,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",62,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,291.3600000000001,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",63,72920.0,179632.0,3072.0,0,0.0,182704.0,182704.0,0.0,192.0,0.0,49152.0,49152.0,2.72,294.08000000000015,12288.0,24576.0,71384.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",64,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.592,296.67200000000014,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",65,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.592,299.2640000000001,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),66,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.216,316.48000000000013,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",67,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.68,320.16000000000014,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",68,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.656,322.81600000000014,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",69,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.056,327.8720000000001,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),70,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.552,343.42400000000015,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",71,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.2,346.62400000000014,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",72,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.008,349.6320000000001,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",73,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.008,352.6400000000001,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",74,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.136,355.7760000000001,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",75,98304.0,3462144.0,0.0,0,0.0,3462144.0,3462144.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,10.272,366.0480000000001,2666496.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",76,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,16.128,382.1760000000001,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",77,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,384.7680000000001,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",78,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.216,389.9840000000001,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",79,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,17.248,407.2320000000001,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",80,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,409.82400000000007,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",81,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.528,412.3520000000001,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",82,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.624,414.9760000000001,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",83,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.656,417.6320000000001,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",84,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,420.2240000000001,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",85,72912.0,179616.0,3072.0,0,0.0,182688.0,182688.0,0.0,192.0,0.0,49152.0,49152.0,2.688,422.9120000000001,12288.0,24576.0,71376.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",86,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.496,425.4080000000001,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",87,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.688,428.09600000000006,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),88,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.12,445.21600000000007,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",89,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.616,448.83200000000005,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",90,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.784,451.61600000000004,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",91,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.056,456.672,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),92,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.84,472.512,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",93,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.2,475.712,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",94,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.04,478.752,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",95,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.04,481.79200000000003,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",96,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.104,484.896,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",97,98304.0,3462144.0,0.0,0,0.0,3462144.0,3462144.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,10.304,495.2,2666496.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",98,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,15.904,511.104,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",99,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.72,513.824,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",100,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.056,518.88,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",101,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,16.896,535.776,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",102,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.752,538.5279999999999,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",103,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.592,541.1199999999999,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",104,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,543.7119999999999,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",105,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.592,546.3039999999999,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",106,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,548.8959999999998,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",107,72984.0,179760.0,3072.0,0,0.0,182832.0,182832.0,0.0,192.0,0.0,49152.0,49152.0,2.624,551.5199999999999,12288.0,24576.0,71448.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",108,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.56,554.0799999999998,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",109,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.656,556.7359999999998,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),110,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,16.928,573.6639999999998,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",111,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.68,577.3439999999997,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",112,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,579.9359999999997,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",113,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.024,584.9599999999997,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),114,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,16.0,600.9599999999997,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",115,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.264,604.2239999999997,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",116,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.072,607.2959999999997,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",117,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.04,610.3359999999997,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",118,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.04,613.3759999999996,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",119,98304.0,3462144.0,0.0,0,0.0,3462144.0,3462144.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,10.304,623.6799999999996,2666496.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",120,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,15.84,639.5199999999996,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",121,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.624,642.1439999999997,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",122,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.056,647.1999999999997,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",123,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,17.248,664.4479999999998,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",124,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.528,666.9759999999998,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",125,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.528,669.5039999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",126,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,672.0959999999998,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",127,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.624,674.7199999999998,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",128,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.656,677.3759999999997,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",129,72948.0,179688.0,3072.0,0,0.0,182760.0,182760.0,0.0,192.0,0.0,49152.0,49152.0,2.816,680.1919999999998,12288.0,24576.0,71412.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",130,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.496,682.6879999999998,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",131,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.656,685.3439999999997,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),132,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.376,702.7199999999997,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",133,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.648,706.3679999999997,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",134,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.656,709.0239999999997,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",135,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,4.992,714.0159999999996,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),136,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.68,729.6959999999996,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",137,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.168,732.8639999999996,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",138,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.072,735.9359999999996,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",139,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.136,739.0719999999995,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",140,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.168,742.2399999999996,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",141,98304.0,3462144.0,0.0,0,0.0,3462144.0,3462144.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,10.4,752.6399999999995,2666496.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",142,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,15.712,768.3519999999995,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",143,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.624,770.9759999999995,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",144,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.056,776.0319999999996,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",145,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,17.28,793.3119999999996,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",146,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.56,795.8719999999995,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",147,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.752,798.6239999999995,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",148,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.56,801.1839999999994,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",149,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.688,803.8719999999994,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",150,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.56,806.4319999999993,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",151,72948.0,179688.0,3072.0,0,0.0,182760.0,182760.0,0.0,192.0,0.0,49152.0,49152.0,2.624,809.0559999999994,12288.0,24576.0,71412.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",152,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.624,811.6799999999994,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",153,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.624,814.3039999999994,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),154,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.024,831.3279999999994,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",155,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,4.064,835.3919999999994,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",156,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,837.9839999999994,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",157,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.024,843.0079999999994,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),158,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.648,858.6559999999994,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",159,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.168,861.8239999999994,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",160,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.136,864.9599999999994,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",161,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.136,868.0959999999993,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",162,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.072,871.1679999999993,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",163,98304.0,3462144.0,0.0,0,0.0,3462144.0,3462144.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,10.24,881.4079999999993,2666496.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",164,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,15.68,897.0879999999993,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",165,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.624,899.7119999999993,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",166,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.056,904.7679999999993,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",167,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,16.8,921.5679999999993,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",168,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.56,924.1279999999992,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",169,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.528,926.6559999999993,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",170,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.656,929.3119999999992,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",171,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.688,931.9999999999992,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",172,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.528,934.5279999999992,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",173,72916.0,179624.0,3072.0,0,0.0,182696.0,182696.0,0.0,192.0,0.0,49152.0,49152.0,2.656,937.1839999999992,12288.0,24576.0,71380.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",174,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.56,939.7439999999991,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",175,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.592,942.3359999999991,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),176,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.248,959.5839999999992,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",177,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.616,963.1999999999991,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",178,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.624,965.8239999999992,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",179,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.056,970.8799999999992,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),180,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.872,986.7519999999992,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",181,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.296,990.0479999999992,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",182,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.04,993.0879999999992,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",183,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.04,996.1279999999991,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",184,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.072,999.1999999999991,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",185,98304.0,3462144.0,0.0,0,0.0,3462144.0,3462144.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,10.24,1009.4399999999991,2666496.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",186,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,15.712,1025.1519999999991,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",187,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.848,1027.999999999999,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",188,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.024,1033.023999999999,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",189,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,16.672,1049.695999999999,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",190,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.56,1052.255999999999,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",191,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.56,1054.815999999999,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",192,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,1057.407999999999,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",193,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.624,1060.031999999999,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",194,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,1062.6239999999991,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",195,73072.0,179936.0,3072.0,0,0.0,183008.0,183008.0,0.0,192.0,0.0,49152.0,49152.0,2.624,1065.2479999999991,12288.0,24576.0,71536.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",196,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.56,1067.807999999999,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",197,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.624,1070.431999999999,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),198,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,16.992,1087.423999999999,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",199,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.744,1091.167999999999,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",200,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.688,1093.855999999999,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",201,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.12,1098.975999999999,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_TN_kernel<float, 128, 16, 2, 4, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",202,158410176.0,340946176.0,8041344.0,0,0.0,348987520.0,348987520.0,2387276.0,1909876.0,0.5555484190459169,210343040.0,1164352.0,175.264,1274.2399999999989,12865792.0,19301376.0,154389504.0,4020672.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6573220.0,36386.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",203,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.048,1276.2879999999989,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",204,260.0,0.0,520.0,0,0.0,520.0,520.0,0.0,6.0,0.0,64.0,128.0,2.624,1278.911999999999,0.0,0.0,0.0,260.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,4.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",205,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.432,1281.343999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",206,0.0,201728.0,0.0,0,0.0,201728.0,201728.0,0.0,3158.0,0.0,804128.0,804128.0,3.36,1284.7039999999988,0.0,201728.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25129.0,25129.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",207,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.112,1286.815999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",208,302404.0,0.0,604808.0,0,0.0,604808.0,604808.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,4.16,1290.975999999999,0.0,0.0,0.0,302404.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",209,172800.0,0.0,345600.0,0,0.0,345600.0,345600.0,13200.0,82608.0,0.1377755511022044,5029376.0,0.0,6.464,1297.439999999999,0.0,0.0,0.0,172800.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",210,101712.0,0.0,203424.0,0,0.0,203424.0,203424.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,4.16,1301.599999999999,0.0,0.0,0.0,101712.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",211,211200.0,0.0,422400.0,0,0.0,422400.0,422400.0,13200.0,83808.0,0.13607125185551708,5029376.0,0.0,6.464,1308.063999999999,0.0,0.0,0.0,211200.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",212,101388.0,0.0,202776.0,0,0.0,202776.0,202776.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,4.032,1312.0959999999989,0.0,0.0,0.0,101388.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",213,204800.0,0.0,409600.0,0,0.0,409600.0,409600.0,13200.0,83608.0,0.13635236757292785,5029376.0,0.0,6.656,1318.7519999999988,0.0,0.0,0.0,204800.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",214,101380.0,0.0,202760.0,0,0.0,202760.0,202760.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,4.192,1322.9439999999988,0.0,0.0,0.0,101380.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",215,185600.0,0.0,371200.0,0,0.0,371200.0,371200.0,13200.0,83008.0,0.13720272742391484,5029376.0,128.0,6.752,1329.6959999999988,0.0,0.0,0.0,185600.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,4.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",216,856.0,0.0,1712.0,0,0.0,1712.0,1712.0,0.0,21.0,0.0,6432.0,800.0,3.2,1332.8959999999988,0.0,0.0,0.0,856.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,201.0,25.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",217,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.048,1334.9439999999988,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",218,576.0,0.0,1152.0,0,0.0,1152.0,1152.0,636.0,23.0,0.9650986342943855,800.0,0.0,4.512,1339.4559999999988,0.0,0.0,0.0,576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",219,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.08,1341.5359999999987,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",220,576.0,0.0,1152.0,0,0.0,1152.0,1152.0,636.0,23.0,0.9650986342943855,800.0,0.0,4.448,1345.9839999999988,0.0,0.0,0.0,576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",221,253328.0,0.0,506656.0,0,0.0,506656.0,506656.0,57400.0,13012.0,0.8152019542123502,829920.0,8704.0,6.08,1352.0639999999987,0.0,0.0,0.0,253328.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25935.0,272.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",222,512.0,0.0,1024.0,0,0.0,1024.0,1024.0,1832.0,32.0,0.9828326180257511,2560.0,0.0,6.368,1358.4319999999987,0.0,0.0,0.0,512.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,80.0,0.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",223,402056.0,0.0,804112.0,0,0.0,804112.0,804112.0,0.0,18849.0,0.0,814528.0,50272.0,4.192,1362.6239999999987,0.0,0.0,0.0,402056.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25454.0,1571.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",224,25216.0,0.0,50432.0,0,0.0,50432.0,50432.0,0.0,4737.0,0.0,1005184.0,0.0,4.16,1366.7839999999987,0.0,0.0,0.0,25216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,31412.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",225,603784.0,0.0,1207568.0,0,0.0,1207568.0,1207568.0,0.0,6283.0,0.0,0.0,1608224.0,3.712,1370.4959999999987,0.0,0.0,0.0,603784.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,50257.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",226,387858.0,0.0,775716.0,0,0.0,775716.0,775716.0,64512.0,6283.0,0.9112507945476376,804128.0,0.0,4.64,1375.1359999999988,0.0,0.0,0.0,387858.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25129.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",227,0.0,0.0,0.0,0,0.0,0.0,0.0,128.0,64.0,0.6666666666666666,8192.0,0.0,2.592,1377.727999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",228,189312.0,0.0,378624.0,0,0.0,378624.0,378624.0,65758.0,28720.0,0.6960138868307966,2720192.0,1870528.0,16.352,1394.079999999999,0.0,0.0,0.0,189312.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,85006.0,58454.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",229,62276.0,0.0,124552.0,0,0.0,124552.0,124552.0,14202.0,28467.0,0.33284117274836533,2719040.0,2451648.0,13.664,1407.743999999999,0.0,0.0,0.0,62276.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,84970.0,76614.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",230,64896.0,0.0,129792.0,0,0.0,129792.0,129792.0,15214.0,28333.0,0.3493696465887432,2712768.0,2451648.0,14.304,1422.047999999999,0.0,0.0,0.0,64896.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,84774.0,76614.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",231,64896.0,0.0,129792.0,0,0.0,129792.0,129792.0,15214.0,28438.0,0.34852927700907177,2713920.0,1879616.0,14.336,1436.383999999999,0.0,0.0,0.0,64896.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,84810.0,58738.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",232,301056.0,0.0,602112.0,0,0.0,602112.0,602112.0,9408.0,6283.0,0.5995793767127653,1608224.0,0.0,5.088,1441.471999999999,0.0,0.0,0.0,301056.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",233,0.0,0.0,0.0,0,0.0,0.0,0.0,32.0,16.0,0.6666666666666666,2048.0,0.0,2.56,1444.031999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",234,41123.0,0.0,82246.0,0,0.0,82246.0,82246.0,14849.0,15235.0,0.49358462970349687,1864992.0,1285984.0,9.024,1453.055999999999,0.0,0.0,0.0,41123.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,58281.0,40187.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",235,603272.0,0.0,1206544.0,0,0.0,1206544.0,1206544.0,0.0,25132.0,0.0,2428896.0,2412352.0,5.984,1459.0399999999988,0.0,0.0,0.0,603272.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,75903.0,75386.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",236,3284156.0,6655044.0,939528.0,0,0.0,7594572.0,7594572.0,528.0,6704.0,0.07300884955752213,2280448.0,750976.0,25.344,1484.3839999999989,825232.0,201028.0,2814392.0,469764.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,71264.0,23468.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",237,210944.0,1024200.0,421888.0,0,0.0,1446088.0,1446088.0,112284.0,12568.0,0.8993368147887099,804480.0,575552.0,78.976,1563.3599999999988,1024200.0,0.0,0.0,210944.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25140.0,17986.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",238,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,3158.0,0.0,804128.0,200800.0,3.296,1566.6559999999988,0.0,0.0,0.0,256.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25129.0,6275.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",239,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,128.0,2.144,1568.7999999999988,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,4.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",240,603084.0,0.0,1206168.0,0,0.0,1206168.0,1206168.0,0.0,18849.0,0.0,1809280.0,78464.0,8.96,1577.7599999999989,0.0,0.0,0.0,603084.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,56540.0,2452.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",241,25216.0,0.0,50432.0,0,0.0,50432.0,50432.0,0.0,4737.0,0.0,1005184.0,0.0,3.84,1581.5999999999988,0.0,0.0,0.0,25216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,31412.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",242,3284168.0,6655044.0,939552.0,0,0.0,7594596.0,7594596.0,528.0,6704.0,0.07300884955752213,2281984.0,751232.0,25.472,1607.0719999999988,825232.0,201028.0,2814392.0,469776.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,71312.0,23476.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",243,39936.0,0.0,79872.0,0,0.0,79872.0,79872.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.656,1613.7279999999987,0.0,0.0,0.0,39936.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",244,129.0,0.0,258.0,0,0.0,258.0,258.0,0.0,2.0,0.0,32.0,32.0,2.4,1616.1279999999988,0.0,0.0,0.0,129.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",245,39936.0,0.0,79872.0,0,0.0,79872.0,79872.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.624,1622.7519999999988,0.0,0.0,0.0,39936.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",246,129.0,0.0,258.0,0,0.0,258.0,258.0,0.0,2.0,0.0,32.0,32.0,2.56,1625.3119999999988,0.0,0.0,0.0,129.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",247,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,3.0,0.0,64.0,32.0,2.624,1627.9359999999988,0.0,0.0,0.0,2.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",248,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,3.296,1631.2319999999988,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",249,8192.0,220484.0,16384.0,0,0.0,236868.0,236868.0,320.0,1582.0,0.16824395373291273,804224.0,128.0,13.152,1644.3839999999989,220484.0,0.0,0.0,8192.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25132.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",250,132.0,0.0,264.0,0,0.0,264.0,264.0,0.0,2.0,0.0,32.0,32.0,2.432,1646.815999999999,0.0,0.0,0.0,132.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",251,12.0,0.0,24.0,0,0.0,24.0,24.0,0.0,2.0,0.0,32.0,32.0,3.456,1650.2719999999988,0.0,0.0,0.0,12.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",252,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,2.0,0.0,32.0,32.0,2.496,1652.767999999999,0.0,0.0,0.0,2.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",253,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,3.296,1656.063999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",254,1806336.0,2724488.0,1290240.0,0,0.0,4014728.0,4014728.0,0.0,6283.0,0.0,0.0,804128.0,4.192,1660.255999999999,0.0,402056.0,1161216.0,645120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,25129.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",255,1210564.0,2017280.0,403848.0,0,0.0,2421128.0,2421128.0,0.0,4737.0,0.0,1608256.0,0.0,5.28,1665.535999999999,0.0,0.0,1008640.0,201924.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50258.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",256,110754.0,0.0,221508.0,0,0.0,221508.0,221508.0,640.0,1582.0,0.28802880288028804,804352.0,128.0,17.568,1683.103999999999,0.0,0.0,0.0,110754.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25136.0,4.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",257,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.4,1685.503999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",258,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.464,1687.967999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",259,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,2.56,1690.5279999999989,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",260,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.528,1693.055999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",261,260.0,0.0,520.0,0,0.0,520.0,520.0,0.0,6.0,0.0,64.0,128.0,2.592,1695.647999999999,0.0,0.0,0.0,260.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,4.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",262,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,32.0,2.016,1697.663999999999,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",263,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,32.0,2.048,1699.711999999999,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",264,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,2.56,1702.271999999999,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",265,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,32.0,2.016,1704.287999999999,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",266,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,96.0,32.0,2.528,1706.8159999999991,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",267,5.0,0.0,10.0,0,0.0,10.0,10.0,0.0,5.0,0.0,32.0,32.0,5.664,1712.479999999999,0.0,0.0,0.0,5.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",268,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,2.528,1715.0079999999991,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",269,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,2.432,1717.4399999999991,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",270,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,3.2,1720.6399999999992,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",271,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,3.2,1723.8399999999992,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",272,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,2.368,1726.2079999999992,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",273,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,2.4,1728.6079999999993,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::index_elementwise_kernel<128, 4, void native::gpu_index_kernel<void native::index_kernel_impl<native::OpaqueType<8>>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>, const T1 &)::[lambda(int) (instance 1)]>(long, T3)",274,136.0,0.0,272.0,0,0.0,272.0,272.0,0.0,3.0,0.0,96.0,32.0,3.264,1731.8719999999992,0.0,0.0,0.0,136.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",275,1152.0,0.0,2304.0,0,0.0,2304.0,2304.0,56.0,4.0,0.9333333333333333,64.0,32.0,2.944,1734.8159999999991,0.0,0.0,0.0,1152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",276,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,64.0,2.4,1737.2159999999992,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,2.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",277,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,64.0,32.0,2.432,1739.6479999999992,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",278,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,96.0,0.0,2.496,1742.1439999999993,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,0.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",279,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,2.0,0.0,64.0,32.0,2.464,1744.6079999999993,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",280,3840.0,0.0,7680.0,0,0.0,7680.0,7680.0,0.0,288.0,0.0,12480.0,12288.0,4.704,1749.3119999999992,0.0,0.0,0.0,3840.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,390.0,384.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",281,3840.0,0.0,7680.0,0,0.0,7680.0,7680.0,0.0,288.0,0.0,3264.0,12288.0,3.776,1753.0879999999993,0.0,0.0,0.0,3840.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,102.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",282,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,1755.6799999999994,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",283,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,64.0,32.0,2.432,1758.1119999999994,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",284,24.0,0.0,48.0,0,0.0,48.0,48.0,0.0,2.0,0.0,32.0,32.0,3.296,1761.4079999999994,0.0,0.0,0.0,24.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",285,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.088,1766.4959999999994,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),286,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.776,1782.2719999999995,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",287,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.264,1785.5359999999994,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",288,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.648,1789.1839999999993,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",289,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.648,1792.8319999999992,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",290,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.04,1795.8719999999992,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",291,98304.0,3465216.0,0.0,0,0.0,3465216.0,3465216.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,10.496,1806.3679999999993,2669568.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1920.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",292,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,15.584,1821.9519999999993,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",293,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,1824.5439999999994,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",294,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.088,1829.6319999999994,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",295,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,17.184,1846.8159999999993,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",296,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.656,1849.4719999999993,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",297,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.56,1852.0319999999992,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",298,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.784,1854.8159999999993,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",299,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.688,1857.5039999999995,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",300,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,1860.0959999999995,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",301,73050.0,179892.0,3072.0,0,0.0,182964.0,182964.0,0.0,192.0,0.0,49152.0,49152.0,2.656,1862.7519999999995,12288.0,24576.0,71514.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",302,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.528,1865.2799999999995,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",303,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.624,1867.9039999999995,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),304,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,16.928,1884.8319999999997,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",305,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.68,1888.5119999999997,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",306,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.56,1891.0719999999997,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",307,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.024,1896.0959999999995,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),308,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.744,1911.8399999999995,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",309,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.2,1915.0399999999995,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",310,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,4.0,1919.0399999999995,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",311,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.808,1922.8479999999995,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",312,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.104,1925.9519999999995,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",313,98304.0,3465216.0,0.0,0,0.0,3465216.0,3465216.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,10.368,1936.3199999999995,2669568.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1920.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",314,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,15.744,1952.0639999999994,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",315,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.624,1954.6879999999994,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",316,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.056,1959.7439999999995,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",317,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,17.184,1976.9279999999994,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",318,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.56,1979.4879999999994,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",319,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.688,1982.1759999999995,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",320,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,1984.7679999999996,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",321,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.784,1987.5519999999997,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",322,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.848,1990.3999999999996,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",323,72970.0,179732.0,3072.0,0,0.0,182804.0,182804.0,0.0,192.0,0.0,49152.0,49152.0,2.592,1992.9919999999997,12288.0,24576.0,71434.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",324,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.528,1995.5199999999998,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",325,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.688,1998.2079999999999,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),326,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.184,2015.3919999999998,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",327,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.648,2019.0399999999997,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",328,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.56,2021.5999999999997,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",329,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.056,2026.6559999999997,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),330,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.904,2042.5599999999997,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",331,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.296,2045.8559999999998,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",332,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.712,2049.5679999999998,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",333,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.68,2053.2479999999996,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",334,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.104,2056.3519999999994,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",335,98304.0,3465216.0,0.0,0,0.0,3465216.0,3465216.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,10.4,2066.7519999999995,2669568.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1920.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",336,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,15.808,2082.5599999999995,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",337,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,2085.1519999999996,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",338,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,4.96,2090.1119999999996,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",339,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,16.896,2107.008,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",340,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.656,2109.6639999999998,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",341,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.752,2112.4159999999997,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",342,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.912,2115.3279999999995,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",343,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.656,2117.9839999999995,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",344,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.624,2120.6079999999993,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",345,73041.0,179874.0,3072.0,0,0.0,182946.0,182946.0,0.0,192.0,0.0,49152.0,49152.0,2.592,2123.1999999999994,12288.0,24576.0,71505.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",346,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.592,2125.7919999999995,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",347,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.624,2128.4159999999993,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),348,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.344,2145.7599999999993,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",349,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.68,2149.439999999999,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",350,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.656,2152.095999999999,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",351,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.056,2157.151999999999,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),352,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.968,2173.119999999999,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",353,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.36,2176.479999999999,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",354,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.648,2180.1279999999992,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",355,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.68,2183.807999999999,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",356,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.008,2186.815999999999,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",357,98304.0,3465216.0,0.0,0,0.0,3465216.0,3465216.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,10.272,2197.087999999999,2669568.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1920.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",358,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,15.584,2212.6719999999987,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",359,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.528,2215.1999999999985,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",360,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.12,2220.3199999999983,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",361,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,16.992,2237.3119999999985,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",362,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.56,2239.8719999999985,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",363,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.528,2242.3999999999983,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",364,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.528,2244.927999999998,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",365,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.656,2247.583999999998,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",366,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.72,2250.303999999998,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",367,72941.0,179674.0,3072.0,0,0.0,182746.0,182746.0,0.0,192.0,0.0,49152.0,49152.0,2.656,2252.9599999999978,12288.0,24576.0,71405.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",368,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.624,2255.5839999999976,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",369,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.624,2258.2079999999974,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),370,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.056,2275.2639999999974,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",371,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.744,2279.0079999999975,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",372,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.528,2281.5359999999973,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",373,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.024,2286.559999999997,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),374,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.744,2302.3039999999974,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",375,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.168,2305.4719999999975,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",376,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.648,2309.1199999999976,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",377,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.68,2312.7999999999975,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",378,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.008,2315.8079999999973,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",379,98304.0,3465216.0,0.0,0,0.0,3465216.0,3465216.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,10.304,2326.1119999999974,2669568.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1920.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",380,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,16.192,2342.3039999999974,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",381,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,2344.8959999999975,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",382,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.12,2350.0159999999973,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",383,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,16.864,2366.8799999999974,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",384,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,2369.4719999999975,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",385,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.656,2372.1279999999974,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",386,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.56,2374.6879999999974,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",387,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.88,2377.5679999999975,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",388,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.528,2380.0959999999973,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",389,72936.0,179664.0,3072.0,0,0.0,182736.0,182736.0,0.0,192.0,0.0,49152.0,49152.0,2.624,2382.719999999997,12288.0,24576.0,71400.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",390,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.688,2385.407999999997,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",391,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.592,2387.9999999999973,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),392,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.472,2405.4719999999975,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",393,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.616,2409.0879999999975,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",394,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.56,2411.6479999999974,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",395,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.024,2416.6719999999973,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),396,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.968,2432.639999999997,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",397,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.136,2435.775999999997,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",398,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.68,2439.455999999997,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",399,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.616,2443.071999999997,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",400,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.232,2446.303999999997,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",401,98304.0,3465216.0,0.0,0,0.0,3465216.0,3465216.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,10.4,2456.703999999997,2669568.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1920.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",402,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,15.84,2472.543999999997,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",403,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,2475.1359999999972,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",404,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,4.992,2480.1279999999974,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",405,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,17.248,2497.3759999999975,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",406,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.624,2499.9999999999973,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",407,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.56,2502.559999999997,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",408,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,2505.1519999999973,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",409,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.72,2507.871999999997,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",410,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.752,2510.623999999997,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",411,72973.0,179738.0,3072.0,0,0.0,182810.0,182810.0,0.0,192.0,0.0,49152.0,49152.0,2.592,2513.215999999997,12288.0,24576.0,71437.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",412,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.528,2515.743999999997,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",413,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.656,2518.399999999997,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),414,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.408,2535.807999999997,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",415,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.712,2539.519999999997,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",416,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,2542.111999999997,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",417,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.024,2547.135999999997,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),418,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.968,2563.1039999999966,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",419,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.232,2566.3359999999966,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",420,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.616,2569.9519999999966,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",421,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.648,2573.5999999999967,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",422,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.04,2576.6399999999967,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",423,98304.0,3465216.0,0.0,0,0.0,3465216.0,3465216.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,10.272,2586.9119999999966,2669568.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1920.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",424,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,15.776,2602.6879999999965,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",425,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,2605.2799999999966,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",426,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,4.992,2610.2719999999968,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",427,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,16.96,2627.231999999997,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",428,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.592,2629.823999999997,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",429,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.56,2632.383999999997,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",430,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.528,2634.9119999999966,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",431,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.624,2637.5359999999964,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",432,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.624,2640.159999999996,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",433,73040.0,179872.0,3072.0,0,0.0,182944.0,182944.0,0.0,192.0,0.0,49152.0,49152.0,2.592,2642.7519999999963,12288.0,24576.0,71504.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",434,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.528,2645.279999999996,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",435,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.752,2648.031999999996,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),436,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.056,2665.087999999996,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",437,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.648,2668.7359999999962,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",438,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.56,2671.295999999996,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",439,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.088,2676.3839999999964,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),440,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,2304.0,0.9888579387186629,7299072.0,294912.0,15.904,2692.2879999999964,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,228096.0,9216.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",441,55296.0,92160.0,110592.0,0,0.0,202752.0,202752.0,0.0,4320.0,0.0,304128.0,36864.0,3.2,2695.487999999996,82944.0,9216.0,0.0,55296.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,9504.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",442,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.648,2699.1359999999963,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",443,184320.0,0.0,368640.0,0,0.0,368640.0,368640.0,0.0,384.0,0.0,24576.0,24576.0,3.68,2702.815999999996,0.0,0.0,0.0,184320.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",444,4608.0,0.0,9216.0,0,0.0,9216.0,9216.0,0.0,192.0,0.0,12288.0,12288.0,3.072,2705.8879999999963,0.0,0.0,0.0,4608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",445,98304.0,3465216.0,0.0,0,0.0,3465216.0,3465216.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,10.24,2716.127999999996,2669568.0,599040.0,98304.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1920.0,384.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",446,2371584.0,4792320.0,24576.0,0,0.0,4816896.0,4816896.0,21504.0,19776.0,0.5209302325581395,2509824.0,12288.0,15.936,2732.063999999996,36864.0,36864.0,2359296.0,12288.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,78432.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",447,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.624,2734.687999999996,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",448,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.024,2739.711999999996,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",449,9486336.0,19169280.0,98304.0,0,0.0,19267584.0,19267584.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,16.992,2756.703999999996,147456.0,147456.0,9437184.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313728.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",450,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.752,2759.455999999996,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",451,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.656,2762.111999999996,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",452,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.56,2764.671999999996,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",453,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.752,2767.423999999996,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",454,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.624,2770.0479999999957,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",455,73032.0,179856.0,3072.0,0,0.0,182928.0,182928.0,0.0,192.0,0.0,49152.0,49152.0,2.656,2772.7039999999956,12288.0,24576.0,71496.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",456,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.528,2775.2319999999954,0.0,0.0,12288.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",457,0.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.592,2777.8239999999955,0.0,12288.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),458,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,17.088,2794.9119999999957,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",459,18432.0,104448.0,36864.0,0,0.0,141312.0,141312.0,0.0,3744.0,0.0,396288.0,12288.0,3.648,2798.559999999996,101376.0,3072.0,0.0,18432.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12384.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",460,3072.0,6144.0,0.0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.592,2801.151999999996,0.0,0.0,3072.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",461,24900.0,86772.0,3072.0,0,0.0,89844.0,89844.0,68.0,272.0,0.2,36864.0,12544.0,5.12,2806.271999999996,24112.0,15932.0,23364.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1152.0,392.0
"void gemmSN_TN_kernel<float, 128, 16, 2, 4, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",462,158410176.0,340946176.0,8041344.0,0,0.0,348987520.0,348987520.0,2387276.0,1909876.0,0.5555484190459169,206241280.0,1143872.0,174.784,2981.055999999996,12865792.0,19301376.0,154389504.0,4020672.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6445040.0,35746.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",463,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.016,2983.071999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",464,262.0,0.0,524.0,0,0.0,524.0,524.0,0.0,6.0,0.0,96.0,160.0,2.496,2985.567999999996,0.0,0.0,0.0,262.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,5.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",465,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.496,2988.063999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",466,0.0,201728.0,0.0,0,0.0,201728.0,201728.0,0.0,3158.0,0.0,804128.0,804128.0,3.68,2991.743999999996,0.0,201728.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25129.0,25129.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",467,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.048,2993.791999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",468,302404.0,0.0,604808.0,0,0.0,604808.0,604808.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,4.192,2997.983999999996,0.0,0.0,0.0,302404.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",469,172800.0,0.0,345600.0,0,0.0,345600.0,345600.0,13200.0,82608.0,0.1377755511022044,5029376.0,0.0,6.336,3004.3199999999956,0.0,0.0,0.0,172800.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",470,101746.0,0.0,203492.0,0,0.0,203492.0,203492.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,4.384,3008.7039999999956,0.0,0.0,0.0,101746.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",471,211200.0,0.0,422400.0,0,0.0,422400.0,422400.0,13200.0,83808.0,0.13607125185551708,5029376.0,0.0,6.528,3015.2319999999954,0.0,0.0,0.0,211200.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",472,101391.0,0.0,202782.0,0,0.0,202782.0,202782.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,4.384,3019.6159999999954,0.0,0.0,0.0,101391.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",473,190400.0,0.0,380800.0,0,0.0,380800.0,380800.0,13200.0,83158.0,0.13698914464808318,5029376.0,0.0,6.528,3026.1439999999952,0.0,0.0,0.0,190400.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",474,101380.0,0.0,202760.0,0,0.0,202760.0,202760.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,4.064,3030.207999999995,0.0,0.0,0.0,101380.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",475,187200.0,0.0,374400.0,0,0.0,374400.0,374400.0,13200.0,83058.0,0.1371314592033909,5029376.0,128.0,6.528,3036.735999999995,0.0,0.0,0.0,187200.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,4.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",476,856.0,0.0,1712.0,0,0.0,1712.0,1712.0,0.0,21.0,0.0,6432.0,800.0,3.136,3039.871999999995,0.0,0.0,0.0,856.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,201.0,25.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",477,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.144,3042.0159999999946,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",478,576.0,0.0,1152.0,0,0.0,1152.0,1152.0,636.0,23.0,0.9650986342943855,800.0,0.0,4.448,3046.4639999999945,0.0,0.0,0.0,576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",479,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.048,3048.5119999999943,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",480,576.0,0.0,1152.0,0,0.0,1152.0,1152.0,636.0,23.0,0.9650986342943855,800.0,0.0,4.64,3053.151999999994,0.0,0.0,0.0,576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",481,253328.0,0.0,506656.0,0,0.0,506656.0,506656.0,41246.0,13000.0,0.7603509936216495,829920.0,9024.0,6.08,3059.231999999994,0.0,0.0,0.0,253328.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25935.0,282.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",482,512.0,0.0,1024.0,0,0.0,1024.0,1024.0,1832.0,32.0,0.9828326180257511,2560.0,0.0,6.24,3065.471999999994,0.0,0.0,0.0,512.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,80.0,0.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",483,402056.0,0.0,804112.0,0,0.0,804112.0,804112.0,0.0,18849.0,0.0,814528.0,50272.0,4.256,3069.7279999999937,0.0,0.0,0.0,402056.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25454.0,1571.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",484,25216.0,0.0,50432.0,0,0.0,50432.0,50432.0,0.0,4737.0,0.0,1005184.0,0.0,4.16,3073.8879999999936,0.0,0.0,0.0,25216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,31412.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",485,603784.0,0.0,1207568.0,0,0.0,1207568.0,1207568.0,0.0,6283.0,0.0,0.0,1608224.0,3.68,3077.5679999999934,0.0,0.0,0.0,603784.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,50257.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",486,387854.0,0.0,775708.0,0,0.0,775708.0,775708.0,64512.0,6283.0,0.9112507945476376,804128.0,0.0,4.512,3082.0799999999936,0.0,0.0,0.0,387854.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25129.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",487,0.0,0.0,0.0,0,0.0,0.0,0.0,128.0,64.0,0.6666666666666666,8192.0,0.0,2.624,3084.7039999999934,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",488,189312.0,0.0,378624.0,0,0.0,378624.0,378624.0,65758.0,28720.0,0.6960138868307966,2719040.0,1849280.0,16.64,3101.3439999999932,0.0,0.0,0.0,189312.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,84970.0,57790.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",489,69188.0,0.0,138376.0,0,0.0,138376.0,138376.0,17010.0,28628.0,0.37271571935667647,2725952.0,2199008.0,13.76,3115.1039999999934,0.0,0.0,0.0,69188.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,85186.0,68719.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",490,64896.0,0.0,129792.0,0,0.0,129792.0,129792.0,15214.0,28315.0,0.34951411702543134,2708672.0,2451904.0,14.688,3129.7919999999935,0.0,0.0,0.0,64896.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,84646.0,76622.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",491,64896.0,0.0,129792.0,0,0.0,129792.0,129792.0,15214.0,28356.0,0.34918521918751433,2713920.0,1874112.0,14.56,3144.3519999999935,0.0,0.0,0.0,64896.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,84810.0,58566.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",492,301056.0,0.0,602112.0,0,0.0,602112.0,602112.0,9408.0,6283.0,0.5995793767127653,1608224.0,0.0,4.896,3149.2479999999937,0.0,0.0,0.0,301056.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",493,0.0,0.0,0.0,0,0.0,0.0,0.0,32.0,16.0,0.6666666666666666,2048.0,0.0,2.592,3151.839999999994,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",494,41123.0,0.0,82246.0,0,0.0,82246.0,82246.0,14849.0,15293.0,0.4926348616548338,1859616.0,1285248.0,8.864,3160.703999999994,0.0,0.0,0.0,41123.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,58113.0,40164.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",495,603272.0,0.0,1206544.0,0,0.0,1206544.0,1206544.0,0.0,25132.0,0.0,2429472.0,2412352.0,5.952,3166.655999999994,0.0,0.0,0.0,603272.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,75921.0,75386.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",496,3284156.0,6655044.0,939528.0,0,0.0,7594572.0,7594572.0,528.0,6704.0,0.07300884955752213,2279296.0,751616.0,25.152,3191.807999999994,825232.0,201028.0,2814392.0,469764.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,71228.0,23488.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",497,210944.0,1024200.0,421888.0,0,0.0,1446088.0,1446088.0,112284.0,12568.0,0.8993368147887099,804544.0,579648.0,78.208,3270.015999999994,1024200.0,0.0,0.0,210944.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25142.0,18114.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",498,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,3158.0,0.0,804128.0,200800.0,3.36,3273.3759999999943,0.0,0.0,0.0,256.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25129.0,6275.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",499,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,128.0,2.144,3275.519999999994,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,4.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",500,603084.0,0.0,1206168.0,0,0.0,1206168.0,1206168.0,0.0,18849.0,0.0,1809280.0,76384.0,8.928,3284.447999999994,0.0,0.0,0.0,603084.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,56540.0,2387.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",501,25216.0,0.0,50432.0,0,0.0,50432.0,50432.0,0.0,4737.0,0.0,1005184.0,0.0,3.872,3288.319999999994,0.0,0.0,0.0,25216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,31412.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",502,3284167.0,6655044.0,939550.0,0,0.0,7594594.0,7594594.0,528.0,6704.0,0.07300884955752213,2280448.0,751424.0,25.184,3313.503999999994,825232.0,201028.0,2814392.0,469775.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,71264.0,23482.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",503,39936.0,0.0,79872.0,0,0.0,79872.0,79872.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.496,3319.999999999994,0.0,0.0,0.0,39936.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",504,129.0,0.0,258.0,0,0.0,258.0,258.0,0.0,2.0,0.0,32.0,32.0,2.528,3322.527999999994,0.0,0.0,0.0,129.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",505,39936.0,0.0,79872.0,0,0.0,79872.0,79872.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.496,3329.023999999994,0.0,0.0,0.0,39936.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",506,129.0,0.0,258.0,0,0.0,258.0,258.0,0.0,2.0,0.0,32.0,32.0,2.464,3331.487999999994,0.0,0.0,0.0,129.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",507,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,3.0,0.0,64.0,32.0,2.624,3334.1119999999937,0.0,0.0,0.0,2.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",508,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,3.2,3337.3119999999935,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",509,8192.0,220484.0,16384.0,0,0.0,236868.0,236868.0,320.0,1582.0,0.16824395373291273,804224.0,128.0,13.152,3350.4639999999936,220484.0,0.0,0.0,8192.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25132.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",510,132.0,0.0,264.0,0,0.0,264.0,264.0,0.0,2.0,0.0,32.0,32.0,2.432,3352.8959999999934,0.0,0.0,0.0,132.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",511,12.0,0.0,24.0,0,0.0,24.0,24.0,0.0,2.0,0.0,32.0,32.0,3.264,3356.1599999999935,0.0,0.0,0.0,12.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",512,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,2.0,0.0,32.0,32.0,2.464,3358.6239999999934,0.0,0.0,0.0,2.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",513,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,3.2,3361.8239999999932,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",514,1806336.0,2724488.0,1290240.0,0,0.0,4014728.0,4014728.0,0.0,6283.0,0.0,0.0,804128.0,4.224,3366.0479999999934,0.0,402056.0,1161216.0,645120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,25129.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",515,1210563.0,2017280.0,403846.0,0,0.0,2421126.0,2421126.0,0.0,4737.0,0.0,1608256.0,12288.0,5.184,3371.2319999999936,0.0,0.0,1008640.0,201923.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50258.0,384.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",516,110754.0,0.0,221508.0,0,0.0,221508.0,221508.0,640.0,1582.0,0.28802880288028804,804384.0,128.0,17.76,3388.991999999994,0.0,0.0,0.0,110754.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25137.0,4.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",517,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.4,3391.391999999994,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",518,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.528,3393.9199999999937,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",519,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,2.4,3396.319999999994,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",520,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.464,3398.7839999999937,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",521,262.0,0.0,524.0,0,0.0,524.0,524.0,0.0,6.0,0.0,96.0,160.0,2.496,3401.279999999994,0.0,0.0,0.0,262.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,5.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",522,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,32.0,2.176,3403.4559999999938,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",523,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,32.0,2.016,3405.471999999994,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",524,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,2.528,3407.9999999999936,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",525,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,32.0,2.048,3410.0479999999934,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",526,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,128.0,32.0,2.496,3412.5439999999935,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",527,5.0,0.0,10.0,0,0.0,10.0,10.0,0.0,5.0,0.0,32.0,32.0,5.664,3418.2079999999937,0.0,0.0,0.0,5.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",528,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,2.56,3420.7679999999937,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",529,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,2.432,3423.1999999999935,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",530,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,3.104,3426.3039999999933,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",531,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,3.136,3429.4399999999932,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",532,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,2.432,3431.871999999993,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
