Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",1,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,64.0,1.696,1.696,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,2.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",2,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,64.0,1.568,3.2640000000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,2.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",3,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,1.0,0.0,0.0,32.0,1.632,4.896,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",4,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,3.0,0.0,64.0,32.0,2.208,7.104,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",5,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,2.56,9.664,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",6,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,96.0,32.0,2.56,12.224,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,1.0
"void native::unrolled_elementwise_kernel<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)], std::array<char *, 2>, 8, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<1>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",7,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,2.0,0.0,32.0,64.0,3.744,15.968,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,2.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::sum_functor<long, long, long>::operator ()(at::TensorIterator &)::[lambda(long, long) (instance 1)]>, unsigned int, long, 4, 4>>(T3)",8,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,2.0,0.0,64.0,32.0,3.392,19.36,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",9,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,2.528,21.887999999999998,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",10,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,64.0,2.176,24.064,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,2.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",11,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.048,26.112000000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void at_cuda_detail::DeviceScanInitKernel<at_cuda_detail::ScanTileState<long, 1>>(T1, int)",12,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.08,28.192,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanKernel<at_cuda_detail::DeviceScanPolicy<long, std::plus<long>>::Policy900, const long *, long *, at_cuda_detail::ScanTileState<long, 1>, std::plus<long>, at_cuda_detail::NullType, unsigned int, long, 0>(T2, T3, T4, int, T5, T6, T7)",13,128.0,0.0,256.0,0,0.0,256.0,256.0,129.0,6.0,0.9555555555555556,32.0,32.0,3.04,31.232,0.0,0.0,0.0,128.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",14,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.688,33.92,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",15,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,2.432,36.352000000000004,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",16,1280.0,0.0,2560.0,0,0.0,2560.0,2560.0,96.0,8.0,0.9230769230769231,64.0,32.0,3.04,39.392,0.0,0.0,0.0,1280.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",17,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,64.0,2.4,41.792,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,2.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",18,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,64.0,32.0,2.464,44.256,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",19,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,96.0,0.0,2.656,46.912,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,0.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",20,6912.0,0.0,13824.0,0,0.0,13824.0,13824.0,0.0,576.0,0.0,3456.0,24576.0,5.728,52.64,0.0,0.0,0.0,6912.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,108.0,768.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",21,6912.0,0.0,13824.0,0,0.0,13824.0,13824.0,0.0,576.0,0.0,3456.0,24576.0,5.184,57.824,0.0,0.0,0.0,6912.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,108.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",22,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,60.416,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",23,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,64.0,32.0,2.464,62.879999999999995,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",24,24.0,0.0,48.0,0,0.0,48.0,48.0,0.0,2.0,0.0,32.0,32.0,3.392,66.27199999999999,0.0,0.0,0.0,24.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",25,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.088,71.35999999999999,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),26,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,16.032,87.39199999999998,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",27,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.552,90.94399999999999,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",28,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,93.984,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",29,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.072,97.056,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",30,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,100.096,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",31,196608.0,6924288.0,0.0,0,0.0,6924288.0,6924288.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,15.776,115.872,5332992.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",32,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.52,131.392,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",33,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.624,134.016,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",34,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,4.992,139.00799999999998,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",35,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20030720.0,98304.0,17.504,156.51199999999997,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,625960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",36,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.656,159.16799999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",37,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.688,161.85599999999997,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",38,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.688,164.54399999999995,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",39,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.656,167.19999999999996,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",40,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.56,169.75999999999996,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",41,146008.0,359600.0,6144.0,0,0.0,365744.0,365744.0,0.0,384.0,0.0,98304.0,98304.0,2.752,172.51199999999997,24576.0,49152.0,142936.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",42,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.56,175.07199999999997,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",43,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.752,177.82399999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),44,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.216,195.04,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",45,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.0,199.04,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",46,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,201.632,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",47,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,4.928,206.56,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),48,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,16.32,222.88,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",49,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.424,226.304,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",50,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,229.344,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",51,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,232.384,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",52,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,235.42399999999998,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",53,196608.0,6924288.0,0.0,0,0.0,6924288.0,6924288.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,14.368,249.79199999999997,5332992.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",54,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,16.16,265.952,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",55,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.784,268.736,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",56,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.088,273.824,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",57,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20032768.0,98304.0,16.992,290.81600000000003,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,626024.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",58,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.624,293.44000000000005,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",59,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.624,296.0640000000001,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",60,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.56,298.6240000000001,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",61,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.784,301.4080000000001,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",62,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,304.00000000000006,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",63,145840.0,359264.0,6144.0,0,0.0,365408.0,365408.0,0.0,384.0,0.0,98304.0,98304.0,2.72,306.7200000000001,24576.0,49152.0,142768.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",64,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.848,309.5680000000001,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",65,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.752,312.3200000000001,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),66,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.216,329.5360000000001,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",67,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.128,333.6640000000001,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",68,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.56,336.2240000000001,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",69,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.024,341.2480000000001,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),70,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,16.192,357.4400000000001,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",71,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.776,361.2160000000001,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",72,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,364.25600000000014,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",73,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.008,367.2640000000001,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",74,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,370.30400000000014,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",75,196608.0,6924288.0,0.0,0,0.0,6924288.0,6924288.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,14.4,384.7040000000001,5332992.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",76,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.872,400.57600000000014,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",77,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.56,403.13600000000014,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",78,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,4.992,408.12800000000016,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",79,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20032128.0,98304.0,17.344,425.47200000000015,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,626004.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",80,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.688,428.16000000000014,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",81,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.56,430.72000000000014,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",82,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,433.3120000000001,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",83,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.816,436.1280000000001,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",84,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.816,438.9440000000001,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",85,145824.0,359232.0,6144.0,0,0.0,365376.0,365376.0,0.0,384.0,0.0,98304.0,98304.0,2.688,441.63200000000006,24576.0,49152.0,142752.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",86,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.656,444.28800000000007,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",87,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.752,447.0400000000001,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),88,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.632,464.6720000000001,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",89,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.224,468.8960000000001,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",90,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.528,471.4240000000001,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",91,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.024,476.4480000000001,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),92,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,15.872,492.3200000000001,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",93,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.584,495.9040000000001,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",94,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.008,498.9120000000001,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",95,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,501.9520000000001,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",96,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.072,505.0240000000001,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",97,196608.0,6924288.0,0.0,0,0.0,6924288.0,6924288.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,14.656,519.6800000000001,5332992.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",98,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.392,535.0720000000001,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",99,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.56,537.6320000000001,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",100,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,4.992,542.624,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",101,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20031360.0,98304.0,17.216,559.84,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,625980.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",102,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,562.432,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",103,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.592,565.024,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",104,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.56,567.584,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",105,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.752,570.3359999999999,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",106,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.944,573.2799999999999,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",107,145968.0,359520.0,6144.0,0,0.0,365664.0,365664.0,0.0,384.0,0.0,98304.0,98304.0,2.752,576.0319999999998,24576.0,49152.0,142896.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",108,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.592,578.6239999999998,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",109,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.72,581.3439999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),110,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.088,598.4319999999998,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",111,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,3.968,602.3999999999997,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",112,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.656,605.0559999999997,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",113,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,4.96,610.0159999999997,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),114,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,16.064,626.0799999999997,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",115,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.456,629.5359999999997,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",116,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,632.5759999999997,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",117,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,635.6159999999996,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",118,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,638.6559999999996,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",119,196608.0,6924288.0,0.0,0,0.0,6924288.0,6924288.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,14.432,653.0879999999996,5332992.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",120,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.744,668.8319999999997,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",121,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.56,671.3919999999996,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",122,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.024,676.4159999999996,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",123,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20030592.0,98304.0,17.056,693.4719999999996,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,625956.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",124,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.624,696.0959999999997,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",125,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.592,698.6879999999996,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",126,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.72,701.4079999999997,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",127,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.784,704.1919999999997,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",128,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.56,706.7519999999996,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",129,145896.0,359376.0,6144.0,0,0.0,365520.0,365520.0,0.0,384.0,0.0,98304.0,98304.0,2.656,709.4079999999996,24576.0,49152.0,142824.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",130,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.656,712.0639999999995,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",131,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.752,714.8159999999995,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),132,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.28,732.0959999999994,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",133,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.032,736.1279999999995,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",134,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.656,738.7839999999994,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",135,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,4.96,743.7439999999995,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),136,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,15.936,759.6799999999995,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",137,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.744,763.4239999999995,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",138,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.072,766.4959999999995,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",139,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.008,769.5039999999996,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",140,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.008,772.5119999999996,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",141,196608.0,6924288.0,0.0,0,0.0,6924288.0,6924288.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,14.496,787.0079999999996,5332992.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",142,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.712,802.7199999999996,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",143,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.56,805.2799999999995,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",144,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.12,810.3999999999995,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",145,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20030464.0,98304.0,17.056,827.4559999999996,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,625952.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",146,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,830.0479999999995,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",147,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.592,832.6399999999995,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",148,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.656,835.2959999999995,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",149,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.688,837.9839999999995,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",150,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.752,840.7359999999994,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",151,145896.0,359376.0,6144.0,0,0.0,365520.0,365520.0,0.0,384.0,0.0,98304.0,98304.0,2.688,843.4239999999994,24576.0,49152.0,142824.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",152,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.56,845.9839999999994,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",153,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.944,848.9279999999993,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),154,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.408,866.3359999999993,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",155,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.224,870.5599999999994,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",156,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,873.1519999999994,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",157,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.12,878.2719999999994,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),158,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,15.712,893.9839999999994,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",159,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.52,897.5039999999993,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",160,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.072,900.5759999999993,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",161,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,903.6159999999993,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",162,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.072,906.6879999999993,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",163,196608.0,6924288.0,0.0,0,0.0,6924288.0,6924288.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,14.464,921.1519999999994,5332992.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",164,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.744,936.8959999999994,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",165,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,939.4879999999994,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",166,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,4.992,944.4799999999993,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",167,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20032128.0,98304.0,17.632,962.1119999999993,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,626004.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",168,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.624,964.7359999999993,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",169,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.56,967.2959999999993,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",170,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,969.8879999999992,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",171,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.72,972.6079999999993,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",172,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.56,975.1679999999992,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",173,145832.0,359248.0,6144.0,0,0.0,365392.0,365392.0,0.0,384.0,0.0,98304.0,98304.0,2.912,978.0799999999992,24576.0,49152.0,142760.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",174,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.72,980.7999999999993,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",175,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.784,983.5839999999993,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),176,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,16.8,1000.3839999999992,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",177,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.0,1004.3839999999992,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",178,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.56,1006.9439999999992,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",179,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.024,1011.9679999999992,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),180,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,16.256,1028.2239999999993,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",181,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.488,1031.7119999999993,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",182,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.008,1034.7199999999993,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",183,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.008,1037.7279999999994,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",184,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,1040.7679999999993,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",185,196608.0,6924288.0,0.0,0,0.0,6924288.0,6924288.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,14.528,1055.2959999999994,5332992.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",186,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.36,1070.6559999999993,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",187,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.624,1073.2799999999993,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",188,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.024,1078.3039999999992,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",189,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20032384.0,98304.0,17.312,1095.615999999999,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,626012.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",190,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,1098.2079999999992,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",191,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.592,1100.7999999999993,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",192,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.72,1103.5199999999993,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",193,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.784,1106.3039999999994,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",194,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,1108.8959999999995,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",195,146144.0,359872.0,6144.0,0,0.0,366016.0,366016.0,0.0,384.0,0.0,98304.0,98304.0,2.688,1111.5839999999996,24576.0,49152.0,143072.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",196,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.592,1114.1759999999997,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",197,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.752,1116.9279999999997,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),198,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.248,1134.1759999999997,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",199,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.032,1138.2079999999996,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",200,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.72,1140.9279999999997,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",201,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.152,1146.0799999999997,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_TN_kernel<float, 128, 16, 2, 4, 8, 9, 0, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",202,312799680.0,681892352.0,8041344.0,0,0.0,689933696.0,689933696.0,13167764.0,2613560.0,0.8343890537954863,286253696.0,2146400.0,189.76,1335.8399999999997,25731584.0,38602752.0,308779008.0,4020672.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,8945428.0,67075.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",203,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,64.0,2.048,1337.8879999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,2.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",204,264.0,0.0,528.0,0,0.0,528.0,528.0,0.0,6.0,0.0,128.0,256.0,2.528,1340.4159999999997,0.0,0.0,0.0,264.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,8.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",205,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.528,1342.9439999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",206,0.0,402432.0,0.0,0,0.0,402432.0,402432.0,0.0,6314.0,0.0,1608224.0,1608224.0,4.352,1347.2959999999998,0.0,402432.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50257.0,50257.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",207,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.08,1349.3759999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",208,604808.0,0.0,1209616.0,0,0.0,1209616.0,1209616.0,6400.0,18968.0,0.2522863450015768,1622112.0,102400.0,4.832,1354.2079999999999,0.0,0.0,0.0,604808.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50691.0,3200.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",209,345600.0,0.0,691200.0,0,0.0,691200.0,691200.0,26400.0,165216.0,0.1377755511022044,10272256.0,0.0,7.648,1361.8559999999998,0.0,0.0,0.0,345600.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,321008.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",210,203424.0,0.0,406848.0,0,0.0,406848.0,406848.0,6400.0,18968.0,0.2522863450015768,1622112.0,102400.0,4.864,1366.7199999999998,0.0,0.0,0.0,203424.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50691.0,3200.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",211,422400.0,0.0,844800.0,0,0.0,844800.0,844800.0,26400.0,167616.0,0.13607125185551708,10311040.0,0.0,7.744,1374.4639999999997,0.0,0.0,0.0,422400.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,322220.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",212,202776.0,0.0,405552.0,0,0.0,405552.0,405552.0,6400.0,18968.0,0.2522863450015768,1622112.0,102400.0,4.704,1379.1679999999997,0.0,0.0,0.0,202776.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50691.0,3200.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",213,409600.0,0.0,819200.0,0,0.0,819200.0,819200.0,26400.0,167216.0,0.13635236757292785,10305536.0,0.0,7.712,1386.8799999999997,0.0,0.0,0.0,409600.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,322048.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",214,202760.0,0.0,405520.0,0,0.0,405520.0,405520.0,6400.0,18968.0,0.2522863450015768,1622112.0,102400.0,4.896,1391.7759999999996,0.0,0.0,0.0,202760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50691.0,3200.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",215,371200.0,0.0,742400.0,0,0.0,742400.0,742400.0,26400.0,166016.0,0.13720272742391484,10283328.0,224.0,7.52,1399.2959999999996,0.0,0.0,0.0,371200.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,321354.0,7.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",216,1168.0,0.0,2336.0,0,0.0,2336.0,2336.0,0.0,39.0,0.0,12832.0,1600.0,3.68,1402.9759999999997,0.0,0.0,0.0,1168.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,401.0,50.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",217,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.048,1405.0239999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",218,576.0,0.0,1152.0,0,0.0,1152.0,1152.0,636.0,35.0,0.9478390461997019,1600.0,0.0,4.576,1409.5999999999997,0.0,0.0,0.0,576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",219,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.08,1411.6799999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",220,576.0,0.0,1152.0,0,0.0,1152.0,1152.0,636.0,35.0,0.9478390461997019,1600.0,0.0,4.608,1416.2879999999996,0.0,0.0,0.0,576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",221,506656.0,0.0,1013312.0,0,0.0,1013312.0,1013312.0,114800.0,26024.0,0.8152019542123502,1658976.0,17344.0,7.296,1423.5839999999996,0.0,0.0,0.0,506656.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,51843.0,542.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",222,1024.0,0.0,2048.0,0,0.0,2048.0,2048.0,3664.0,64.0,0.9828326180257511,5120.0,0.0,6.272,1429.8559999999995,0.0,0.0,0.0,1024.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,160.0,0.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",223,804112.0,0.0,1608224.0,0,0.0,1608224.0,1608224.0,0.0,37695.0,0.0,1629536.0,100544.0,5.056,1434.9119999999996,0.0,0.0,0.0,804112.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50923.0,3142.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",224,50304.0,0.0,100608.0,0,0.0,100608.0,100608.0,0.0,9471.0,0.0,2010304.0,0.0,6.08,1440.9919999999995,0.0,0.0,0.0,50304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,62822.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",225,1206544.0,0.0,2413088.0,0,0.0,2413088.0,2413088.0,0.0,12565.0,0.0,0.0,3216448.0,4.928,1445.9199999999996,0.0,0.0,0.0,1206544.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,100514.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",226,388640.0,0.0,777280.0,0,0.0,777280.0,777280.0,64512.0,12565.0,0.8369812006175642,1608224.0,0.0,5.6,1451.5199999999995,0.0,0.0,0.0,388640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",227,0.0,0.0,0.0,0,0.0,0.0,0.0,128.0,64.0,0.6666666666666666,8192.0,0.0,2.624,1454.1439999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",228,374016.0,0.0,748032.0,0,0.0,748032.0,748032.0,130439.0,59254.0,0.6876321213750639,5664736.0,3752832.0,18.432,1472.5759999999996,0.0,0.0,0.0,374016.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,177023.0,117276.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",229,120000.0,0.0,240000.0,0,0.0,240000.0,240000.0,26671.0,61877.0,0.30120386682929035,5647840.0,4901600.0,15.392,1487.9679999999996,0.0,0.0,0.0,120000.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,176495.0,153175.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",230,121728.0,0.0,243456.0,0,0.0,243456.0,243456.0,27947.0,62748.0,0.3081426760019847,5640928.0,4903264.0,16.64,1504.6079999999997,0.0,0.0,0.0,121728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,176279.0,153227.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",231,121728.0,0.0,243456.0,0,0.0,243456.0,243456.0,27947.0,62006.0,0.31068446855580134,5645024.0,4117184.0,17.088,1521.6959999999997,0.0,0.0,0.0,121728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,176407.0,128662.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",232,301056.0,0.0,602112.0,0,0.0,602112.0,602112.0,9408.0,12565.0,0.4281618349792928,3216448.0,0.0,6.592,1528.2879999999998,0.0,0.0,0.0,301056.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,100514.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",233,0.0,0.0,0.0,0,0.0,0.0,0.0,32.0,16.0,0.6666666666666666,2048.0,0.0,2.624,1530.9119999999998,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",234,81862.0,0.0,163724.0,0,0.0,163724.0,163724.0,29680.0,33867.0,0.4670558798999166,3935296.0,2692800.0,11.904,1542.8159999999998,0.0,0.0,0.0,81862.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,122978.0,84150.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",235,1206544.0,0.0,2413088.0,0,0.0,2413088.0,2413088.0,0.0,50260.0,0.0,4857472.0,4824672.0,8.608,1551.4239999999998,0.0,0.0,0.0,1206544.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,151796.0,150771.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",236,6568312.0,13310088.0,1879056.0,0,0.0,15189144.0,15189144.0,1056.0,13408.0,0.07300884955752213,4432672.0,1479264.0,25.376,1576.7999999999997,1650464.0,402056.0,5628784.0,939528.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,138521.0,46227.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",237,421888.0,2048400.0,843776.0,0,0.0,2892176.0,2892176.0,224568.0,25136.0,0.8993368147887099,1608512.0,1079840.0,78.848,1655.6479999999997,2048400.0,0.0,0.0,421888.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50266.0,33745.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",238,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,6314.0,0.0,1608224.0,401600.0,4.352,1659.9999999999998,0.0,0.0,0.0,256.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50257.0,12550.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",239,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,1.0,0.0,0.0,256.0,2.144,1662.1439999999998,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,8.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",240,1206168.0,0.0,2412336.0,0,0.0,2412336.0,2412336.0,0.0,37695.0,0.0,3618528.0,152320.0,9.792,1671.9359999999997,0.0,0.0,0.0,1206168.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,113079.0,4760.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",241,50304.0,0.0,100608.0,0,0.0,100608.0,100608.0,0.0,9471.0,0.0,2010304.0,0.0,6.08,1678.0159999999996,0.0,0.0,0.0,50304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,62822.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",242,6568336.0,13310088.0,1879104.0,0,0.0,15189192.0,15189192.0,1056.0,13408.0,0.07300884955752213,4432096.0,1478272.0,25.184,1703.1999999999996,1650464.0,402056.0,5628784.0,939552.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,138503.0,46196.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",243,78336.0,0.0,156672.0,0,0.0,156672.0,156672.0,4012.0,3195.0,0.5566810045788816,1608448.0,1632.0,7.168,1710.3679999999995,0.0,0.0,0.0,78336.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50264.0,51.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",244,129.0,0.0,258.0,0,0.0,258.0,258.0,0.0,2.0,0.0,32.0,32.0,2.432,1712.7999999999995,0.0,0.0,0.0,129.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",245,78336.0,0.0,156672.0,0,0.0,156672.0,156672.0,4012.0,3195.0,0.5566810045788816,1608448.0,1632.0,7.52,1720.3199999999995,0.0,0.0,0.0,78336.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50264.0,51.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",246,129.0,0.0,258.0,0,0.0,258.0,258.0,0.0,2.0,0.0,32.0,32.0,2.528,1722.8479999999995,0.0,0.0,0.0,129.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",247,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,3.0,0.0,64.0,32.0,2.688,1725.5359999999996,0.0,0.0,0.0,2.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",248,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,3.328,1728.8639999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",249,16384.0,440456.0,32768.0,0,0.0,473224.0,473224.0,608.0,3164.0,0.16118769883351008,1608448.0,256.0,14.176,1743.0399999999995,440456.0,0.0,0.0,16384.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50264.0,8.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",250,136.0,0.0,272.0,0,0.0,272.0,272.0,0.0,2.0,0.0,32.0,32.0,2.816,1745.8559999999995,0.0,0.0,0.0,136.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",251,24.0,0.0,48.0,0,0.0,48.0,48.0,0.0,2.0,0.0,32.0,32.0,3.744,1749.5999999999995,0.0,0.0,0.0,24.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",252,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,2.0,0.0,32.0,32.0,2.528,1752.1279999999995,0.0,0.0,0.0,2.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",253,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,3.2,1755.3279999999995,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",254,1935360.0,3126544.0,1548288.0,0,0.0,4674832.0,4674832.0,0.0,12565.0,0.0,0.0,1608224.0,4.256,1759.5839999999996,0.0,804112.0,1161216.0,774144.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,50257.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",255,2414600.0,4024320.0,804880.0,0,0.0,4829200.0,4829200.0,0.0,9471.0,0.0,3216448.0,0.0,7.392,1766.9759999999997,0.0,0.0,2012160.0,402440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,100514.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",256,221508.0,0.0,443016.0,0,0.0,443016.0,443016.0,1216.0,3164.0,0.2776255707762557,1608512.0,256.0,17.056,1784.0319999999997,0.0,0.0,0.0,221508.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50266.0,8.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",257,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,128.0,64.0,2.816,1786.8479999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,2.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",258,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,64.0,2.88,1789.7279999999998,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,2.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",259,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,96.0,64.0,2.56,1792.2879999999998,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,2.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",260,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,128.0,64.0,2.496,1794.7839999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,2.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",261,264.0,0.0,528.0,0,0.0,528.0,528.0,0.0,6.0,0.0,128.0,256.0,2.496,1797.28,0.0,0.0,0.0,264.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,8.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",262,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,1.0,0.0,0.0,32.0,2.048,1799.328,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",263,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,1.0,0.0,0.0,32.0,2.112,1801.44,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",264,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,64.0,32.0,2.592,1804.0320000000002,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",265,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,1.0,0.0,0.0,32.0,2.016,1806.0480000000002,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",266,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,160.0,32.0,2.592,1808.6400000000003,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",267,10.0,0.0,20.0,0,0.0,20.0,20.0,0.0,5.0,0.0,32.0,32.0,5.696,1814.3360000000002,0.0,0.0,0.0,10.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",268,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,64.0,32.0,2.496,1816.8320000000003,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",269,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,2.0,0.0,32.0,32.0,2.464,1819.2960000000003,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",270,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,96.0,64.0,3.168,1822.4640000000002,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,2.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",271,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,2.0,0.0,64.0,32.0,3.2,1825.6640000000002,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",272,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,2.432,1828.0960000000002,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",273,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,2.848,1830.9440000000002,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::index_elementwise_kernel<128, 4, void native::gpu_index_kernel<void native::index_kernel_impl<native::OpaqueType<8>>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>, const T1 &)::[lambda(int) (instance 1)]>(long, T3)",274,144.0,0.0,288.0,0,0.0,288.0,288.0,0.0,3.0,0.0,160.0,64.0,3.584,1834.5280000000002,0.0,0.0,0.0,144.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5.0,2.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",275,1280.0,0.0,2560.0,0,0.0,2560.0,2560.0,96.0,8.0,0.9230769230769231,128.0,32.0,3.008,1837.5360000000003,0.0,0.0,0.0,1280.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",276,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,128.0,128.0,2.464,1840.0000000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",277,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,2.0,0.0,128.0,32.0,2.464,1842.4640000000002,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",278,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,160.0,0.0,2.528,1844.9920000000002,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5.0,0.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",279,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,128.0,64.0,2.528,1847.5200000000002,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,2.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",280,6912.0,0.0,13824.0,0,0.0,13824.0,13824.0,0.0,576.0,0.0,24960.0,24576.0,8.064,1855.5840000000003,0.0,0.0,0.0,6912.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,780.0,768.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",281,6912.0,0.0,13824.0,0,0.0,13824.0,13824.0,0.0,576.0,0.0,3456.0,24576.0,4.896,1860.4800000000002,0.0,0.0,0.0,6912.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,108.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",282,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.56,1863.0400000000002,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",283,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,2.0,0.0,128.0,32.0,2.432,1865.4720000000002,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",284,48.0,0.0,96.0,0,0.0,96.0,96.0,0.0,2.0,0.0,32.0,32.0,3.424,1868.8960000000002,0.0,0.0,0.0,48.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",285,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.408,1874.304,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),286,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,15.904,1890.208,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",287,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.488,1893.6960000000001,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",288,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.68,1897.3760000000002,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",289,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.904,1901.2800000000002,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",290,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,1904.3200000000002,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",291,196608.0,6930432.0,0.0,0,0.0,6930432.0,6930432.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,15.744,1920.064,5339136.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3840.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",292,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,16.224,1936.288,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",293,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.688,1938.976,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",294,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.088,1944.064,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",295,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20031744.0,98304.0,17.824,1961.8880000000001,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,625992.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",296,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.656,1964.544,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",297,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.624,1967.1680000000001,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",298,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,1969.7600000000002,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",299,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.752,1972.5120000000002,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",300,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.624,1975.1360000000002,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",301,145999.0,359582.0,6144.0,0,0.0,365726.0,365726.0,0.0,384.0,0.0,98304.0,98304.0,3.264,1978.4,24576.0,49152.0,142927.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",302,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.592,1980.9920000000002,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",303,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.784,1983.7760000000003,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),304,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.12,2000.8960000000002,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",305,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.032,2004.928,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",306,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.624,2007.5520000000001,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",307,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.056,2012.6080000000002,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),308,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,16.288,2028.8960000000002,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",309,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.456,2032.352,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",310,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.68,2036.0320000000002,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",311,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.712,2039.7440000000001,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",312,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,2042.784,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",313,196608.0,6930432.0,0.0,0,0.0,6930432.0,6930432.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,14.496,2057.28,5339136.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3840.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",314,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.968,2073.248,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",315,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.56,2075.808,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",316,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,4.928,2080.736,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",317,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20032896.0,98304.0,17.408,2098.144,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,626028.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",318,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.656,2100.7999999999997,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",319,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.56,2103.3599999999997,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",320,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,2105.9519999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",321,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.72,2108.6719999999996,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",322,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,2111.2639999999997,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",323,145939.0,359462.0,6144.0,0,0.0,365606.0,365606.0,0.0,384.0,0.0,98304.0,98304.0,2.72,2113.9839999999995,24576.0,49152.0,142867.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",324,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.72,2116.7039999999993,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",325,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.752,2119.455999999999,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),326,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.28,2136.7359999999994,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",327,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.352,2141.0879999999993,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",328,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,2143.6799999999994,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",329,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.024,2148.7039999999993,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),330,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,15.872,2164.575999999999,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",331,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.456,2168.0319999999992,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",332,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.68,2171.711999999999,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",333,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.68,2175.391999999999,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",334,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.168,2178.559999999999,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",335,196608.0,6930432.0,0.0,0,0.0,6930432.0,6930432.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,14.624,2193.183999999999,5339136.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3840.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",336,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.424,2208.607999999999,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",337,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.624,2211.2319999999986,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",338,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.088,2216.319999999999,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",339,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20030592.0,98304.0,17.056,2233.375999999999,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,625956.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",340,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.72,2236.0959999999986,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",341,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.656,2238.7519999999986,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",342,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.624,2241.3759999999984,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",343,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.688,2244.0639999999985,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",344,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.72,2246.7839999999983,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",345,146068.0,359720.0,6144.0,0,0.0,365864.0,365864.0,0.0,384.0,0.0,98304.0,98304.0,2.72,2249.503999999998,24576.0,49152.0,142996.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",346,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.624,2252.127999999998,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",347,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.72,2254.8479999999977,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),348,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.632,2272.4799999999977,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",349,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.096,2276.5759999999977,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",350,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.56,2279.1359999999977,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",351,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.024,2284.1599999999976,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),352,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,15.904,2300.0639999999976,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",353,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.552,2303.6159999999977,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",354,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.744,2307.359999999998,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",355,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.712,2311.071999999998,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",356,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.296,2314.3679999999977,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",357,196608.0,6930432.0,0.0,0,0.0,6930432.0,6930432.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,14.496,2328.8639999999978,5339136.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3840.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",358,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.424,2344.2879999999977,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",359,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,2346.879999999998,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",360,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.056,2351.935999999998,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",361,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20031104.0,98304.0,17.664,2369.599999999998,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,625972.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",362,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,2372.191999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",363,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.88,2375.0719999999983,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",364,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.816,2377.887999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",365,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.752,2380.639999999998,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",366,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.656,2383.295999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",367,145893.0,359370.0,6144.0,0,0.0,365514.0,365514.0,0.0,384.0,0.0,98304.0,98304.0,2.912,2386.207999999998,24576.0,49152.0,142821.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",368,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.592,2388.799999999998,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",369,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.752,2391.551999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),370,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.248,2408.799999999998,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",371,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.0,2412.799999999998,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",372,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,2415.391999999998,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",373,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.024,2420.415999999998,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),374,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,15.84,2436.255999999998,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",375,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.488,2439.743999999998,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",376,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.68,2443.4239999999977,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",377,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.744,2447.167999999998,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",378,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,2450.207999999998,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",379,196608.0,6930432.0,0.0,0,0.0,6930432.0,6930432.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,14.496,2464.703999999998,5339136.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3840.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",380,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.52,2480.223999999998,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",381,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.624,2482.8479999999977,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",382,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,4.96,2487.8079999999977,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",383,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20032128.0,98304.0,17.088,2504.895999999998,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,626004.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",384,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,2507.487999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",385,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.784,2510.271999999998,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",386,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.624,2512.895999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",387,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.944,2515.839999999998,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",388,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,2518.431999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",389,145884.0,359352.0,6144.0,0,0.0,365496.0,365496.0,0.0,384.0,0.0,98304.0,98304.0,2.688,2521.119999999998,24576.0,49152.0,142812.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",390,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.592,2523.711999999998,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",391,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.784,2526.4959999999983,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),392,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.472,2543.9679999999985,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",393,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.0,2547.9679999999985,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",394,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,2550.5599999999986,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",395,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,4.96,2555.5199999999986,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),396,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,16.224,2571.743999999999,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",397,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.52,2575.2639999999988,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",398,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.712,2578.9759999999987,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",399,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.712,2582.6879999999987,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",400,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.04,2585.7279999999987,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",401,196608.0,6930432.0,0.0,0,0.0,6930432.0,6930432.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,14.592,2600.319999999999,5339136.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3840.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",402,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.456,2615.775999999999,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",403,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.56,2618.335999999999,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",404,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.248,2623.583999999999,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",405,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20031360.0,98304.0,17.568,2641.151999999999,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,625980.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",406,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,2643.7439999999992,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",407,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.72,2646.463999999999,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",408,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,2649.055999999999,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",409,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.752,2651.807999999999,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",410,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.624,2654.431999999999,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",411,145997.0,359578.0,6144.0,0,0.0,365722.0,365722.0,0.0,384.0,0.0,98304.0,98304.0,2.72,2657.1519999999987,24576.0,49152.0,142925.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",412,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.656,2659.8079999999986,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",413,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.784,2662.5919999999987,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),414,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.408,2679.9999999999986,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",415,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.032,2684.031999999999,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",416,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,2686.623999999999,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",417,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.056,2691.679999999999,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),418,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,16.224,2707.903999999999,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",419,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.488,2711.391999999999,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",420,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.808,2715.199999999999,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",421,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.68,2718.8799999999987,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",422,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.136,2722.0159999999987,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",423,196608.0,6930432.0,0.0,0,0.0,6930432.0,6930432.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,14.496,2736.511999999999,5339136.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3840.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",424,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.68,2752.1919999999986,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",425,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,2754.7839999999987,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",426,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.024,2759.8079999999986,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",427,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20035584.0,98304.0,17.024,2776.8319999999985,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,626112.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",428,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.752,2779.5839999999985,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",429,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.688,2782.2719999999986,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",430,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.624,2784.8959999999984,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",431,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.72,2787.615999999998,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",432,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.592,2790.2079999999983,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",433,146099.0,359782.0,6144.0,0,0.0,365926.0,365926.0,0.0,384.0,0.0,98304.0,98304.0,2.72,2792.927999999998,24576.0,49152.0,143027.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",434,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.624,2795.551999999998,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",435,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.72,2798.2719999999977,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),436,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.376,2815.647999999998,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",437,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,4.192,2819.839999999998,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",438,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,2822.431999999998,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",439,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,4.992,2827.423999999998,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),440,56659968.0,113836032.0,73728.0,0,0.0,113909760.0,113909760.0,204480.0,4608.0,0.977961432506887,7520256.0,589824.0,16.256,2843.679999999998,0.0,589824.0,56623104.0,36864.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,235008.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",441,73728.0,184320.0,147456.0,0,0.0,331776.0,331776.0,0.0,7488.0,0.0,599040.0,73728.0,3.744,2847.423999999998,165888.0,18432.0,0.0,73728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,18720.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",442,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.68,2851.103999999998,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",443,196608.0,0.0,393216.0,0,0.0,393216.0,393216.0,0.0,768.0,0.0,49152.0,49152.0,3.68,2854.783999999998,0.0,0.0,0.0,196608.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",444,9216.0,0.0,18432.0,0,0.0,18432.0,18432.0,0.0,384.0,0.0,24576.0,24576.0,3.136,2857.919999999998,0.0,0.0,0.0,9216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",445,196608.0,6930432.0,0.0,0,0.0,6930432.0,6930432.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,14.496,2872.415999999998,5339136.0,1198080.0,196608.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3840.0,768.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",446,4743168.0,9584640.0,49152.0,0,0.0,9633792.0,9633792.0,43008.0,39552.0,0.5209302325581395,5019648.0,24576.0,15.584,2887.9999999999977,73728.0,73728.0,4718592.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,156864.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",447,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,2890.591999999998,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",448,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,4.992,2895.583999999998,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",449,18972672.0,38338560.0,196608.0,0,0.0,38535168.0,38535168.0,172032.0,158208.0,0.5209302325581395,20031104.0,98304.0,16.96,2912.543999999998,294912.0,294912.0,18874368.0,98304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,625972.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",450,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.752,2915.295999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",451,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.752,2918.047999999998,0.0,49152.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",452,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.656,2920.703999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",453,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.688,2923.391999999998,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",454,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.688,2926.079999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",455,146044.0,359672.0,6144.0,0,0.0,365816.0,365816.0,0.0,384.0,0.0,98304.0,98304.0,2.72,2928.799999999998,24576.0,49152.0,142972.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",456,24576.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.592,2931.391999999998,0.0,0.0,24576.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",457,0.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.976,2934.367999999998,0.0,24576.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6144.0,3072.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),458,75546624.0,151781376.0,98304.0,0,0.0,151879680.0,151879680.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,17.248,2951.615999999998,0.0,786432.0,75497472.0,49152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",459,24576.0,208896.0,49152.0,0,0.0,258048.0,258048.0,0.0,7104.0,0.0,789504.0,24576.0,3.968,2955.583999999998,202752.0,6144.0,0.0,24576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,24672.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",460,6144.0,12288.0,0.0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.592,2958.175999999998,0.0,0.0,6144.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",461,49800.0,173544.0,6144.0,0,0.0,179688.0,179688.0,136.0,544.0,0.2,73728.0,25088.0,5.024,2963.199999999998,48224.0,31864.0,46728.0,3072.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2304.0,784.0
"void gemmSN_TN_kernel<float, 128, 16, 2, 4, 8, 9, 0, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",462,312799680.0,681892352.0,8041344.0,0,0.0,689933696.0,689933696.0,13167764.0,2613560.0,0.8343890537954863,287480704.0,2145312.0,190.56,3153.759999999998,25731584.0,38602752.0,308779008.0,4020672.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,8983772.0,67041.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",463,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,64.0,2.048,3155.8079999999977,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,2.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",464,268.0,0.0,536.0,0,0.0,536.0,536.0,0.0,6.0,0.0,192.0,320.0,2.496,3158.303999999998,0.0,0.0,0.0,268.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6.0,10.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",465,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.432,3160.7359999999976,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",466,0.0,402432.0,0.0,0,0.0,402432.0,402432.0,0.0,6314.0,0.0,1608224.0,1608224.0,4.16,3164.8959999999975,0.0,402432.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50257.0,50257.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",467,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.048,3166.9439999999972,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",468,604808.0,0.0,1209616.0,0,0.0,1209616.0,1209616.0,6400.0,18968.0,0.2522863450015768,1622112.0,102400.0,5.088,3172.0319999999974,0.0,0.0,0.0,604808.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50691.0,3200.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",469,345600.0,0.0,691200.0,0,0.0,691200.0,691200.0,26400.0,165216.0,0.1377755511022044,10270976.0,0.0,7.392,3179.4239999999972,0.0,0.0,0.0,345600.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,320968.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",470,203451.0,0.0,406902.0,0,0.0,406902.0,406902.0,6400.0,18968.0,0.2522863450015768,1622112.0,102400.0,4.832,3184.255999999997,0.0,0.0,0.0,203451.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50691.0,3200.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",471,422400.0,0.0,844800.0,0,0.0,844800.0,844800.0,26400.0,167616.0,0.13607125185551708,10312000.0,0.0,7.264,3191.5199999999973,0.0,0.0,0.0,422400.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,322250.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",472,202788.0,0.0,405576.0,0,0.0,405576.0,405576.0,6400.0,18968.0,0.2522863450015768,1622112.0,102400.0,4.992,3196.5119999999974,0.0,0.0,0.0,202788.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50691.0,3200.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",473,379200.0,0.0,758400.0,0,0.0,758400.0,758400.0,26400.0,166266.0,0.13702469558718197,10286912.0,0.0,7.424,3203.9359999999974,0.0,0.0,0.0,379200.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,321466.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",474,202760.0,0.0,405520.0,0,0.0,405520.0,405520.0,6400.0,18968.0,0.2522863450015768,1622112.0,102400.0,4.704,3208.6399999999976,0.0,0.0,0.0,202760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50691.0,3200.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",475,379200.0,0.0,758400.0,0,0.0,758400.0,758400.0,26400.0,166266.0,0.13702469558718197,10287872.0,224.0,7.488,3216.1279999999974,0.0,0.0,0.0,379200.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,321496.0,7.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",476,1168.0,0.0,2336.0,0,0.0,2336.0,2336.0,0.0,39.0,0.0,12832.0,1600.0,3.552,3219.6799999999976,0.0,0.0,0.0,1168.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,401.0,50.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",477,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.08,3221.7599999999975,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",478,576.0,0.0,1152.0,0,0.0,1152.0,1152.0,636.0,35.0,0.9478390461997019,1600.0,0.0,4.64,3226.3999999999974,0.0,0.0,0.0,576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",479,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.08,3228.4799999999973,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",480,576.0,0.0,1152.0,0,0.0,1152.0,1152.0,636.0,35.0,0.9478390461997019,1600.0,0.0,4.64,3233.119999999997,0.0,0.0,0.0,576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",481,506656.0,0.0,1013312.0,0,0.0,1013312.0,1013312.0,84419.0,26008.0,0.7644778903710143,1658976.0,17248.0,6.944,3240.063999999997,0.0,0.0,0.0,506656.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,51843.0,539.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",482,1024.0,0.0,2048.0,0,0.0,2048.0,2048.0,3664.0,64.0,0.9828326180257511,5120.0,0.0,6.176,3246.239999999997,0.0,0.0,0.0,1024.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,160.0,0.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",483,804112.0,0.0,1608224.0,0,0.0,1608224.0,1608224.0,0.0,37695.0,0.0,1629536.0,100544.0,4.736,3250.975999999997,0.0,0.0,0.0,804112.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50923.0,3142.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",484,50304.0,0.0,100608.0,0,0.0,100608.0,100608.0,0.0,9471.0,0.0,2010304.0,0.0,6.272,3257.247999999997,0.0,0.0,0.0,50304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,62822.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",485,1206544.0,0.0,2413088.0,0,0.0,2413088.0,2413088.0,0.0,12565.0,0.0,0.0,3216448.0,4.992,3262.239999999997,0.0,0.0,0.0,1206544.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,100514.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",486,388626.0,0.0,777252.0,0,0.0,777252.0,777252.0,64512.0,12565.0,0.8369812006175642,1608224.0,0.0,5.888,3268.127999999997,0.0,0.0,0.0,388626.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",487,0.0,0.0,0.0,0,0.0,0.0,0.0,128.0,64.0,0.6666666666666666,8192.0,0.0,2.592,3270.719999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",488,374016.0,0.0,748032.0,0,0.0,748032.0,748032.0,130439.0,57376.0,0.694507893405745,5659744.0,3790336.0,18.208,3288.927999999997,0.0,0.0,0.0,374016.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,176867.0,118448.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",489,126912.0,0.0,253824.0,0,0.0,253824.0,253824.0,29479.0,61356.0,0.3245334947982606,5675488.0,4648096.0,15.36,3304.2879999999973,0.0,0.0,0.0,126912.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,177359.0,145253.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",490,121728.0,0.0,243456.0,0,0.0,243456.0,243456.0,27947.0,60069.0,0.31752181421559716,5651424.0,4902880.0,16.896,3321.1839999999975,0.0,0.0,0.0,121728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,176607.0,153215.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",491,121728.0,0.0,243456.0,0,0.0,243456.0,243456.0,27947.0,63978.0,0.30401958118031003,5636192.0,4116480.0,17.248,3338.4319999999975,0.0,0.0,0.0,121728.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,176131.0,128640.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",492,301056.0,0.0,602112.0,0,0.0,602112.0,602112.0,9408.0,12565.0,0.4281618349792928,3216448.0,0.0,6.848,3345.2799999999975,0.0,0.0,0.0,301056.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,100514.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",493,0.0,0.0,0.0,0,0.0,0.0,0.0,32.0,16.0,0.6666666666666666,2048.0,0.0,2.784,3348.0639999999976,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",494,81862.0,0.0,163724.0,0,0.0,163724.0,163724.0,29680.0,33922.0,0.4666519920757209,3913024.0,2687360.0,11.328,3359.3919999999976,0.0,0.0,0.0,81862.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,122282.0,83980.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",495,1206544.0,0.0,2413088.0,0,0.0,2413088.0,2413088.0,0.0,50260.0,0.0,4858944.0,4824672.0,9.12,3368.5119999999974,0.0,0.0,0.0,1206544.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,151842.0,150771.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",496,6568312.0,13310088.0,1879056.0,0,0.0,15189144.0,15189144.0,1056.0,13408.0,0.07300884955752213,4437248.0,1479488.0,25.568,3394.0799999999977,1650464.0,402056.0,5628784.0,939528.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,138664.0,46234.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",497,421888.0,2048400.0,843776.0,0,0.0,2892176.0,2892176.0,224568.0,25136.0,0.8993368147887099,1609088.0,1086752.0,78.432,3472.5119999999974,2048400.0,0.0,0.0,421888.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50284.0,33961.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",498,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,6314.0,0.0,1608224.0,401600.0,4.256,3476.7679999999973,0.0,0.0,0.0,256.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50257.0,12550.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",499,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,1.0,0.0,0.0,256.0,2.144,3478.911999999997,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,8.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",500,1206168.0,0.0,2412336.0,0,0.0,2412336.0,2412336.0,0.0,37695.0,0.0,3618528.0,155872.0,9.504,3488.415999999997,0.0,0.0,0.0,1206168.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,113079.0,4871.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",501,50304.0,0.0,100608.0,0,0.0,100608.0,100608.0,0.0,9471.0,0.0,2010304.0,256.0,6.144,3494.5599999999968,0.0,0.0,0.0,50304.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,62822.0,8.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",502,6568335.0,13310088.0,1879102.0,0,0.0,15189190.0,15189190.0,1056.0,13408.0,0.07300884955752213,4437472.0,1478464.0,25.344,3519.903999999997,1650464.0,402056.0,5628784.0,939551.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,138671.0,46202.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",503,78336.0,0.0,156672.0,0,0.0,156672.0,156672.0,4012.0,3195.0,0.5566810045788816,1608448.0,1632.0,7.552,3527.455999999997,0.0,0.0,0.0,78336.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50264.0,51.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",504,129.0,0.0,258.0,0,0.0,258.0,258.0,0.0,2.0,0.0,32.0,32.0,2.464,3529.919999999997,0.0,0.0,0.0,129.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",505,78336.0,0.0,156672.0,0,0.0,156672.0,156672.0,4012.0,3195.0,0.5566810045788816,1608448.0,1632.0,7.008,3536.9279999999967,0.0,0.0,0.0,78336.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50264.0,51.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",506,129.0,0.0,258.0,0,0.0,258.0,258.0,0.0,2.0,0.0,32.0,32.0,2.4,3539.327999999997,0.0,0.0,0.0,129.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",507,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,3.0,0.0,64.0,32.0,2.976,3542.303999999997,0.0,0.0,0.0,2.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",508,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,3.2,3545.5039999999967,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",509,16384.0,440456.0,32768.0,0,0.0,473224.0,473224.0,608.0,3164.0,0.16118769883351008,1608480.0,256.0,13.088,3558.591999999997,440456.0,0.0,0.0,16384.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50265.0,8.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",510,136.0,0.0,272.0,0,0.0,272.0,272.0,0.0,2.0,0.0,32.0,32.0,2.4,3560.991999999997,0.0,0.0,0.0,136.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",511,24.0,0.0,48.0,0,0.0,48.0,48.0,0.0,2.0,0.0,32.0,32.0,3.296,3564.287999999997,0.0,0.0,0.0,24.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",512,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,2.0,0.0,32.0,32.0,2.432,3566.7199999999966,0.0,0.0,0.0,2.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",513,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,3.2,3569.9199999999964,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",514,1935360.0,3126544.0,1548288.0,0,0.0,4674832.0,4674832.0,0.0,12565.0,0.0,0.0,1608224.0,4.256,3574.1759999999963,0.0,804112.0,1161216.0,774144.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,50257.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",515,2414599.0,4024320.0,804878.0,0,0.0,4829198.0,4829198.0,0.0,9471.0,0.0,3216448.0,0.0,7.296,3581.471999999996,0.0,0.0,2012160.0,402439.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,100514.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",516,221508.0,0.0,443016.0,0,0.0,443016.0,443016.0,1216.0,3164.0,0.2776255707762557,1608448.0,256.0,17.056,3598.527999999996,0.0,0.0,0.0,221508.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50264.0,8.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",517,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,128.0,64.0,2.432,3600.959999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,2.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",518,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,64.0,2.432,3603.3919999999957,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,2.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",519,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,96.0,64.0,2.496,3605.887999999996,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,2.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",520,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,128.0,64.0,2.496,3608.383999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,2.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",521,268.0,0.0,536.0,0,0.0,536.0,536.0,0.0,6.0,0.0,192.0,320.0,2.656,3611.039999999996,0.0,0.0,0.0,268.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,6.0,10.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",522,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,1.0,0.0,0.0,32.0,2.048,3613.0879999999956,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",523,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,1.0,0.0,0.0,32.0,2.016,3615.1039999999957,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",524,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,64.0,32.0,2.528,3617.6319999999955,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",525,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,1.0,0.0,0.0,32.0,2.048,3619.6799999999953,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",526,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,224.0,32.0,2.496,3622.1759999999954,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",527,10.0,0.0,20.0,0,0.0,20.0,20.0,0.0,5.0,0.0,32.0,32.0,5.824,3627.9999999999955,0.0,0.0,0.0,10.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",528,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,64.0,32.0,2.592,3630.5919999999956,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",529,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,2.0,0.0,32.0,32.0,2.432,3633.0239999999953,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",530,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,96.0,64.0,3.04,3636.0639999999953,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,2.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",531,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,2.0,0.0,64.0,32.0,3.2,3639.263999999995,0.0,0.0,0.0,16.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",532,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,2.432,3641.695999999995,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
