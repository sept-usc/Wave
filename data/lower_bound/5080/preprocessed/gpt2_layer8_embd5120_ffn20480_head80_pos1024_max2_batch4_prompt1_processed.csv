Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",1,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,0.001664,0.001664,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",2,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,0.001568,0.0032319999999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",3,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,1.0,0.0,0.0,32.0,0.001632,0.004863999999999999,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",4,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,3.0,0.0,64.0,32.0,0.002016,0.006879999999999999,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",5,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,0.002656,0.009536,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",6,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,0.002592,0.012128,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::unrolled_elementwise_kernel<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)], std::array<char *, 2>, 8, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<1>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",7,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,0.003776,0.015904,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::sum_functor<long, long, long>::operator ()(at::TensorIterator &)::[lambda(long, long) (instance 1)]>, unsigned int, long, 4, 4>>(T3)",8,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,0.003296,0.019200000000000002,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",9,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,0.00288,0.022080000000000002,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",10,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,0.002144,0.024224000000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",11,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,0.002048,0.026272000000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void at_cuda_detail::DeviceScanInitKernel<at_cuda_detail::ScanTileState<long, 1>>(T1, int)",12,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,0.00208,0.028352000000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanKernel<at_cuda_detail::DeviceScanPolicy<long, std::plus<long>>::Policy900, const long *, long *, at_cuda_detail::ScanTileState<long, 1>, std::plus<long>, at_cuda_detail::NullType, unsigned int, long, 0>(T2, T3, T4, int, T5, T6, T7)",13,128.0,0.0,256.0,0,0.0,256.0,256.0,129.0,6.0,0.9555555555555556,32.0,32.0,0.002912,0.031264,0.0,0.0,0.0,128.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",14,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,0.002496,0.03376,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",15,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,0.002432,0.036192,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",16,1152.0,0.0,2304.0,0,0.0,2304.0,2304.0,56.0,4.0,0.9333333333333333,32.0,32.0,0.003072,0.039264,0.0,0.0,0.0,1152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",17,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,0.002496,0.04176,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",18,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,2.0,0.0,32.0,32.0,0.002368,0.044128,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",19,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,3.0,0.0,64.0,0.0,0.00256,0.046688,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,0.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",20,25600.0,0.0,51200.0,0,0.0,51200.0,51200.0,0.0,1920.0,0.0,21760.0,81920.0,0.004128,0.050816,0.0,0.0,0.0,25600.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,680.0,2560.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",21,25600.0,0.0,51200.0,0,0.0,51200.0,51200.0,0.0,1920.0,0.0,21760.0,81920.0,0.003904,0.05472,0.0,0.0,0.0,25600.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,680.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",22,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002688,0.057408,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",23,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,2.0,0.0,32.0,32.0,0.002432,0.059840000000000004,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",24,12.0,0.0,24.0,0,0.0,24.0,24.0,0.0,2.0,0.0,32.0,32.0,0.00336,0.0632,0.0,0.0,0.0,12.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",25,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012864,0.076064,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),26,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324293824.0,491520.0,0.352064,0.428128,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134182.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",27,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006176,0.434304,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",28,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003136,0.43744000000000005,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",29,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003008,0.44044800000000006,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",30,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003072,0.4435200000000001,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",31,655360.0,23080960.0,0.0,0,0.0,23080960.0,23080960.0,170240.0,320.0,0.99812382739212,245760.0,81920.0,0.026336,0.46985600000000005,17776640.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",32,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.128768,0.598624,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",33,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002656,0.60128,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",34,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012992,0.614272,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",35,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,427110144.0,327680.0,0.470976,1.085248,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13347192.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",36,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,1.088128,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",37,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002944,1.091072,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",38,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,1.093952,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",39,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003264,1.097216,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",40,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,1.100032,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",41,451908.0,1129096.0,20480.0,0,0.0,1149576.0,1149576.0,0.0,1280.0,0.0,327680.0,327680.0,0.002944,1.102976,81920.0,163840.0,441668.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",42,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,1.105824,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",43,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.003232,1.1090559999999998,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",44,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.496608,1.6056639999999998,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",45,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002688,1.6083519999999998,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",46,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,246272.0,82176.0,0.012864,1.6212159999999998,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7696.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),47,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324291072.0,491520.0,0.353792,1.9750079999999999,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134096.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",48,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006272,1.98128,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",49,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.00336,1.98464,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",50,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003136,1.987776,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",51,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.00304,1.990816,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",52,655360.0,23080960.0,0.0,0,0.0,23080960.0,23080960.0,170240.0,320.0,0.99812382739212,245760.0,81920.0,0.025088,2.015904,17776640.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",53,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.126368,2.1422719999999997,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",54,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002688,2.1449599999999998,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",55,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.01264,2.1576,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",56,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,426925440.0,327680.0,0.469248,2.626848,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13341420.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",57,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,2.629632,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",58,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,2.632416,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",59,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,2.6352960000000003,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",60,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003168,2.6384640000000004,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",61,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,2.6412480000000005,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",62,451468.0,1128216.0,20480.0,0,0.0,1148696.0,1148696.0,0.0,1280.0,0.0,327680.0,327680.0,0.003136,2.6443840000000005,81920.0,163840.0,441228.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",63,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,2.6472320000000007,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",64,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.0032,2.650432000000001,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",65,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.495776,3.1462080000000006,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",66,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.00272,3.1489280000000006,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",67,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012864,3.1617920000000006,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),68,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324290528.0,491520.0,0.351808,3.5136000000000007,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134079.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",69,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006144,3.5197440000000006,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",70,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003008,3.5227520000000005,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",71,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.002976,3.5257280000000004,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",72,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.00304,3.5287680000000003,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",73,655360.0,23080960.0,0.0,0,0.0,23080960.0,23080960.0,170240.0,320.0,0.99812382739212,245760.0,81920.0,0.025248,3.5540160000000003,17776640.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",74,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.127072,3.6810880000000004,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",75,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.00272,3.6838080000000004,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",76,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.013024,3.6968320000000006,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",77,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,427620224.0,327680.0,0.470528,4.16736,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13363132.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",78,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002752,4.1701120000000005,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",79,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,4.172896000000001,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",80,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,4.175680000000001,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",81,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003104,4.178784000000001,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",82,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,4.181632000000001,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",83,451956.0,1129192.0,20480.0,0,0.0,1149672.0,1149672.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,4.184512000000002,81920.0,163840.0,441716.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",84,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,4.187328000000002,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",85,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.003232,4.190560000000001,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",86,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.495296,4.685856000000001,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",87,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002752,4.688608000000001,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",88,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.01296,4.701568000000001,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),89,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324299968.0,491520.0,0.352448,5.054016000000001,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134374.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",90,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006496,5.060512000000001,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",91,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.00304,5.063552000000001,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",92,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.00304,5.066592000000002,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",93,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003104,5.069696000000002,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",94,655360.0,23080960.0,0.0,0,0.0,23080960.0,23080960.0,170240.0,320.0,0.99812382739212,245760.0,81920.0,0.02512,5.0948160000000025,17776640.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",95,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.126912,5.221728000000002,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",96,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002944,5.224672000000003,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",97,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.0128,5.237472000000003,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",98,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,428420608.0,327680.0,0.46832,5.705792000000003,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13388144.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",99,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.003104,5.708896000000004,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",100,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,5.711744000000004,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",101,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,5.714624000000004,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",102,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003232,5.717856000000004,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",103,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,5.720640000000004,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",104,451596.0,1128472.0,20480.0,0,0.0,1148952.0,1148952.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,5.723520000000004,81920.0,163840.0,441356.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",105,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,5.726304000000004,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",106,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.003168,5.729472000000004,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",107,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.495872,6.225344000000004,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",108,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002976,6.2283200000000045,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",109,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012896,6.241216000000004,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),110,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324302272.0,491520.0,0.353536,6.594752000000004,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134446.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",111,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.00624,6.600992000000004,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",112,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003072,6.604064000000005,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",113,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003168,6.607232000000004,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",114,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003136,6.610368000000004,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",115,655360.0,23080960.0,0.0,0,0.0,23080960.0,23080960.0,170240.0,320.0,0.99812382739212,245760.0,81920.0,0.025088,6.635456000000004,17776640.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",116,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.127008,6.762464000000004,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",117,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.00272,6.765184000000004,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",118,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012864,6.7780480000000045,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",119,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,429344512.0,327680.0,0.469696,7.247744000000004,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13417016.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",120,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,7.2505280000000045,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",121,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,7.253344000000005,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",122,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,7.256160000000005,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",123,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003136,7.259296000000004,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",124,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002976,7.262272000000005,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",125,452184.0,1129648.0,20480.0,0,0.0,1150128.0,1150128.0,0.0,1280.0,0.0,327680.0,327680.0,0.002912,7.265184000000005,81920.0,163840.0,441944.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",126,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,7.268000000000005,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",127,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.003392,7.271392000000005,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",128,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.4944,7.765792000000005,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",129,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.00272,7.768512000000005,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",130,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.013024,7.7815360000000044,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),131,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324308544.0,491520.0,0.351872,8.133408000000005,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134642.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",132,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006176,8.139584000000005,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",133,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003136,8.142720000000004,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",134,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.0032,8.145920000000004,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",135,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003008,8.148928000000003,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",136,655360.0,23080960.0,0.0,0,0.0,23080960.0,23080960.0,170240.0,320.0,0.99812382739212,245760.0,81920.0,0.02512,8.174048000000003,17776640.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",137,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.126816,8.300864000000002,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",138,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002784,8.303648000000003,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",139,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012864,8.316512000000003,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",140,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,428261376.0,327680.0,0.469728,8.786240000000003,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13383168.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",141,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,8.789120000000002,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",142,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,8.791968000000002,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",143,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.003104,8.795072000000003,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",144,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003232,8.798304000000003,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",145,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.00272,8.801024000000004,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",146,452072.0,1129424.0,20480.0,0,0.0,1149904.0,1149904.0,0.0,1280.0,0.0,327680.0,327680.0,0.003168,8.804192000000004,81920.0,163840.0,441832.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",147,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,8.807072000000003,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",148,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.003456,8.810528000000003,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",149,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.495456,9.305984000000004,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",150,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002752,9.308736000000003,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",151,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.01296,9.321696000000003,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),152,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324292096.0,491520.0,0.358816,9.680512000000004,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134128.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",153,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006432,9.686944000000004,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",154,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003296,9.690240000000005,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",155,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003104,9.693344000000005,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",156,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.00304,9.696384000000005,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",157,655360.0,23080960.0,0.0,0,0.0,23080960.0,23080960.0,170240.0,320.0,0.99812382739212,245760.0,81920.0,0.025184,9.721568000000005,17776640.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",158,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.126304,9.847872000000004,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",159,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002784,9.850656000000004,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",160,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012928,9.863584000000005,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",161,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,432797568.0,327680.0,0.466144,10.329728000000005,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13524924.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",162,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,10.332544000000004,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",163,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,10.335392000000004,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",164,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,10.338208000000003,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",165,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003264,10.341472000000003,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",166,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,10.344352000000002,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",167,452076.0,1129432.0,20480.0,0,0.0,1149912.0,1149912.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,10.347232000000002,81920.0,163840.0,441836.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",168,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,10.350048000000001,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",169,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.0032,10.353248,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",170,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.495744,10.848992,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",171,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002752,10.851744,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",172,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012928,10.864672,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),173,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324295840.0,491520.0,0.35504,11.219712000000001,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134245.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",174,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006208,11.225920000000002,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",175,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003072,11.228992000000002,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",176,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003008,11.232000000000001,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",177,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003008,11.235008,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",178,655360.0,23080960.0,0.0,0,0.0,23080960.0,23080960.0,170240.0,320.0,0.99812382739212,245760.0,81920.0,0.025056,11.260064,17776640.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",179,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.127104,11.387167999999999,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",180,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002784,11.389952,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",181,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012928,11.40288,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",182,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,427757440.0,327680.0,0.471136,11.874016,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13367420.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",183,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,11.876864,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",184,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,11.879712,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",185,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.003104,11.882816,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",186,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003264,11.88608,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",187,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,11.888864,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",188,451764.0,1128808.0,20480.0,0,0.0,1149288.0,1149288.0,0.0,1280.0,0.0,327680.0,327680.0,0.002944,11.891808,81920.0,163840.0,441524.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",189,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,11.894592,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",190,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.003488,11.89808,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",191,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.495392,12.393472000000001,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",192,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002688,12.39616,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",193,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012896,12.409056,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_TN_kernel<float, 128, 16, 2, 4, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",194,1046953936.0,2254747968.0,35381152.0,0,0.0,2290129120.0,2290129120.0,13494396.0,12162916.0,0.5259473790551403,1508947200.0,1176352.0,1.152384,13.56144,67545408.0,128675840.0,1029263360.0,17690576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,47154600.0,36761.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",195,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,0.002048,13.563488,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",196,260.0,0.0,520.0,0,0.0,520.0,520.0,0.0,6.0,0.0,64.0,128.0,0.002528,13.566016,0.0,0.0,0.0,260.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,4.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",197,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,0.002496,13.568512,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",198,0.0,201728.0,0.0,0,0.0,201728.0,201728.0,0.0,3158.0,0.0,804128.0,804128.0,0.003424,13.571936,0.0,201728.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25129.0,25129.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",199,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,0.002304,13.574240000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",200,302404.0,0.0,604808.0,0,0.0,604808.0,604808.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,0.004096,13.578336000000002,0.0,0.0,0.0,302404.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",201,172800.0,0.0,345600.0,0,0.0,345600.0,345600.0,13200.0,82608.0,0.1377755511022044,5029376.0,0.0,0.006656,13.584992000000002,0.0,0.0,0.0,172800.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",202,127716.0,0.0,255432.0,0,0.0,255432.0,255432.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,0.004192,13.589184000000001,0.0,0.0,0.0,127716.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",203,179200.0,0.0,358400.0,0,0.0,358400.0,358400.0,13200.0,82808.0,0.13748854262144822,5029376.0,0.0,0.006464,13.595648,0.0,0.0,0.0,179200.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",204,101392.0,0.0,202784.0,0,0.0,202784.0,202784.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,0.00432,13.599968,0.0,0.0,0.0,101392.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",205,185600.0,0.0,371200.0,0,0.0,371200.0,371200.0,13200.0,83008.0,0.13720272742391484,5029376.0,0.0,0.006464,13.606432,0.0,0.0,0.0,185600.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",206,101380.0,0.0,202760.0,0,0.0,202760.0,202760.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,0.004352,13.610784,0.0,0.0,0.0,101380.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",207,198400.0,0.0,396800.0,0,0.0,396800.0,396800.0,13200.0,83408.0,0.1366346472341835,5029376.0,128.0,0.006528,13.617312,0.0,0.0,0.0,198400.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,4.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",208,856.0,0.0,1712.0,0,0.0,1712.0,1712.0,0.0,21.0,0.0,6432.0,800.0,0.003232,13.620544,0.0,0.0,0.0,856.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,201.0,25.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",209,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,0.002048,13.622592000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",210,576.0,0.0,1152.0,0,0.0,1152.0,1152.0,636.0,23.0,0.9650986342943855,800.0,0.0,0.004448,13.627040000000001,0.0,0.0,0.0,576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",211,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,0.002016,13.629056,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",212,576.0,0.0,1152.0,0,0.0,1152.0,1152.0,636.0,23.0,0.9650986342943855,800.0,0.0,0.004384,13.63344,0.0,0.0,0.0,576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",213,253328.0,0.0,506656.0,0,0.0,506656.0,506656.0,42148.0,13004.0,0.7642152596460691,829920.0,8160.0,0.006368,13.639808,0.0,0.0,0.0,253328.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25935.0,255.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",214,512.0,0.0,1024.0,0,0.0,1024.0,1024.0,1832.0,32.0,0.9828326180257511,2560.0,0.0,0.006304,13.646112,0.0,0.0,0.0,512.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,80.0,0.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",215,402056.0,0.0,804112.0,0,0.0,804112.0,804112.0,0.0,18849.0,0.0,814528.0,50272.0,0.00416,13.650272000000001,0.0,0.0,0.0,402056.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25454.0,1571.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",216,25216.0,0.0,50432.0,0,0.0,50432.0,50432.0,0.0,4737.0,0.0,1005184.0,0.0,0.00384,13.654112000000001,0.0,0.0,0.0,25216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,31412.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",217,603784.0,0.0,1207568.0,0,0.0,1207568.0,1207568.0,0.0,6283.0,0.0,0.0,1608224.0,0.00384,13.657952000000002,0.0,0.0,0.0,603784.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,50257.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",218,388049.0,0.0,776098.0,0,0.0,776098.0,776098.0,64512.0,6283.0,0.9112507945476376,804128.0,0.0,0.004544,13.662496,0.0,0.0,0.0,388049.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25129.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",219,0.0,0.0,0.0,0,0.0,0.0,0.0,128.0,64.0,0.6666666666666666,8192.0,0.0,0.002624,13.665120000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",220,189312.0,0.0,378624.0,0,0.0,378624.0,378624.0,65758.0,28755.0,0.6957561393670711,2731968.0,1868480.0,0.01648,13.681600000000001,0.0,0.0,0.0,189312.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,85374.0,58390.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",221,62276.0,0.0,124552.0,0,0.0,124552.0,124552.0,14202.0,28503.0,0.3325605900948367,2720832.0,2451904.0,0.013696,13.695296,0.0,0.0,0.0,62276.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,85026.0,76622.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",222,64896.0,0.0,129792.0,0,0.0,129792.0,129792.0,15214.0,28245.0,0.3500770841482777,2714304.0,1875008.0,0.014688,13.709984,0.0,0.0,0.0,64896.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,84822.0,58594.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",223,61440.0,0.0,122880.0,0,0.0,122880.0,122880.0,13810.0,28305.0,0.32791167042621394,2720064.0,2451648.0,0.015168,13.725152,0.0,0.0,0.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,85002.0,76614.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",224,301056.0,0.0,602112.0,0,0.0,602112.0,602112.0,9408.0,6283.0,0.5995793767127653,1608224.0,0.0,0.004928,13.73008,0.0,0.0,0.0,301056.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",225,0.0,0.0,0.0,0,0.0,0.0,0.0,32.0,16.0,0.6666666666666666,2048.0,0.0,0.002592,13.732671999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",226,41123.0,0.0,82246.0,0,0.0,82246.0,82246.0,14849.0,15186.0,0.49438987847511234,1870624.0,1285152.0,0.008992,13.741663999999998,0.0,0.0,0.0,41123.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,58457.0,40161.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",227,603272.0,0.0,1206544.0,0,0.0,1206544.0,1206544.0,0.0,25132.0,0.0,2430048.0,2412352.0,0.00592,13.747583999999998,0.0,0.0,0.0,603272.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,75939.0,75386.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",228,3284156.0,6655044.0,939528.0,0,0.0,7594572.0,7594572.0,528.0,6704.0,0.07300884955752213,2279552.0,751232.0,0.025376,13.772959999999998,825232.0,201028.0,2814392.0,469764.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,71236.0,23476.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",229,210944.0,1024200.0,421888.0,0,0.0,1446088.0,1446088.0,112284.0,12568.0,0.8993368147887099,804416.0,582784.0,0.078944,13.851903999999998,1024200.0,0.0,0.0,210944.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25138.0,18212.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",230,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,3158.0,0.0,804128.0,200800.0,0.003392,13.855295999999997,0.0,0.0,0.0,256.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25129.0,6275.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",231,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,128.0,0.002272,13.857567999999997,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,4.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",232,603084.0,0.0,1206168.0,0,0.0,1206168.0,1206168.0,0.0,18849.0,0.0,1809280.0,76128.0,0.009152,13.866719999999997,0.0,0.0,0.0,603084.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,56540.0,2379.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",233,25216.0,0.0,50432.0,0,0.0,50432.0,50432.0,0.0,4737.0,0.0,1005184.0,0.0,0.003776,13.870495999999997,0.0,0.0,0.0,25216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,31412.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",234,3284172.0,6655044.0,939560.0,0,0.0,7594604.0,7594604.0,528.0,6704.0,0.07300884955752213,2280448.0,751424.0,0.025472,13.895967999999998,825232.0,201028.0,2814392.0,469780.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,71264.0,23482.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",235,39936.0,0.0,79872.0,0,0.0,79872.0,79872.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,0.006688,13.902655999999999,0.0,0.0,0.0,39936.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",236,129.0,0.0,258.0,0,0.0,258.0,258.0,0.0,2.0,0.0,32.0,32.0,0.002432,13.905088,0.0,0.0,0.0,129.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",237,39936.0,0.0,79872.0,0,0.0,79872.0,79872.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,0.006688,13.911776,0.0,0.0,0.0,39936.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",238,129.0,0.0,258.0,0,0.0,258.0,258.0,0.0,2.0,0.0,32.0,32.0,0.002464,13.91424,0.0,0.0,0.0,129.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",239,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,3.0,0.0,64.0,32.0,0.002592,13.916832,0.0,0.0,0.0,2.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",240,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,0.003264,13.920096,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",241,8192.0,220484.0,16384.0,0,0.0,236868.0,236868.0,320.0,1582.0,0.16824395373291273,804224.0,128.0,0.013344,13.93344,220484.0,0.0,0.0,8192.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25132.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",242,132.0,0.0,264.0,0,0.0,264.0,264.0,0.0,2.0,0.0,32.0,32.0,0.002464,13.935903999999999,0.0,0.0,0.0,132.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",243,12.0,0.0,24.0,0,0.0,24.0,24.0,0.0,2.0,0.0,32.0,32.0,0.003264,13.939167999999999,0.0,0.0,0.0,12.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",244,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,2.0,0.0,32.0,32.0,0.00256,13.941728,0.0,0.0,0.0,2.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",245,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,0.003232,13.94496,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",246,1806336.0,2724488.0,1290240.0,0,0.0,4014728.0,4014728.0,0.0,6283.0,0.0,0.0,804128.0,0.004096,13.949056,0.0,402056.0,1161216.0,645120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,25129.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",247,1210568.0,2017280.0,403856.0,0,0.0,2421136.0,2421136.0,0.0,4737.0,0.0,1608256.0,0.0,0.005536,13.954592,0.0,0.0,1008640.0,201928.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50258.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",248,110754.0,0.0,221508.0,0,0.0,221508.0,221508.0,640.0,1582.0,0.28802880288028804,804288.0,128.0,0.017472,13.972064,0.0,0.0,0.0,110754.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25134.0,4.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",249,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,0.002432,13.974496,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",250,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,0.002464,13.97696,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",251,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,0.002496,13.979456,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",252,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,0.002432,13.981888000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",253,260.0,0.0,520.0,0,0.0,520.0,520.0,0.0,6.0,0.0,64.0,128.0,0.002496,13.984384000000002,0.0,0.0,0.0,260.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,4.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",254,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,32.0,0.002048,13.986432000000002,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",255,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,32.0,0.002016,13.988448000000002,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",256,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,0.002528,13.990976000000002,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",257,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,32.0,0.002016,13.992992000000001,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",258,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,96.0,32.0,0.002528,13.99552,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",259,5.0,0.0,10.0,0,0.0,10.0,10.0,0.0,5.0,0.0,32.0,32.0,0.005728,14.001248,0.0,0.0,0.0,5.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",260,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,0.002528,14.003776,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",261,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,0.002432,14.006208,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",262,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,0.003072,14.00928,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",263,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,0.003232,14.012512000000001,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",264,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,0.002432,14.014944000000002,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",265,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,0.002464,14.017408000000001,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::index_elementwise_kernel<128, 4, void native::gpu_index_kernel<void native::index_kernel_impl<native::OpaqueType<8>>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>, const T1 &)::[lambda(int) (instance 1)]>(long, T3)",266,136.0,0.0,272.0,0,0.0,272.0,272.0,0.0,3.0,0.0,96.0,32.0,0.003392,14.020800000000001,0.0,0.0,0.0,136.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",267,1152.0,0.0,2304.0,0,0.0,2304.0,2304.0,56.0,4.0,0.9333333333333333,64.0,32.0,0.003008,14.023808,0.0,0.0,0.0,1152.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",268,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,64.0,0.002432,14.026240000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,2.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",269,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,64.0,32.0,0.002624,14.028864000000002,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",270,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,96.0,0.0,0.002464,14.031328000000002,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,0.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",271,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,2.0,0.0,64.0,32.0,0.002496,14.033824000000003,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",272,25600.0,0.0,51200.0,0,0.0,51200.0,51200.0,0.0,1920.0,0.0,83200.0,81920.0,0.004928,14.038752000000002,0.0,0.0,0.0,25600.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2600.0,2560.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",273,25600.0,0.0,51200.0,0,0.0,51200.0,51200.0,0.0,1920.0,0.0,21760.0,81920.0,0.003808,14.042560000000002,0.0,0.0,0.0,25600.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,680.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",274,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002752,14.045312000000001,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",275,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,64.0,32.0,0.0024,14.047712,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",276,24.0,0.0,48.0,0,0.0,48.0,48.0,0.0,2.0,0.0,32.0,32.0,0.003264,14.050976,0.0,0.0,0.0,24.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",277,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012928,14.063904,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),278,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324299072.0,491520.0,0.351776,14.41568,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134346.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",279,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006144,14.421824,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",280,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.003808,14.425632,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",281,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.00368,14.429312,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",282,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003072,14.432383999999999,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",283,655360.0,23101440.0,0.0,0,0.0,23101440.0,23101440.0,170240.0,320.0,0.99812382739212,409600.0,81920.0,0.025056,14.457439999999998,17797120.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12800.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",284,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.126208,14.583647999999998,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",285,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002624,14.586272,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",286,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012736,14.599008,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",287,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,428138752.0,327680.0,0.468992,15.068,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13379336.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",288,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,15.070784,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",289,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,15.073568,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",290,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,15.076448,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",291,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003168,15.079616,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",292,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,15.082495999999999,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",293,451910.0,1129100.0,20480.0,0,0.0,1149580.0,1149580.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,15.085375999999998,81920.0,163840.0,441670.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",294,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,15.088255999999998,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",295,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.003296,15.091551999999998,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",296,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.494784,15.586335999999998,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",297,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.00272,15.589055999999998,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",298,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.0128,15.601855999999998,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),299,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324297120.0,491520.0,0.352,15.953855999999998,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134285.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",300,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006336,15.960191999999997,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",301,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.003712,15.963903999999998,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",302,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.003712,15.967615999999998,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",303,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.00304,15.970655999999998,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",304,655360.0,23101440.0,0.0,0,0.0,23101440.0,23101440.0,170240.0,320.0,0.99812382739212,409600.0,81920.0,0.025312,15.995967999999998,17797120.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12800.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",305,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.126176,16.122144,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",306,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002816,16.124959999999998,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",307,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012896,16.137856,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",308,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,427318016.0,327680.0,0.468992,16.606848,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13353688.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",309,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002752,16.6096,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",310,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,16.61248,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",311,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,16.615328,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",312,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003392,16.618720000000003,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",313,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,16.621504,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",314,451609.0,1128498.0,20480.0,0,0.0,1148978.0,1148978.0,0.0,1280.0,0.0,327680.0,327680.0,0.002912,16.624416,81920.0,163840.0,441369.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",315,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,16.627232,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",316,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.003328,16.63056,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",317,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.497152,17.127712,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",318,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002816,17.130527999999998,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",319,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012928,17.143455999999997,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),320,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324295456.0,491520.0,0.355072,17.498527999999997,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134233.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",321,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006272,17.504799999999996,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",322,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.003712,17.508511999999996,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",323,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.003712,17.512223999999996,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",324,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003008,17.515231999999997,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",325,655360.0,23101440.0,0.0,0,0.0,23101440.0,23101440.0,170240.0,320.0,0.99812382739212,409600.0,81920.0,0.025152,17.540383999999996,17797120.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12800.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",326,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.126112,17.666495999999995,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",327,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002656,17.669151999999997,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",328,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.0128,17.681951999999995,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",329,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,427431808.0,327680.0,0.471072,18.153023999999995,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13357244.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",330,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,18.155807999999993,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",331,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,18.15859199999999,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",332,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.003008,18.161599999999993,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",333,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003424,18.165023999999992,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",334,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,18.167871999999992,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",335,451866.0,1129012.0,20480.0,0,0.0,1149492.0,1149492.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,18.170751999999993,81920.0,163840.0,441626.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",336,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002944,18.173695999999993,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",337,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.0032,18.176895999999992,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",338,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.494336,18.671231999999993,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",339,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002688,18.673919999999992,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",340,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.013056,18.68697599999999,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),341,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324299424.0,491520.0,0.354592,19.04156799999999,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134357.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",342,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006432,19.04799999999999,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",343,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.003712,19.05171199999999,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",344,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.003712,19.05542399999999,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",345,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.00304,19.05846399999999,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",346,655360.0,23101440.0,0.0,0,0.0,23101440.0,23101440.0,170240.0,320.0,0.99812382739212,409600.0,81920.0,0.025088,19.08355199999999,17797120.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12800.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",347,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.126112,19.20966399999999,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",348,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002912,19.212575999999988,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",349,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.013024,19.22559999999999,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",350,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,426829440.0,327680.0,0.466304,19.69190399999999,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13338420.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",351,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,19.69468799999999,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",352,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,19.697503999999988,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",353,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,19.700319999999987,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",354,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003168,19.703487999999986,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",355,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,19.706271999999984,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",356,452123.0,1129526.0,20480.0,0,0.0,1150006.0,1150006.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,19.709151999999985,81920.0,163840.0,441883.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",357,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,19.711999999999986,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",358,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.003232,19.715231999999986,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",359,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.494944,20.210175999999986,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",360,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.00272,20.212895999999986,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",361,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.01296,20.225855999999986,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),362,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324303744.0,491520.0,0.352384,20.578239999999987,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134492.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",363,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.0064,20.584639999999986,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",364,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.004032,20.588671999999985,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",365,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.00368,20.592351999999984,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",366,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.003104,20.595455999999984,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",367,655360.0,23101440.0,0.0,0,0.0,23101440.0,23101440.0,170240.0,320.0,0.99812382739212,409600.0,81920.0,0.02528,20.620735999999983,17797120.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12800.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",368,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.126016,20.746751999999983,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",369,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.00272,20.749471999999983,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",370,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012928,20.76239999999998,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",371,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,427077888.0,327680.0,0.470016,21.232415999999983,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13346184.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",372,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002944,21.235359999999982,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",373,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.00304,21.23839999999998,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",374,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,21.24124799999998,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",375,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003296,21.24454399999998,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",376,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002912,21.24745599999998,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",377,452030.0,1129340.0,20480.0,0,0.0,1149820.0,1149820.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,21.25033599999998,81920.0,163840.0,441790.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",378,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,21.25321599999998,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",379,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.003296,21.25651199999998,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",380,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.492896,21.74940799999998,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",381,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002816,21.75222399999998,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",382,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.013024,21.765247999999982,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),383,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324304864.0,491520.0,0.352352,22.11759999999998,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134527.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",384,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006208,22.123807999999983,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",385,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.003744,22.127551999999984,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",386,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.003936,22.131487999999983,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",387,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.00304,22.13452799999998,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",388,655360.0,23101440.0,0.0,0,0.0,23101440.0,23101440.0,170240.0,320.0,0.99812382739212,409600.0,81920.0,0.025184,22.15971199999998,17797120.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12800.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",389,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.126048,22.285759999999982,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",390,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.00272,22.288479999999982,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",391,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012768,22.301247999999983,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",392,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,427348608.0,327680.0,0.470592,22.771839999999983,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13354644.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",393,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,22.774655999999982,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",394,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,22.77747199999998,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",395,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,22.780319999999982,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",396,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003168,22.78348799999998,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",397,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,22.78636799999998,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",398,451788.0,1128856.0,20480.0,0,0.0,1149336.0,1149336.0,0.0,1280.0,0.0,327680.0,327680.0,0.002912,22.78927999999998,81920.0,163840.0,441548.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",399,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,22.79215999999998,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",400,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.003232,22.795391999999982,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",401,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.498624,23.29401599999998,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",402,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002848,23.29686399999998,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",403,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012704,23.30956799999998,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),404,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324301024.0,491520.0,0.35376,23.663327999999982,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134407.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",405,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006208,23.669535999999983,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",406,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.00384,23.673375999999983,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",407,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.003648,23.67702399999998,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",408,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.00304,23.68006399999998,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",409,655360.0,23101440.0,0.0,0,0.0,23101440.0,23101440.0,170240.0,320.0,0.99812382739212,409600.0,81920.0,0.025248,23.70531199999998,17797120.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12800.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",410,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.125312,23.830623999999982,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",411,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002752,23.833375999999983,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",412,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012896,23.846271999999985,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",413,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,429214208.0,327680.0,0.46752,24.313791999999985,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13412944.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",414,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.003008,24.316799999999986,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",415,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,24.319615999999986,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",416,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002848,24.322463999999986,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",417,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003232,24.325695999999986,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",418,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002976,24.328671999999987,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",419,451815.0,1128910.0,20480.0,0,0.0,1149390.0,1149390.0,0.0,1280.0,0.0,327680.0,327680.0,0.002976,24.331647999999987,81920.0,163840.0,441575.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",420,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.00288,24.334527999999988,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",421,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.0032,24.337727999999988,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",422,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.494816,24.832543999999988,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",423,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002688,24.835231999999987,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",424,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.013088,24.848319999999987,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),425,2516643840.0,5034147840.0,122880.0,0,0.0,5034270720.0,5034270720.0,7437120.0,3840.0,0.9994839375564444,324300608.0,491520.0,0.353184,25.201503999999986,0.0,983040.0,2516582400.0,61440.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10134394.0,15360.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",426,368640.0,245760.0,737280.0,0,0.0,983040.0,983040.0,0.0,17280.0,0.0,552960.0,245760.0,0.006144,25.207647999999985,184320.0,61440.0,0.0,368640.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,17280.0,7680.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",427,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.003872,25.211519999999986,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",428,253952.0,0.0,507904.0,0,0.0,507904.0,507904.0,0.0,2560.0,0.0,163840.0,163840.0,0.003648,25.215167999999984,0.0,0.0,0.0,253952.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,5120.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",429,30720.0,0.0,61440.0,0,0.0,61440.0,61440.0,0.0,1280.0,0.0,81920.0,81920.0,0.00304,25.218207999999983,0.0,0.0,0.0,30720.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2560.0,2560.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",430,655360.0,23101440.0,0.0,0,0.0,23101440.0,23101440.0,170240.0,320.0,0.99812382739212,409600.0,81920.0,0.025408,25.24361599999998,17797120.0,3993600.0,655360.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,12800.0,2560.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",431,105103360.0,212254720.0,491520.0,0,0.0,212746240.0,212746240.0,924800.0,871680.0,0.5147844674029213,111431680.0,81920.0,0.12592,25.369535999999982,901120.0,1638400.0,104857600.0,245760.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3482240.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",432,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002752,25.372287999999983,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",433,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.013024,25.385311999999985,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_NN_kernel<float, 128, 2, 4, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",434,420331520.0,847216640.0,1802240.0,0,0.0,849018880.0,849018880.0,3590400.0,3485440.0,0.5074167872648335,426865152.0,327680.0,0.468192,25.853503999999983,1802240.0,6553600.0,419430400.0,901120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13339536.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",435,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,25.85628799999998,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",436,0.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002816,25.85910399999998,0.0,163840.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",437,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.003072,25.86217599999998,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",438,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,0.003168,25.86534399999998,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",439,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,0.002784,25.868127999999977,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",440,451871.0,1129022.0,20480.0,0,0.0,1149502.0,1149502.0,0.0,1280.0,0.0,327680.0,327680.0,0.002944,25.871071999999977,81920.0,163840.0,441631.0,10240.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",441,81920.0,163840.0,0.0,0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,0.002912,25.873983999999975,0.0,0.0,81920.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,10240.0,10240.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",442,0.0,81920.0,0.0,0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,0.003264,25.877247999999977,0.0,81920.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,20480.0,10240.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",443,420290560.0,848773120.0,1720320.0,0,0.0,850493440.0,850493440.0,3689600.0,3482880.0,0.5144106362095119,445665280.0,81920.0,0.49488,26.372127999999975,3358720.0,6553600.0,419430400.0,860160.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,13927040.0,2560.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",444,20480.0,40960.0,0.0,0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,0.002656,26.374783999999977,0.0,0.0,20480.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,5120.0,2560.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",445,114244.0,371956.0,3072.0,0,0.0,375028.0,375028.0,68.0,1768.0,0.037037037037037035,245760.0,82176.0,0.012896,26.38767999999998,113200.0,33340.0,112708.0,1536.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,7680.0,2568.0
"void gemmSN_TN_kernel<float, 128, 16, 2, 4, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",446,1046953936.0,2254747968.0,35381152.0,0,0.0,2290129120.0,2290129120.0,13494396.0,12162916.0,0.5259473790551403,1514372352.0,1181984.0,1.151328,27.539007999999978,67545408.0,128675840.0,1029263360.0,17690576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,47324136.0,36937.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",447,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,0.002048,27.541055999999976,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",448,262.0,0.0,524.0,0,0.0,524.0,524.0,0.0,6.0,0.0,96.0,160.0,0.002496,27.543551999999977,0.0,0.0,0.0,262.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,5.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",449,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,0.002464,27.546015999999977,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",450,0.0,201728.0,0.0,0,0.0,201728.0,201728.0,0.0,3158.0,0.0,804128.0,804128.0,0.003488,27.549503999999978,0.0,201728.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25129.0,25129.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",451,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,0.002016,27.55151999999998,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",452,302404.0,0.0,604808.0,0,0.0,604808.0,604808.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,0.00432,27.55583999999998,0.0,0.0,0.0,302404.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",453,172800.0,0.0,345600.0,0,0.0,345600.0,345600.0,13200.0,82608.0,0.1377755511022044,5029376.0,0.0,0.006144,27.561983999999978,0.0,0.0,0.0,172800.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",454,127835.0,0.0,255670.0,0,0.0,255670.0,255670.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,0.00432,27.566303999999977,0.0,0.0,0.0,127835.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",455,179200.0,0.0,358400.0,0,0.0,358400.0,358400.0,13200.0,82808.0,0.13748854262144822,5029376.0,0.0,0.00624,27.572543999999976,0.0,0.0,0.0,179200.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",456,101389.0,0.0,202778.0,0,0.0,202778.0,202778.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,0.004224,27.576767999999976,0.0,0.0,0.0,101389.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",457,188800.0,0.0,377600.0,0,0.0,377600.0,377600.0,13200.0,83108.0,0.13706026498317897,5029376.0,0.0,0.006688,27.583455999999977,0.0,0.0,0.0,188800.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",458,101381.0,0.0,202762.0,0,0.0,202762.0,202762.0,3200.0,9484.0,0.2522863450015768,811616.0,51200.0,0.004096,27.587551999999977,0.0,0.0,0.0,101381.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25363.0,1600.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",459,190400.0,0.0,380800.0,0,0.0,380800.0,380800.0,13200.0,83158.0,0.13698914464808318,5029376.0,128.0,0.006432,27.593983999999978,0.0,0.0,0.0,190400.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,157168.0,4.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",460,856.0,0.0,1712.0,0,0.0,1712.0,1712.0,0.0,21.0,0.0,6432.0,800.0,0.003104,27.597087999999978,0.0,0.0,0.0,856.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,201.0,25.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",461,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,0.002048,27.599135999999977,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",462,576.0,0.0,1152.0,0,0.0,1152.0,1152.0,636.0,23.0,0.9650986342943855,800.0,0.0,0.004608,27.603743999999978,0.0,0.0,0.0,576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",463,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,0.002048,27.605791999999976,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",464,576.0,0.0,1152.0,0,0.0,1152.0,1152.0,636.0,23.0,0.9650986342943855,800.0,0.0,0.004448,27.610239999999976,0.0,0.0,0.0,576.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",465,253328.0,0.0,506656.0,0,0.0,506656.0,506656.0,51209.0,13018.0,0.7973126566708705,829920.0,8448.0,0.006272,27.616511999999975,0.0,0.0,0.0,253328.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25935.0,264.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",466,512.0,0.0,1024.0,0,0.0,1024.0,1024.0,1832.0,32.0,0.9828326180257511,2560.0,0.0,0.006304,27.622815999999975,0.0,0.0,0.0,512.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,80.0,0.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",467,402056.0,0.0,804112.0,0,0.0,804112.0,804112.0,0.0,18849.0,0.0,814528.0,50272.0,0.004096,27.626911999999976,0.0,0.0,0.0,402056.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25454.0,1571.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",468,25216.0,0.0,50432.0,0,0.0,50432.0,50432.0,0.0,4737.0,0.0,1005184.0,0.0,0.00384,27.630751999999976,0.0,0.0,0.0,25216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,31412.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",469,603784.0,0.0,1207568.0,0,0.0,1207568.0,1207568.0,0.0,6283.0,0.0,0.0,1608224.0,0.003648,27.634399999999975,0.0,0.0,0.0,603784.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,50257.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",470,388048.0,0.0,776096.0,0,0.0,776096.0,776096.0,64512.0,6283.0,0.9112507945476376,804128.0,0.0,0.00448,27.638879999999975,0.0,0.0,0.0,388048.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25129.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",471,0.0,0.0,0.0,0,0.0,0.0,0.0,128.0,64.0,0.6666666666666666,8192.0,0.0,0.002624,27.641503999999976,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",472,189312.0,0.0,378624.0,0,0.0,378624.0,378624.0,65758.0,28356.0,0.698705824850713,2724544.0,1832992.0,0.016832,27.658335999999977,0.0,0.0,0.0,189312.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,85142.0,57281.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",473,62276.0,0.0,124552.0,0,0.0,124552.0,124552.0,14202.0,28318.0,0.33400752587017873,2719296.0,2451392.0,0.013696,27.672031999999977,0.0,0.0,0.0,62276.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,84978.0,76606.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",474,64896.0,0.0,129792.0,0,0.0,129792.0,129792.0,15214.0,28262.0,0.3499401968902383,2715328.0,2225760.0,0.014624,27.686655999999978,0.0,0.0,0.0,64896.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,84854.0,69555.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",475,64896.0,0.0,129792.0,0,0.0,129792.0,129792.0,15214.0,28219.0,0.35028664840098545,2718912.0,2343552.0,0.014912,27.701567999999977,0.0,0.0,0.0,64896.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,84966.0,73236.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",476,301056.0,0.0,602112.0,0,0.0,602112.0,602112.0,9408.0,6283.0,0.5995793767127653,1608224.0,0.0,0.004832,27.706399999999977,0.0,0.0,0.0,301056.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",477,0.0,0.0,0.0,0,0.0,0.0,0.0,32.0,16.0,0.6666666666666666,2048.0,0.0,0.002656,27.70905599999998,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",478,41123.0,0.0,82246.0,0,0.0,82246.0,82246.0,14849.0,15100.0,0.49580954288957896,1875104.0,1285696.0,0.008832,27.71788799999998,0.0,0.0,0.0,41123.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,58597.0,40178.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",479,603272.0,0.0,1206544.0,0,0.0,1206544.0,1206544.0,0.0,25132.0,0.0,2429408.0,2412352.0,0.006112,27.723999999999982,0.0,0.0,0.0,603272.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,75919.0,75386.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",480,3284156.0,6655044.0,939528.0,0,0.0,7594572.0,7594572.0,528.0,6704.0,0.07300884955752213,2278656.0,751104.0,0.02544,27.749439999999982,825232.0,201028.0,2814392.0,469764.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,71208.0,23472.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",481,210944.0,1024200.0,421888.0,0,0.0,1446088.0,1446088.0,112284.0,12568.0,0.8993368147887099,804448.0,581344.0,0.078016,27.827455999999984,1024200.0,0.0,0.0,210944.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25139.0,18167.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",482,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,3158.0,0.0,804128.0,200800.0,0.003392,27.830847999999985,0.0,0.0,0.0,256.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25129.0,6275.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",483,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,128.0,0.002112,27.832959999999986,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,4.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",484,603084.0,0.0,1206168.0,0,0.0,1206168.0,1206168.0,0.0,18849.0,0.0,1809280.0,76928.0,0.00912,27.842079999999985,0.0,0.0,0.0,603084.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,56540.0,2404.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",485,25216.0,0.0,50432.0,0,0.0,50432.0,50432.0,0.0,4737.0,0.0,1005184.0,0.0,0.003872,27.845951999999986,0.0,0.0,0.0,25216.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,31412.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",486,3284173.0,6655044.0,939562.0,0,0.0,7594606.0,7594606.0,528.0,6704.0,0.07300884955752213,2282240.0,750976.0,0.025472,27.871423999999987,825232.0,201028.0,2814392.0,469781.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,71320.0,23468.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",487,39936.0,0.0,79872.0,0,0.0,79872.0,79872.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,0.006592,27.878015999999988,0.0,0.0,0.0,39936.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",488,129.0,0.0,258.0,0,0.0,258.0,258.0,0.0,2.0,0.0,32.0,32.0,0.002432,27.880447999999987,0.0,0.0,0.0,129.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",489,39936.0,0.0,79872.0,0,0.0,79872.0,79872.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,0.006752,27.887199999999986,0.0,0.0,0.0,39936.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",490,129.0,0.0,258.0,0,0.0,258.0,258.0,0.0,2.0,0.0,32.0,32.0,0.002496,27.889695999999986,0.0,0.0,0.0,129.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",491,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,3.0,0.0,64.0,32.0,0.002528,27.892223999999988,0.0,0.0,0.0,2.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",492,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,0.003264,27.89548799999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",493,8192.0,220484.0,16384.0,0,0.0,236868.0,236868.0,320.0,1582.0,0.16824395373291273,804224.0,128.0,0.013472,27.90895999999999,220484.0,0.0,0.0,8192.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25132.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",494,132.0,0.0,264.0,0,0.0,264.0,264.0,0.0,2.0,0.0,32.0,32.0,0.002432,27.91139199999999,0.0,0.0,0.0,132.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",495,12.0,0.0,24.0,0,0.0,24.0,24.0,0.0,2.0,0.0,32.0,32.0,0.003296,27.914687999999988,0.0,0.0,0.0,12.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",496,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,2.0,0.0,32.0,32.0,0.0024,27.91708799999999,0.0,0.0,0.0,2.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",497,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,0.0032,27.92028799999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",498,1806336.0,2724488.0,1290240.0,0,0.0,4014728.0,4014728.0,0.0,6283.0,0.0,0.0,804128.0,0.004096,27.92438399999999,0.0,402056.0,1161216.0,645120.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,25129.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",499,1210569.0,2017280.0,403858.0,0,0.0,2421138.0,2421138.0,0.0,4737.0,0.0,1608256.0,0.0,0.005248,27.92963199999999,0.0,0.0,1008640.0,201929.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,50258.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",500,110754.0,0.0,221508.0,0,0.0,221508.0,221508.0,640.0,1582.0,0.28802880288028804,804384.0,128.0,0.017696,27.94732799999999,0.0,0.0,0.0,110754.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,25137.0,4.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",501,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,0.002432,27.94975999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",502,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,0.002464,27.95222399999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",503,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,0.002624,27.95484799999999,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",504,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,0.002496,27.957343999999992,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",505,262.0,0.0,524.0,0,0.0,524.0,524.0,0.0,6.0,0.0,96.0,160.0,0.002496,27.959839999999993,0.0,0.0,0.0,262.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,3.0,5.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",506,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,32.0,0.002016,27.961855999999994,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",507,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,32.0,0.002048,27.963903999999992,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",508,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,0.002496,27.966399999999993,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",509,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,32.0,0.002048,27.96844799999999,0.0,0.0,0.0,4.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",510,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,128.0,32.0,0.002528,27.970975999999993,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,4.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",511,5.0,0.0,10.0,0,0.0,10.0,10.0,0.0,5.0,0.0,32.0,32.0,0.0056,27.976575999999994,0.0,0.0,0.0,5.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",512,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,0.002464,27.979039999999994,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",513,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,0.0024,27.981439999999996,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",514,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,0.003072,27.984511999999995,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",515,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,0.0032,27.987711999999995,0.0,0.0,0.0,8.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",516,1.0,0.0,2.0,0,0.0,2.0,2.0,0.0,2.0,0.0,32.0,32.0,0.0024,27.990111999999996,0.0,0.0,0.0,1.0,0,0,0,0,0,0.0,0.0,0.0,0,0.0,1.0,1.0
