Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.856,1.856,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",2,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,3.552,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",3,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,5.28,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",4,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.016,7.296,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",5,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.336,9.632,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",6,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.368,12.0,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::unrolled_elementwise_kernel<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)], std::array<char *, 2>, 8, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<1>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",7,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.296,15.296,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::sum_functor<long, long, long>::operator ()(at::TensorIterator &)::[lambda(long, long) (instance 1)]>, unsigned int, long, 4, 4>>(T3)",8,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.168,18.464,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",9,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.176,20.64,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",10,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,22.336000000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",11,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,24.064000000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void at_cuda_detail::DeviceScanInitKernel<at_cuda_detail::ScanTileState<long, 1>>(T1, int)",12,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.76,25.824000000000005,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanKernel<at_cuda_detail::DeviceScanPolicy<long, std::plus<long>>::Policy900, const long *, long *, at_cuda_detail::ScanTileState<long, 1>, std::plus<long>, at_cuda_detail::NullType, unsigned int, long, 0>(T2, T3, T4, int, T5, T6, T7)",13,0.0,0.0,0,0,0.0,0.0,0.0,129.0,6.0,0.9555555555555556,32.0,32.0,2.464,28.288000000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",14,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,1.984,30.272000000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",15,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.208,32.480000000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",16,0.0,0.0,0,0,0.0,0.0,0.0,56.0,4.0,0.9333333333333333,32.0,32.0,2.496,34.976000000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",17,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,36.992000000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",18,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,39.040000000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",19,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,0.0,2.112,41.15200000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,0.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",20,0.0,0.0,0,0,0.0,0.0,0.0,0.0,288.0,0.0,3264.0,12288.0,3.84,44.992000000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,102.0,384.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",21,0.0,0.0,0,0,0.0,0.0,0.0,0.0,288.0,0.0,3264.0,12288.0,3.776,48.768,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,102.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",22,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.048,50.816,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",23,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,52.832,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",24,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.328,56.160000000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",25,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.256,60.416000000000004,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,26,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,11.936,72.352,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",27,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.592,74.944,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",28,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.848,77.792,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",29,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.688,80.48,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",30,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.592,83.072,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",31,98304.0,3265536.0,0,0,0.0,3265536.0,3265536.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,12.896,95.968,2469888.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,32,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682592.0,122880.0,7.264,103.232,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83831.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",33,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,2.912,106.144,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",34,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,108.256,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",35,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.192,112.44800000000001,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),36,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,15.232,127.68,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",37,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.848,130.52800000000002,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",38,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.016,132.544,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",39,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.016,134.56,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",40,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.176,136.736,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",41,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.208,138.944,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",42,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.112,141.05599999999998,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",43,71532.0,179928.0,0,0,0.0,179928.0,179928.0,0.0,192.0,0.0,49152.0,49152.0,2.176,143.23199999999997,12288.0,24576.0,71532.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",44,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.016,145.24799999999996,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",45,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.144,147.39199999999997,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,46,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10659936.0,86016.0,15.648,163.03999999999996,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333123.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",47,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.88,165.91999999999996,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",48,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.048,167.96799999999996,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",49,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.16,172.12799999999996,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,50,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.096,184.22399999999996,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",51,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.592,186.81599999999997,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",52,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.624,189.43999999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",53,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.816,192.25599999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",54,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.688,194.94399999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",55,98304.0,3265536.0,0,0,0.0,3265536.0,3265536.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,11.584,206.52799999999996,2469888.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,56,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682464.0,122880.0,7.296,213.82399999999996,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83827.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",57,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,3.104,216.92799999999997,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",58,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,219.03999999999996,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",59,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.224,223.26399999999995,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),60,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,14.688,237.95199999999994,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",61,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.72,240.67199999999994,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",62,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,242.71999999999994,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",63,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.016,244.73599999999993,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",64,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,246.78399999999993,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",65,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.144,248.92799999999994,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",66,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.08,251.00799999999995,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",67,71436.0,179736.0,0,0,0.0,179736.0,179736.0,0.0,192.0,0.0,49152.0,49152.0,2.176,253.18399999999994,12288.0,24576.0,71436.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",68,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.048,255.23199999999994,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",69,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.144,257.3759999999999,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,70,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10658912.0,86016.0,15.68,273.0559999999999,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333091.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",71,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.88,275.9359999999999,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",72,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.048,277.9839999999999,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",73,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.224,282.2079999999999,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,74,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.256,294.46399999999994,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",75,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.56,297.02399999999994,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",76,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.624,299.64799999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",77,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.656,302.304,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",78,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.688,304.99199999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",79,98304.0,3265536.0,0,0,0.0,3265536.0,3265536.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,11.84,316.83199999999994,2469888.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,80,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682144.0,122880.0,7.2,324.0319999999999,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83817.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",81,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,2.944,326.97599999999994,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",82,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.208,329.18399999999997,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",83,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.224,333.40799999999996,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),84,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,14.624,348.032,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",85,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.88,350.912,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",86,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,352.96,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",87,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.016,354.976,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",88,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.016,356.992,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",89,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.144,359.136,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",90,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,361.184,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",91,71276.0,179416.0,0,0,0.0,179416.0,179416.0,0.0,192.0,0.0,49152.0,49152.0,2.208,363.39200000000005,12288.0,24576.0,71276.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",92,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.016,365.4080000000001,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",93,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.176,367.58400000000006,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,94,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10656736.0,86016.0,16.0,383.58400000000006,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333023.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",95,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.912,386.49600000000004,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",96,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.048,388.54400000000004,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",97,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.224,392.76800000000003,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,98,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.0,404.76800000000003,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",99,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.592,407.36,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",100,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.592,409.952,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",101,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.688,412.64,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",102,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.624,415.264,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",103,98304.0,3265536.0,0,0,0.0,3265536.0,3265536.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,11.616,426.88,2469888.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,104,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682048.0,122880.0,7.328,434.20799999999997,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83814.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",105,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,2.976,437.18399999999997,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",106,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,439.296,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",107,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.288,443.584,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),108,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,14.432,458.016,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",109,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.72,460.73600000000005,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",110,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,462.78400000000005,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",111,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.048,464.83200000000005,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",112,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.08,466.91200000000003,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",113,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.272,469.184,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",114,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.112,471.29600000000005,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",115,71508.0,179880.0,0,0,0.0,179880.0,179880.0,0.0,192.0,0.0,49152.0,49152.0,2.176,473.47200000000004,12288.0,24576.0,71508.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",116,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.112,475.58400000000006,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",117,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.208,477.7920000000001,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,118,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10660000.0,86016.0,15.744,493.53600000000006,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333125.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",119,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.848,496.38400000000007,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",120,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.08,498.46400000000006,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",121,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.352,502.81600000000003,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,122,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.352,515.168,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",123,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.592,517.76,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",124,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.624,520.384,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",125,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.656,523.04,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",126,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.784,525.824,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",127,98304.0,3265536.0,0,0,0.0,3265536.0,3265536.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,11.616,537.4399999999999,2469888.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,128,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2683008.0,122880.0,7.392,544.832,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83844.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",129,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,3.04,547.872,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",130,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,549.9839999999999,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",131,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.16,554.1439999999999,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),132,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,14.688,568.8319999999999,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",133,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.848,571.6799999999998,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",134,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,573.7279999999998,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",135,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.08,575.8079999999999,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",136,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,577.8559999999999,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",137,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.144,579.9999999999999,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",138,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.112,582.1119999999999,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",139,71296.0,179456.0,0,0,0.0,179456.0,179456.0,0.0,192.0,0.0,49152.0,49152.0,2.176,584.2879999999999,12288.0,24576.0,71296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",140,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.048,586.3359999999999,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",141,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.304,588.6399999999999,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,142,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10658816.0,86016.0,16.256,604.8959999999998,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333088.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",143,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.848,607.7439999999998,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",144,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.08,609.8239999999998,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",145,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.224,614.0479999999999,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,146,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.608,626.6559999999998,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",147,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.656,629.3119999999998,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",148,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.656,631.9679999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",149,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.688,634.6559999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",150,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.656,637.3119999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",151,98304.0,3265536.0,0,0,0.0,3265536.0,3265536.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,11.552,648.8639999999997,2469888.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,152,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682784.0,122880.0,7.168,656.0319999999997,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83837.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",153,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,3.008,659.0399999999997,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",154,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,661.1519999999997,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",155,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.192,665.3439999999997,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),156,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,14.656,679.9999999999997,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",157,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.688,682.6879999999996,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",158,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,684.7359999999996,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",159,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.208,686.9439999999996,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",160,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,688.9919999999996,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",161,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.176,691.1679999999997,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",162,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.08,693.2479999999997,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",163,71336.0,179536.0,0,0,0.0,179536.0,179536.0,0.0,192.0,0.0,49152.0,49152.0,2.176,695.4239999999998,12288.0,24576.0,71336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",164,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.048,697.4719999999998,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",165,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.176,699.6479999999998,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,166,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10658912.0,86016.0,15.52,715.1679999999998,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333091.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",167,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.88,718.0479999999998,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",168,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.08,720.1279999999998,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",169,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.16,724.2879999999998,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,170,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.544,736.8319999999998,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",171,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.592,739.4239999999998,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",172,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.688,742.1119999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",173,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.656,744.7679999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",174,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.656,747.4239999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",175,98304.0,3265536.0,0,0,0.0,3265536.0,3265536.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,11.552,758.9759999999997,2469888.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,176,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682944.0,122880.0,7.264,766.2399999999997,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83842.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",177,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,2.944,769.1839999999996,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",178,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,771.2959999999996,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",179,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.16,775.4559999999996,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),180,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,15.104,790.5599999999996,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",181,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.88,793.4399999999996,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",182,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,795.4879999999996,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",183,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.016,797.5039999999996,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",184,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,799.5519999999996,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",185,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.144,801.6959999999996,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",186,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.112,803.8079999999995,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",187,71396.0,179656.0,0,0,0.0,179656.0,179656.0,0.0,192.0,0.0,49152.0,49152.0,2.144,805.9519999999995,12288.0,24576.0,71396.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",188,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.048,807.9999999999995,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",189,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.208,810.2079999999995,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,190,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10657984.0,86016.0,15.904,826.1119999999995,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333062.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",191,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.912,829.0239999999995,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",192,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.08,831.1039999999996,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",193,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.16,835.2639999999996,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,194,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.128,847.3919999999996,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",195,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.624,850.0159999999996,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",196,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.688,852.7039999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",197,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.688,855.3919999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",198,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.624,858.0159999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",199,98304.0,3265536.0,0,0,0.0,3265536.0,3265536.0,25536.0,48.0,0.99812382739212,36864.0,12288.0,11.584,869.5999999999996,2469888.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,200,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2681536.0,122880.0,7.328,876.9279999999995,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83798.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",201,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,2.976,879.9039999999995,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",202,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,882.0159999999995,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",203,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.192,886.2079999999995,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),204,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,14.496,900.7039999999995,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",205,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.88,903.5839999999995,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",206,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.016,905.5999999999995,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",207,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.016,907.6159999999994,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",208,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,909.6639999999994,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",209,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.144,911.8079999999994,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",210,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.08,913.8879999999995,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",211,71424.0,179712.0,0,0,0.0,179712.0,179712.0,0.0,192.0,0.0,49152.0,49152.0,2.176,916.0639999999995,12288.0,24576.0,71424.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",212,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.048,918.1119999999995,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",213,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.24,920.3519999999995,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,214,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10658112.0,86016.0,15.68,936.0319999999995,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333066.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",215,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.944,938.9759999999994,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",216,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,941.0879999999994,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",217,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.16,945.2479999999994,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_64x32_sliced1x4_tn,218,1236271104.0,2483810304.0,0,0,0.0,2483810304.0,2483810304.0,5810112.0,1287908.0,0.818553906582399,159252992.0,767168.0,188.128,1133.3759999999993,4829184.0,6438912.0,1236271104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4976656.0,23974.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",219,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,1135.0719999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",220,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,64.0,128.0,2.56,1137.6319999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,4.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",221,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,1139.6799999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",222,0.0,201028.0,0,0,0.0,201028.0,201028.0,0.0,3158.0,0.0,804128.0,804128.0,3.04,1142.7199999999991,0.0,201028.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,25129.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",223,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,1144.415999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",224,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,56832.0,4.0,1148.415999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1776.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",225,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,82608.0,0.15193823915900131,5134592.0,0.0,5.728,1154.143999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",226,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,56128.0,4.0,1158.143999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1754.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",227,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83808.0,0.15008924225214992,5134592.0,0.0,5.632,1163.7759999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",228,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,55808.0,3.712,1167.4879999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1744.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",229,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83608.0,0.15039427688805787,5134592.0,0.0,5.76,1173.2479999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",230,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,56576.0,3.968,1177.2159999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1768.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",231,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83208.0,0.1510080809729818,5134592.0,128.0,5.76,1182.9759999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,4.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",232,0.0,0.0,0,0,0.0,0.0,0.0,0.0,21.0,0.0,6432.0,800.0,2.432,1185.4079999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,201.0,25.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",233,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.728,1187.1359999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",234,0.0,0.0,0,0,0.0,0.0,0.0,497.0,22.0,0.9576107899807321,800.0,0.0,3.456,1190.5919999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",235,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.728,1192.3199999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",236,0.0,0.0,0,0,0.0,0.0,0.0,497.0,22.0,0.9576107899807321,800.0,0.0,3.488,1195.8079999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",237,0.0,0.0,0,0,0.0,0.0,0.0,52032.0,13012.0,0.7999508025336696,831456.0,8704.0,5.856,1201.6639999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25983.0,272.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",238,0.0,0.0,0,0,0.0,0.0,0.0,1832.0,32.0,0.9828326180257511,2560.0,544.0,6.368,1208.0319999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,80.0,17.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",239,0.0,0.0,0,0,0.0,0.0,0.0,0.0,18849.0,0.0,814496.0,86976.0,3.936,1211.9679999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25453.0,2718.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",240,0.0,0.0,0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,3.488,1215.4559999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,31412.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",241,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6283.0,0.0,0.0,1608224.0,2.688,1218.1439999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,50257.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",242,0.0,0.0,0,0,0.0,0.0,0.0,98304.0,6283.0,0.9399256121697726,804128.0,0.0,4.288,1222.4319999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",243,0.0,0.0,0,0,0.0,0.0,0.0,156.0,64.0,0.7090909090909091,8192.0,0.0,2.144,1224.5759999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",244,0.0,0.0,0,0,0.0,0.0,0.0,78783.0,29619.0,0.7267670338185642,3061568.0,2019264.0,10.304,1234.8799999999994,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,95674.0,63102.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",245,0.0,0.0,0,0,0.0,0.0,0.0,23142.0,36041.0,0.3910244495885643,3104064.0,2479936.0,8.288,1243.1679999999994,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97002.0,77498.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",246,0.0,0.0,0,0,0.0,0.0,0.0,23883.0,35844.0,0.3998694057963735,3113792.0,2479936.0,9.12,1252.2879999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97306.0,77498.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",247,0.0,0.0,0,0,0.0,0.0,0.0,23883.0,35843.0,0.3998761008605967,3092288.0,1890112.0,8.704,1260.9919999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,96634.0,59066.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",248,0.0,0.0,0,0,0.0,0.0,0.0,14336.0,6283.0,0.6952810514573937,1608224.0,0.0,4.416,1265.4079999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",249,0.0,0.0,0,0,0.0,0.0,0.0,39.0,16.0,0.7090909090909091,2048.0,0.0,2.208,1267.6159999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",250,0.0,0.0,0,0,0.0,0.0,0.0,20059.0,17950.0,0.5277434291878239,2096544.0,1399584.0,8.0,1275.6159999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,65517.0,43737.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",251,0.0,0.0,0,0,0.0,0.0,0.0,0.0,25132.0,0.0,2427680.0,2412352.0,5.216,1280.8319999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,75865.0,75386.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",252,2814392.0,6655044.0,0,0,0.0,6655044.0,6655044.0,528.0,6704.0,0.07300884955752213,2280448.0,752896.0,24.128,1304.9599999999991,825232.0,201028.0,2814392.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,71264.0,23528.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",253,0.0,1024200.0,0,0,0.0,1024200.0,1024200.0,112284.0,12568.0,0.8993368147887099,804480.0,627776.0,72.16,1377.1199999999992,1024200.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25140.0,19618.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",254,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3158.0,0.0,804128.0,200800.0,2.848,1379.9679999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,6275.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",255,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,128.0,1.92,1381.8879999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,4.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",256,0.0,0.0,0,0,0.0,0.0,0.0,0.0,18849.0,0.0,1809280.0,85664.0,7.872,1389.7599999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,56540.0,2677.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",257,0.0,0.0,0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,3.232,1392.9919999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,31412.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",258,2814392.0,6655044.0,0,0,0.0,6655044.0,6655044.0,528.0,6704.0,0.07300884955752213,2280320.0,753152.0,23.84,1416.8319999999992,825232.0,201028.0,2814392.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,71260.0,23536.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",259,0.0,0.0,0,0,0.0,0.0,0.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.112,1422.9439999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",260,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.08,1425.0239999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",261,0.0,0.0,0,0,0.0,0.0,0.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.304,1431.3279999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",262,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.176,1433.5039999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",263,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.112,1435.6159999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",264,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.976,1438.5919999999994,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",265,0.0,220484.0,0,0,0.0,220484.0,220484.0,320.0,1582.0,0.16824395373291273,804256.0,128.0,10.944,1449.5359999999994,220484.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",266,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,1451.5519999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",267,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.2,1454.7519999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",268,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,1456.7679999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",269,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.976,1459.7439999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",270,1769472.0,3941000.0,0,0,0.0,3941000.0,3941000.0,0.0,6283.0,0.0,0.0,804128.0,4.0,1463.7439999999997,0.0,402056.0,1769472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,25129.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",271,1005140.0,2010280.0,0,0,0.0,2010280.0,2010280.0,0.0,4737.0,0.0,1608256.0,0.0,4.768,1468.5119999999997,0.0,0.0,1005140.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50258.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",272,0.0,0.0,0,0,0.0,0.0,0.0,640.0,1582.0,0.28802880288028804,804544.0,128.0,16.128,1484.6399999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25142.0,4.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",273,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.016,1486.6559999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",274,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,1488.7039999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",275,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.24,1490.9439999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",276,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.016,1492.9599999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",277,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,64.0,128.0,2.56,1495.5199999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,4.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",278,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,1497.2159999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",279,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.76,1498.9759999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",280,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.208,1501.1839999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",281,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,1502.8799999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",282,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,32.0,2.336,1505.2159999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",283,0.0,0.0,0,0,0.0,0.0,0.0,0.0,5.0,0.0,32.0,32.0,4.704,1509.9199999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",284,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.048,1511.9679999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",285,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,1514.0159999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",286,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.848,1516.8639999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",287,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.2,1520.0639999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",288,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,1522.1119999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",289,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.272,1524.3839999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::index_elementwise_kernel<128, 4, void native::gpu_index_kernel<void native::index_kernel_impl<native::OpaqueType<8>>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>, const T1 &)::[lambda(int) (instance 1)]>(long, T3)",290,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,32.0,3.392,1527.7759999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",291,0.0,0.0,0,0,0.0,0.0,0.0,56.0,4.0,0.9333333333333333,64.0,32.0,2.912,1530.6879999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",292,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,64.0,2.336,1533.0239999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,2.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",293,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,2.368,1535.3919999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",294,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,0.0,2.368,1537.7599999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,0.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",295,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,2.496,1540.2559999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",296,0.0,0.0,0,0,0.0,0.0,0.0,0.0,288.0,0.0,12480.0,12288.0,5.888,1546.1439999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,390.0,384.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",297,0.0,0.0,0,0,0.0,0.0,0.0,0.0,288.0,0.0,3264.0,12288.0,3.744,1549.8879999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,102.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",298,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.048,1551.9359999999995,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",299,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,2.048,1553.9839999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",300,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.68,1557.6639999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",301,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.192,1561.8559999999995,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,302,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,11.968,1573.8239999999996,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",303,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.528,1576.3519999999996,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",304,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,4.288,1580.6399999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",305,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,3.968,1584.6079999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",306,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.656,1587.2639999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",307,98304.0,3268608.0,0,0,0.0,3268608.0,3268608.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,12.384,1599.6479999999997,2472960.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1920.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,308,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2681696.0,122880.0,7.328,1606.9759999999997,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83803.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",309,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,2.944,1609.9199999999996,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",310,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,1612.0319999999997,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",311,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.192,1616.2239999999997,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),312,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,15.52,1631.7439999999997,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",313,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.656,1634.3999999999996,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",314,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.336,1636.7359999999996,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",315,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.08,1638.8159999999996,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",316,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,1640.8639999999996,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",317,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.144,1643.0079999999996,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",318,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,1645.0559999999996,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",319,71471.0,179806.0,0,0,0.0,179806.0,179806.0,0.0,192.0,0.0,49152.0,49152.0,2.144,1647.1999999999996,12288.0,24576.0,71471.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",320,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.144,1649.3439999999996,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",321,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.112,1651.4559999999997,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,322,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10660256.0,86016.0,15.872,1667.3279999999997,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333133.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",323,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.88,1670.2079999999999,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",324,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.08,1672.2879999999998,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",325,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.224,1676.5119999999997,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,326,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.32,1688.8319999999997,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",327,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.528,1691.3599999999997,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",328,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,4.032,1695.3919999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",329,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,3.968,1699.3599999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",330,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.624,1701.9839999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",331,98304.0,3268608.0,0,0,0.0,3268608.0,3268608.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,11.424,1713.4079999999997,2472960.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1920.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,332,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682880.0,122880.0,7.296,1720.7039999999997,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83840.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",333,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,2.944,1723.6479999999997,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",334,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,1725.7599999999998,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",335,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.224,1729.9839999999997,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),336,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,14.368,1744.3519999999996,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",337,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.624,1746.9759999999997,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",338,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.016,1748.9919999999997,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",339,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.048,1751.0399999999997,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",340,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.144,1753.1839999999997,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",341,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.208,1755.3919999999998,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",342,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.08,1757.4719999999998,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",343,71448.0,179760.0,0,0,0.0,179760.0,179760.0,0.0,192.0,0.0,49152.0,49152.0,2.176,1759.6479999999997,12288.0,24576.0,71448.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",344,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.176,1761.8239999999996,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",345,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.112,1763.9359999999997,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,346,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10658944.0,86016.0,15.808,1779.7439999999997,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333092.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",347,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.848,1782.5919999999996,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",348,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.08,1784.6719999999996,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",349,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.256,1788.9279999999997,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,350,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.16,1801.0879999999997,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",351,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.528,1803.6159999999998,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",352,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,4.0,1807.6159999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",353,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,4.0,1811.6159999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",354,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.752,1814.3679999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",355,98304.0,3268608.0,0,0,0.0,3268608.0,3268608.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,11.552,1825.9199999999996,2472960.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1920.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,356,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682432.0,122880.0,7.264,1833.1839999999995,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83826.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",357,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,2.912,1836.0959999999995,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",358,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,1838.2079999999996,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",359,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.16,1842.3679999999997,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),360,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,14.72,1857.0879999999997,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",361,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.624,1859.7119999999998,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",362,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,1861.7599999999998,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",363,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.016,1863.7759999999998,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",364,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,1865.8239999999998,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",365,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.176,1867.9999999999998,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",366,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.08,1870.0799999999997,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",367,71351.0,179566.0,0,0,0.0,179566.0,179566.0,0.0,192.0,0.0,49152.0,49152.0,2.176,1872.2559999999996,12288.0,24576.0,71351.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",368,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.08,1874.3359999999996,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",369,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.112,1876.4479999999996,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,370,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10660096.0,86016.0,15.488,1891.9359999999997,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333128.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",371,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.848,1894.7839999999997,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",372,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.08,1896.8639999999996,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",373,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.256,1901.1199999999997,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,374,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.384,1913.5039999999997,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",375,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.656,1916.1599999999996,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",376,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,4.032,1920.1919999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",377,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,3.968,1924.1599999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",378,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.624,1926.7839999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",379,98304.0,3268608.0,0,0,0.0,3268608.0,3268608.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,11.392,1938.1759999999997,2472960.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1920.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,380,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682208.0,122880.0,7.264,1945.4399999999996,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83819.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",381,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,3.072,1948.5119999999995,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",382,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.08,1950.5919999999994,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",383,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.224,1954.8159999999993,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),384,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,14.304,1969.1199999999994,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",385,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.592,1971.7119999999995,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",386,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,1.984,1973.6959999999995,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",387,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.016,1975.7119999999995,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",388,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.08,1977.7919999999995,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",389,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.304,1980.0959999999995,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",390,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,1982.1439999999996,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",391,71511.0,179886.0,0,0,0.0,179886.0,179886.0,0.0,192.0,0.0,49152.0,49152.0,2.144,1984.2879999999996,12288.0,24576.0,71511.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",392,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.08,1986.3679999999995,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",393,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.112,1988.4799999999996,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,394,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10657824.0,86016.0,15.648,2004.1279999999995,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333057.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",395,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.88,2007.0079999999996,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",396,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,2009.1199999999997,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",397,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.256,2013.3759999999997,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,398,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.288,2025.6639999999998,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",399,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.496,2028.1599999999999,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",400,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,4.128,2032.2879999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",401,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,3.968,2036.2559999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",402,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.688,2038.944,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",403,98304.0,3268608.0,0,0,0.0,3268608.0,3268608.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,11.488,2050.432,2472960.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1920.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,404,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682112.0,122880.0,7.456,2057.888,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83816.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",405,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,2.976,2060.864,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",406,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,2062.976,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",407,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.16,2067.136,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),408,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,14.784,2081.92,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",409,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.848,2084.768,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",410,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,2086.816,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",411,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.144,2088.9599999999996,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",412,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.208,2091.1679999999997,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",413,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.144,2093.3119999999994,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",414,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.08,2095.3919999999994,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",415,71297.0,179458.0,0,0,0.0,179458.0,179458.0,0.0,192.0,0.0,49152.0,49152.0,2.176,2097.5679999999993,12288.0,24576.0,71297.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",416,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.048,2099.615999999999,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",417,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.24,2101.855999999999,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,418,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10660192.0,86016.0,15.744,2117.599999999999,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333131.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",419,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.976,2120.575999999999,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",420,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.144,2122.719999999999,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",421,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.256,2126.9759999999987,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,422,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.064,2139.0399999999986,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",423,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.528,2141.5679999999984,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",424,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,3.968,2145.5359999999982,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",425,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,4.0,2149.5359999999982,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",426,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.624,2152.159999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",427,98304.0,3268608.0,0,0,0.0,3268608.0,3268608.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,11.424,2163.583999999998,2472960.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1920.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,428,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682464.0,122880.0,7.328,2170.911999999998,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83827.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",429,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,2.944,2173.855999999998,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",430,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.08,2175.935999999998,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",431,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.224,2180.159999999998,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),432,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,14.4,2194.559999999998,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",433,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.624,2197.183999999998,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",434,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.016,2199.199999999998,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",435,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.016,2201.215999999998,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",436,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.112,2203.327999999998,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",437,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.176,2205.503999999998,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",438,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.08,2207.583999999998,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",439,71421.0,179706.0,0,0,0.0,179706.0,179706.0,0.0,192.0,0.0,49152.0,49152.0,2.144,2209.727999999998,12288.0,24576.0,71421.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",440,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.048,2211.7759999999976,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",441,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.08,2213.8559999999975,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,442,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10657888.0,86016.0,15.328,2229.1839999999975,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333059.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",443,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,3.04,2232.2239999999974,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",444,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.08,2234.3039999999974,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",445,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.192,2238.4959999999974,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,446,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.288,2250.7839999999974,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",447,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.496,2253.2799999999975,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",448,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,4.032,2257.3119999999976,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",449,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,3.936,2261.247999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",450,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.592,2263.839999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",451,98304.0,3268608.0,0,0,0.0,3268608.0,3268608.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,11.68,2275.5199999999977,2472960.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1920.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,452,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682912.0,122880.0,7.264,2282.783999999998,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83841.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",453,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,2.944,2285.727999999998,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",454,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.08,2287.8079999999977,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",455,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.256,2292.0639999999976,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),456,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,15.072,2307.1359999999977,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",457,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.656,2309.7919999999976,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",458,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.016,2311.8079999999977,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",459,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.048,2313.8559999999975,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",460,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.176,2316.0319999999974,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",461,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.176,2318.2079999999974,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",462,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.112,2320.3199999999974,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",463,71420.0,179704.0,0,0,0.0,179704.0,179704.0,0.0,192.0,0.0,49152.0,49152.0,2.176,2322.4959999999974,12288.0,24576.0,71420.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",464,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.08,2324.5759999999973,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",465,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.08,2326.655999999997,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,466,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10658304.0,86016.0,15.904,2342.559999999997,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333072.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",467,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.944,2345.503999999997,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",468,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.112,2347.6159999999973,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",469,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.256,2351.871999999997,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_32x32_sliced1x4_nn,470,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.256,2364.127999999997,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",471,0.0,46080.0,0,0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.592,2366.719999999997,36864.0,9216.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",472,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,4.032,2370.751999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",473,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,4.0,2374.751999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",474,0.0,0.0,0,0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.656,2377.407999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,384.0,384.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",475,98304.0,3268608.0,0,0,0.0,3268608.0,3268608.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,11.488,2388.895999999997,2472960.0,599040.0,98304.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1920.0,384.0
ampere_sgemm_32x32_sliced1x4_nn,476,23592960.0,48906240.0,0,0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2681152.0,122880.0,7.296,2396.191999999997,737280.0,983040.0,23592960.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,83786.0,3840.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",477,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,3.008,2399.1999999999966,33792.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",478,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.08,2401.2799999999966,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",479,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.224,2405.5039999999967,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),480,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186629,9732096.0,393216.0,14.304,2419.807999999997,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",481,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.816,2422.6239999999966,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",482,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,2424.6719999999964,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",483,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.208,2426.8799999999965,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",484,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.048,2428.9279999999962,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",485,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.176,2431.103999999996,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",486,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.304,2433.4079999999963,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",487,71429.0,179722.0,0,0,0.0,179722.0,179722.0,0.0,192.0,0.0,49152.0,49152.0,2.144,2435.551999999996,12288.0,24576.0,71429.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",488,12288.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.08,2437.631999999996,0.0,0.0,12288.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",489,0.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.368,2439.999999999996,0.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0
ampere_sgemm_32x32_sliced1x4_nn,490,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10657472.0,86016.0,15.584,2455.5839999999957,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,333046.0,2688.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",491,0.0,27648.0,0,0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.816,2458.3999999999955,24576.0,3072.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",492,3072.0,6144.0,0,0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.24,2460.6399999999953,0.0,0.0,3072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,384.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",493,25668.0,93428.0,0,0,0.0,93428.0,93428.0,80.0,272.0,0.22727272727272727,36864.0,12544.0,4.192,2464.8319999999953,26160.0,15932.0,25668.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0
ampere_sgemm_64x32_sliced1x4_tn,494,1236271104.0,2483810304.0,0,0,0.0,2483810304.0,2483810304.0,5810112.0,1287908.0,0.818553906582399,159352576.0,770336.0,185.6,2650.4319999999952,4829184.0,6438912.0,1236271104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4979768.0,24073.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",495,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,2652.127999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",496,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,96.0,160.0,2.72,2654.847999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,5.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",497,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,2656.863999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",498,0.0,201028.0,0,0,0.0,201028.0,201028.0,0.0,3158.0,0.0,804128.0,804128.0,3.072,2659.935999999995,0.0,201028.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,25129.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",499,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,2661.663999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",500,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,59776.0,3.776,2665.439999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1868.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",501,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,82608.0,0.15193823915900131,5134592.0,0.0,5.664,2671.1039999999953,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",502,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,57280.0,3.744,2674.8479999999954,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1790.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",503,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83808.0,0.15008924225214992,5134592.0,0.0,5.92,2680.7679999999955,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",504,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,56832.0,3.84,2684.6079999999956,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1776.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",505,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83158.0,0.1510851589456706,5134592.0,0.0,5.664,2690.271999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",506,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,57024.0,3.776,2694.0479999999957,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1782.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",507,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83158.0,0.1510851589456706,5134592.0,128.0,5.568,2699.615999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,4.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",508,0.0,0.0,0,0,0.0,0.0,0.0,0.0,21.0,0.0,6432.0,800.0,2.464,2702.079999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,201.0,25.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",509,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.696,2703.7759999999957,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",510,0.0,0.0,0,0,0.0,0.0,0.0,497.0,22.0,0.9576107899807321,800.0,0.0,3.488,2707.2639999999956,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",511,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.728,2708.9919999999956,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",512,0.0,0.0,0,0,0.0,0.0,0.0,497.0,22.0,0.9576107899807321,800.0,0.0,3.52,2712.5119999999956,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",513,0.0,0.0,0,0,0.0,0.0,0.0,49488.0,13018.0,0.791731993728602,831456.0,8672.0,5.824,2718.3359999999957,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25983.0,271.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",514,0.0,0.0,0,0,0.0,0.0,0.0,1832.0,32.0,0.9828326180257511,2560.0,544.0,6.432,2724.7679999999955,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,80.0,17.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",515,0.0,0.0,0,0,0.0,0.0,0.0,0.0,18849.0,0.0,814496.0,86656.0,3.776,2728.5439999999953,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25453.0,2708.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",516,0.0,0.0,0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,3.392,2731.935999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,31412.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",517,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6283.0,0.0,0.0,1608224.0,2.688,2734.6239999999952,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,50257.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",518,0.0,0.0,0,0,0.0,0.0,0.0,98304.0,6283.0,0.9399256121697726,804128.0,0.0,4.128,2738.7519999999954,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",519,0.0,0.0,0,0,0.0,0.0,0.0,156.0,64.0,0.7090909090909091,8192.0,0.0,2.176,2740.9279999999953,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",520,0.0,0.0,0,0,0.0,0.0,0.0,78783.0,29671.0,0.7264185737732126,3054784.0,2032000.0,10.464,2751.3919999999953,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,95462.0,63500.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",521,0.0,0.0,0,0,0.0,0.0,0.0,23142.0,34830.0,0.3991927137238667,3113024.0,2479936.0,8.416,2759.8079999999954,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97282.0,77498.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",522,0.0,0.0,0,0,0.0,0.0,0.0,23883.0,34046.0,0.41228055032885086,3129664.0,2479936.0,9.184,2768.9919999999956,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97802.0,77498.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",523,0.0,0.0,0,0,0.0,0.0,0.0,23883.0,35638.0,0.40125333915760825,3109184.0,1887424.0,9.152,2778.1439999999957,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97162.0,58982.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",524,0.0,0.0,0,0,0.0,0.0,0.0,14336.0,6283.0,0.6952810514573937,1608224.0,0.0,4.608,2782.751999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",525,0.0,0.0,0,0,0.0,0.0,0.0,39.0,16.0,0.7090909090909091,2048.0,0.0,2.272,2785.023999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",526,0.0,0.0,0,0,0.0,0.0,0.0,20059.0,18050.0,0.5263586029546826,2099744.0,1396480.0,8.0,2793.023999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,65617.0,43640.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",527,0.0,0.0,0,0,0.0,0.0,0.0,0.0,25132.0,0.0,2428064.0,2412352.0,5.28,2798.303999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,75877.0,75386.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",528,2814392.0,6655044.0,0,0,0.0,6655044.0,6655044.0,528.0,6704.0,0.07300884955752213,2276992.0,752768.0,23.68,2821.983999999996,825232.0,201028.0,2814392.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,71156.0,23524.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",529,0.0,1024200.0,0,0,0.0,1024200.0,1024200.0,112284.0,12568.0,0.8993368147887099,804480.0,626624.0,72.128,2894.111999999996,1024200.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25140.0,19582.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",530,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3158.0,0.0,804128.0,200800.0,3.136,2897.247999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,6275.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",531,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,128.0,1.952,2899.199999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,4.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",532,0.0,0.0,0,0,0.0,0.0,0.0,0.0,18849.0,0.0,1809280.0,88416.0,7.936,2907.1359999999963,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,56540.0,2763.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",533,0.0,0.0,0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,3.392,2910.527999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,31412.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",534,2814392.0,6655044.0,0,0,0.0,6655044.0,6655044.0,528.0,6704.0,0.07300884955752213,2279680.0,753088.0,23.744,2934.2719999999963,825232.0,201028.0,2814392.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,71240.0,23534.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",535,0.0,0.0,0,0,0.0,0.0,0.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.208,2940.4799999999964,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",536,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.112,2942.5919999999965,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",537,0.0,0.0,0,0,0.0,0.0,0.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.624,2949.2159999999963,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",538,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.176,2951.391999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",539,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.08,2953.471999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",540,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,3.296,2956.767999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",541,0.0,220484.0,0,0,0.0,220484.0,220484.0,320.0,1582.0,0.16824395373291273,804320.0,128.0,11.136,2967.903999999996,220484.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25135.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",542,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,2969.919999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",543,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.264,2973.183999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",544,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,2975.231999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",545,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.976,2978.207999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",546,1769472.0,3941000.0,0,0,0.0,3941000.0,3941000.0,0.0,6283.0,0.0,0.0,804128.0,4.0,2982.207999999996,0.0,402056.0,1769472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,25129.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",547,1005140.0,2010280.0,0,0,0.0,2010280.0,2010280.0,0.0,4737.0,0.0,1608256.0,0.0,5.088,2987.295999999996,0.0,0.0,1005140.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50258.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",548,0.0,0.0,0,0,0.0,0.0,0.0,640.0,1582.0,0.28802880288028804,804512.0,128.0,16.32,3003.6159999999963,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25141.0,4.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",549,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.016,3005.6319999999964,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",550,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,3007.679999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",551,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.24,3009.919999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",552,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.016,3011.935999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",553,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,96.0,160.0,2.592,3014.527999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,5.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",554,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,3016.255999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",555,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,3017.951999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",556,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.016,3019.967999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",557,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,3021.663999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",558,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,128.0,32.0,2.272,3023.935999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",559,0.0,0.0,0,0,0.0,0.0,0.0,0.0,5.0,0.0,32.0,32.0,4.8,3028.7359999999962,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",560,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.048,3030.783999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",561,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.08,3032.863999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",562,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.752,3035.615999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",563,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.2,3038.8159999999957,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",564,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,3040.8639999999955,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
