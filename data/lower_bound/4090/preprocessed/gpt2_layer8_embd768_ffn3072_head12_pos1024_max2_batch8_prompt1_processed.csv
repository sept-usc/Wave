Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,64.0,1.856,1.856,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,2.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",2,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,64.0,1.824,3.68,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,2.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",3,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,5.408,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",4,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.08,7.488,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",5,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.464,9.952,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",6,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,32.0,2.368,12.32,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,1.0
"void native::unrolled_elementwise_kernel<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)], std::array<char *, 2>, 8, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<1>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",7,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,64.0,3.328,15.648,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,2.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::sum_functor<long, long, long>::operator ()(at::TensorIterator &)::[lambda(long, long) (instance 1)]>, unsigned int, long, 4, 4>>(T3)",8,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,3.264,18.912,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",9,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.176,21.088,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",10,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,64.0,1.696,22.784000000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,2.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",11,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,24.512000000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void at_cuda_detail::DeviceScanInitKernel<at_cuda_detail::ScanTileState<long, 1>>(T1, int)",12,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.792,26.304000000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanKernel<at_cuda_detail::DeviceScanPolicy<long, std::plus<long>>::Policy900, const long *, long *, at_cuda_detail::ScanTileState<long, 1>, std::plus<long>, at_cuda_detail::NullType, unsigned int, long, 0>(T2, T3, T4, int, T5, T6, T7)",13,0.0,0.0,0,0,0.0,0.0,0.0,129.0,6.0,0.9555555555555556,32.0,32.0,2.432,28.736000000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",14,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,30.752000000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",15,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.176,32.928000000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",16,0.0,0.0,0,0,0.0,0.0,0.0,96.0,8.0,0.9230769230769231,64.0,32.0,2.528,35.456,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",17,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,64.0,2.016,37.472,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,2.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",18,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,2.08,39.552,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",19,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,0.0,2.112,41.664,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,0.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",20,0.0,0.0,0,0,0.0,0.0,0.0,0.0,576.0,0.0,3456.0,24576.0,4.768,46.432,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,108.0,768.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",21,0.0,0.0,0,0,0.0,0.0,0.0,0.0,576.0,0.0,3456.0,24576.0,4.8,51.232,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,108.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",22,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.048,53.28,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",23,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,2.016,55.296,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",24,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.264,58.56,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",25,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.224,62.784000000000006,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,26,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.096,74.88000000000001,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",27,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.656,77.53600000000002,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",28,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.72,80.25600000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",29,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.656,82.91200000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",30,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.752,85.66400000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",31,196608.0,6531072.0,0,0,0.0,6531072.0,6531072.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,13.696,99.36000000000001,4939776.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,32,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,8.48,107.84000000000002,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",33,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.368,110.20800000000001,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",34,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.176,112.38400000000001,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",35,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.192,116.57600000000002,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),36,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,15.136,131.71200000000002,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",37,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.168,134.88000000000002,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",38,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.048,136.92800000000003,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",39,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.016,138.94400000000002,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",40,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.048,140.99200000000002,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",41,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.208,143.20000000000002,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",42,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.144,145.34400000000002,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",43,143064.0,359856.0,0,0,0.0,359856.0,359856.0,0.0,384.0,0.0,98304.0,98304.0,2.208,147.55200000000002,24576.0,49152.0,143064.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",44,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.048,149.60000000000002,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",45,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.4,152.00000000000003,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,46,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11897664.0,172032.0,16.192,168.19200000000004,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371802.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",47,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,3.04,171.23200000000003,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",48,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.08,173.31200000000004,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",49,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.224,177.53600000000003,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,50,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.416,189.95200000000003,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",51,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.688,192.64000000000001,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",52,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.656,195.29600000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",53,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.816,198.11200000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",54,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.688,200.8,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",55,196608.0,6531072.0,0,0,0.0,6531072.0,6531072.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,12.544,213.34400000000002,4939776.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,56,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,8.0,221.34400000000002,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",57,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.4,223.74400000000003,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",58,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.112,225.85600000000002,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",59,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.256,230.11200000000002,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),60,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.432,244.544,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",61,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.296,247.84,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",62,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.144,249.984,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",63,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.048,252.032,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",64,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.144,254.17600000000002,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",65,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.336,256.512,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",66,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,258.624,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",67,142872.0,359472.0,0,0,0.0,359472.0,359472.0,0.0,384.0,0.0,98304.0,98304.0,2.208,260.83200000000005,24576.0,49152.0,142872.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",68,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.08,262.91200000000003,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",69,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.176,265.088,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,70,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11895808.0,172032.0,15.264,280.35200000000003,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371744.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",71,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,2.944,283.29600000000005,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",72,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.112,285.4080000000001,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",73,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.32,289.72800000000007,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,74,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.544,302.27200000000005,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",75,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.688,304.96000000000004,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",76,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.656,307.61600000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",77,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.688,310.30400000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",78,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.624,312.92800000000005,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",79,196608.0,6531072.0,0,0,0.0,6531072.0,6531072.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,12.608,325.53600000000006,4939776.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,80,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,7.584,333.12000000000006,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",81,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.4,335.52000000000004,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",82,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.24,337.76000000000005,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",83,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.192,341.95200000000006,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),84,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.976,356.92800000000005,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",85,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.296,360.22400000000005,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",86,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.144,362.36800000000005,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",87,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.048,364.41600000000005,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",88,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.048,366.46400000000006,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",89,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.208,368.6720000000001,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",90,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.144,370.8160000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",91,142552.0,358832.0,0,0,0.0,358832.0,358832.0,0.0,384.0,0.0,98304.0,98304.0,2.208,373.0240000000001,24576.0,49152.0,142552.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",92,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.048,375.0720000000001,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",93,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.24,377.3120000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,94,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11896672.0,172032.0,15.936,393.2480000000001,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371771.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",95,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,2.912,396.1600000000001,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",96,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.08,398.24000000000007,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",97,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.224,402.46400000000006,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,98,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.192,414.65600000000006,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",99,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.848,417.5040000000001,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",100,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.656,420.1600000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",101,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.816,422.97600000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",102,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.592,425.56800000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",103,196608.0,6531072.0,0,0,0.0,6531072.0,6531072.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,12.544,438.112,4939776.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,104,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,8.096,446.208,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",105,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.368,448.576,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",106,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.144,450.72,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",107,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.288,455.00800000000004,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),108,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.496,469.504,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",109,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.2,472.704,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",110,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.08,474.784,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",111,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.048,476.832,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",112,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.08,478.912,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",113,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.24,481.152,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",114,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.24,483.392,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",115,143016.0,359760.0,0,0,0.0,359760.0,359760.0,0.0,384.0,0.0,98304.0,98304.0,2.208,485.6,24576.0,49152.0,143016.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",116,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.08,487.68,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",117,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.176,489.856,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,118,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11896768.0,172032.0,16.096,505.952,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371774.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",119,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,2.912,508.864,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",120,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.208,511.072,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",121,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.256,515.328,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,122,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.448,527.776,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",123,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.816,530.592,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",124,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.624,533.216,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",125,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.72,535.936,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",126,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.656,538.592,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",127,196608.0,6531072.0,0,0,0.0,6531072.0,6531072.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,12.416,551.008,4939776.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,128,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,7.872,558.88,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",129,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.368,561.248,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",130,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.112,563.36,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",131,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.192,567.552,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),132,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.944,582.496,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",133,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.296,585.792,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",134,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.08,587.8720000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",135,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.048,589.9200000000001,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",136,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.08,592.0000000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",137,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.208,594.2080000000001,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",138,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,596.32,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",139,142592.0,358912.0,0,0,0.0,358912.0,358912.0,0.0,384.0,0.0,98304.0,98304.0,2.176,598.4960000000001,24576.0,49152.0,142592.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",140,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.112,600.6080000000001,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",141,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.176,602.7840000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,142,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11898880.0,172032.0,15.84,618.6240000000001,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371840.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",143,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,2.944,621.5680000000001,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",144,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.08,623.6480000000001,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",145,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.288,627.9360000000001,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,146,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.32,640.2560000000002,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",147,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.624,642.8800000000002,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",148,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.688,645.5680000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",149,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.656,648.2240000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",150,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.592,650.8160000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",151,196608.0,6531072.0,0,0,0.0,6531072.0,6531072.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,12.608,663.4240000000001,4939776.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,152,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,8.16,671.5840000000001,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",153,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.4,673.984,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",154,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.272,676.2560000000001,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",155,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.224,680.4800000000001,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),156,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.688,695.1680000000001,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",157,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.168,698.3360000000001,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",158,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,700.4480000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",159,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.016,702.464,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",160,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.08,704.5440000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",161,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.208,706.7520000000001,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",162,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.144,708.8960000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",163,142672.0,359072.0,0,0,0.0,359072.0,359072.0,0.0,384.0,0.0,98304.0,98304.0,2.208,711.104,24576.0,49152.0,142672.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",164,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.08,713.1840000000001,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",165,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.24,715.4240000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,166,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11894432.0,172032.0,15.552,730.9760000000001,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371701.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",167,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,3.008,733.9840000000002,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",168,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.208,736.1920000000001,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",169,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.192,740.3840000000001,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,170,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.704,753.0880000000001,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",171,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.688,755.7760000000001,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",172,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.656,758.432,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",173,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.688,761.12,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",174,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.656,763.776,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",175,196608.0,6531072.0,0,0,0.0,6531072.0,6531072.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,12.512,776.288,4939776.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,176,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,7.936,784.224,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",177,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.4,786.624,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",178,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.144,788.768,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",179,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.192,792.96,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),180,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.592,807.552,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",181,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.2,810.7520000000001,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",182,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.08,812.8320000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",183,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.048,814.8800000000001,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",184,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.08,816.9600000000002,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",185,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.368,819.3280000000002,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",186,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,821.4400000000002,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",187,142792.0,359312.0,0,0,0.0,359312.0,359312.0,0.0,384.0,0.0,98304.0,98304.0,2.208,823.6480000000001,24576.0,49152.0,142792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",188,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.112,825.7600000000001,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",189,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.208,827.9680000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,190,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11890272.0,172032.0,16.096,844.0640000000001,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371571.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",191,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,3.04,847.104,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",192,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.08,849.1840000000001,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",193,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.256,853.44,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,194,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.832,866.272,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",195,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.624,868.8960000000001,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",196,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.656,871.552,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",197,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.688,874.24,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",198,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.624,876.864,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",199,196608.0,6531072.0,0,0,0.0,6531072.0,6531072.0,51072.0,96.0,0.99812382739212,73728.0,24576.0,12.64,889.504,4939776.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,200,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,7.712,897.216,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",201,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.368,899.5840000000001,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",202,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.144,901.7280000000001,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",203,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.224,905.9520000000001,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),204,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.496,920.4480000000001,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",205,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.296,923.7440000000001,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",206,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,925.8560000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",207,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.048,927.9040000000001,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",208,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.08,929.9840000000002,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",209,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.208,932.1920000000001,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",210,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,934.3040000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",211,142848.0,359424.0,0,0,0.0,359424.0,359424.0,0.0,384.0,0.0,98304.0,98304.0,2.208,936.5120000000001,24576.0,49152.0,142848.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",212,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.144,938.6560000000001,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",213,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.208,940.864,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,214,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11897952.0,172032.0,15.616,956.48,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371811.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",215,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,2.976,959.456,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",216,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.08,961.5360000000001,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",217,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.352,965.888,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_64x32_sliced1x4_tn,218,1236271104.0,2483810304.0,0,0,0.0,2483810304.0,2483810304.0,5810112.0,1369648.0,0.8092348490757352,164927616.0,1554080.0,188.256,1154.144,4829184.0,6438912.0,1236271104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5153988.0,48565.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",219,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,64.0,1.696,1155.84,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,2.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",220,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,128.0,256.0,2.624,1158.464,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,8.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",221,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.112,1160.576,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",222,0.0,402056.0,0,0,0.0,402056.0,402056.0,0.0,6314.0,0.0,1608224.0,1608224.0,3.808,1164.384,0.0,402056.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50257.0,50257.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",223,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,1166.08,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",224,0.0,0.0,0,0,0.0,0.0,0.0,6400.0,18968.0,0.2522863450015768,1622016.0,115968.0,4.48,1170.56,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50688.0,3624.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",225,0.0,0.0,0,0,0.0,0.0,0.0,29600.0,165216.0,0.15193823915900131,8338656.0,32.0,6.016,1176.576,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,260583.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",226,0.0,0.0,0,0,0.0,0.0,0.0,6400.0,18968.0,0.2522863450015768,1622016.0,116160.0,4.576,1181.152,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50688.0,3630.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",227,0.0,0.0,0,0,0.0,0.0,0.0,29600.0,167616.0,0.15008924225214992,8338656.0,32.0,6.112,1187.2640000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,260583.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",228,0.0,0.0,0,0,0.0,0.0,0.0,6400.0,18968.0,0.2522863450015768,1622016.0,115264.0,4.352,1191.6160000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50688.0,3602.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",229,0.0,0.0,0,0,0.0,0.0,0.0,29600.0,167216.0,0.15039427688805787,8338656.0,32.0,5.984,1197.6000000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,260583.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",230,0.0,0.0,0,0,0.0,0.0,0.0,6400.0,18968.0,0.2522863450015768,1622016.0,114240.0,4.416,1202.016,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50688.0,3570.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",231,0.0,0.0,0,0,0.0,0.0,0.0,29600.0,166416.0,0.1510080809729818,8338656.0,320.0,6.112,1208.1280000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,260583.0,10.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",232,0.0,0.0,0,0,0.0,0.0,0.0,0.0,39.0,0.0,12832.0,1600.0,2.912,1211.0400000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,401.0,50.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",233,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.696,1212.736,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",234,0.0,0.0,0,0,0.0,0.0,0.0,497.0,34.0,0.935969868173258,1600.0,0.0,3.616,1216.352,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",235,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.696,1218.048,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",236,0.0,0.0,0,0,0.0,0.0,0.0,497.0,34.0,0.935969868173258,1600.0,0.0,3.84,1221.888,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",237,0.0,0.0,0,0,0.0,0.0,0.0,104064.0,26024.0,0.7999508025336696,1652928.0,15968.0,6.496,1228.384,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,51654.0,499.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",238,0.0,0.0,0,0,0.0,0.0,0.0,3664.0,64.0,0.9828326180257511,5120.0,992.0,6.528,1234.912,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160.0,31.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",239,0.0,0.0,0,0,0.0,0.0,0.0,0.0,37695.0,0.0,1622944.0,168640.0,4.512,1239.424,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50717.0,5270.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",240,0.0,0.0,0,0,0.0,0.0,0.0,0.0,9471.0,0.0,2010304.0,0.0,5.056,1244.48,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,62822.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",241,0.0,0.0,0,0,0.0,0.0,0.0,0.0,12565.0,0.0,0.0,3216448.0,3.584,1248.064,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,100514.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",242,0.0,0.0,0,0,0.0,0.0,0.0,98304.0,12565.0,0.8866680496802533,1608224.0,0.0,4.992,1253.056,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",243,0.0,0.0,0,0,0.0,0.0,0.0,156.0,64.0,0.7090909090909091,8192.0,0.0,2.304,1255.3600000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",244,0.0,0.0,0,0,0.0,0.0,0.0,157514.0,61440.0,0.7193931145354732,6541792.0,4070432.0,13.92,1269.2800000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,204431.0,127201.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",245,0.0,0.0,0,0,0.0,0.0,0.0,45047.0,74323.0,0.37737287425651334,6624992.0,4958816.0,11.136,1280.4160000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,207031.0,154963.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",246,0.0,0.0,0,0,0.0,0.0,0.0,45914.0,74438.0,0.3814976070194097,6607072.0,4958816.0,12.0,1292.4160000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,206471.0,154963.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",247,0.0,0.0,0,0,0.0,0.0,0.0,45914.0,74910.0,0.3800072833211945,6589408.0,4252864.0,12.128,1304.544,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,205919.0,132902.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",248,0.0,0.0,0,0,0.0,0.0,0.0,14336.0,12565.0,0.5329169919333854,3216448.0,0.0,6.24,1310.784,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,100514.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",249,0.0,0.0,0,0,0.0,0.0,0.0,39.0,16.0,0.7090909090909091,2048.0,0.0,11.328,1322.112,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",250,0.0,0.0,0,0,0.0,0.0,0.0,40019.0,48202.0,0.4536221534555265,4558016.0,2874816.0,11.008,1333.1200000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,142438.0,89838.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",251,0.0,0.0,0,0,0.0,0.0,0.0,0.0,50260.0,0.0,4851424.0,4824672.0,7.968,1341.0880000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,151607.0,150771.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",252,5628784.0,13310088.0,0,0,0.0,13310088.0,13310088.0,1056.0,13408.0,0.07300884955752213,4430848.0,1488864.0,24.128,1365.2160000000001,1650464.0,402056.0,5628784.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,138464.0,46527.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",253,0.0,2048400.0,0,0,0.0,2048400.0,2048400.0,224568.0,25136.0,0.8993368147887099,1608960.0,1199648.0,72.224,1437.44,2048400.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50280.0,37489.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",254,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6314.0,0.0,1608224.0,401600.0,4.0,1441.44,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50257.0,12550.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",255,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,256.0,1.952,1443.392,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,8.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",256,0.0,0.0,0,0,0.0,0.0,0.0,0.0,37695.0,0.0,3618528.0,179072.0,8.672,1452.064,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,113079.0,5596.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",257,0.0,0.0,0,0,0.0,0.0,0.0,0.0,9471.0,0.0,2010304.0,0.0,5.504,1457.568,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,62822.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",258,5628784.0,13310088.0,0,0,0.0,13310088.0,13310088.0,1056.0,13408.0,0.07300884955752213,4430624.0,1489088.0,24.48,1482.048,1650464.0,402056.0,5628784.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,138457.0,46534.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",259,0.0,0.0,0,0,0.0,0.0,0.0,4012.0,3195.0,0.5566810045788816,1608448.0,1632.0,7.072,1489.12,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50264.0,51.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",260,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.176,1491.2959999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",261,0.0,0.0,0,0,0.0,0.0,0.0,4012.0,3195.0,0.5566810045788816,1608448.0,1632.0,6.88,1498.176,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50264.0,51.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",262,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.208,1500.384,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",263,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.048,1502.432,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",264,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,3.072,1505.504,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",265,0.0,440456.0,0,0,0.0,440456.0,440456.0,608.0,3164.0,0.16118769883351008,1608512.0,256.0,11.424,1516.9279999999999,440456.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50266.0,8.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",266,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,1518.9759999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",267,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.328,1522.3039999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",268,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.304,1524.608,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",269,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.944,1527.552,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",270,1769472.0,4343056.0,0,0,0.0,4343056.0,4343056.0,0.0,12565.0,0.0,0.0,1608224.0,4.096,1531.648,0.0,804112.0,1769472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,50257.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",271,2010280.0,4020560.0,0,0,0.0,4020560.0,4020560.0,0.0,9471.0,0.0,3216448.0,0.0,6.976,1538.624,0.0,0.0,2010280.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,100514.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",272,0.0,0.0,0,0,0.0,0.0,0.0,1216.0,3164.0,0.2776255707762557,1608896.0,256.0,15.936,1554.56,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50278.0,8.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",273,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,128.0,64.0,2.304,1556.864,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,2.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",274,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,64.0,2.4,1559.2640000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,2.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",275,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,64.0,2.496,1561.7600000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,2.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",276,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,128.0,64.0,2.08,1563.8400000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,2.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",277,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,128.0,256.0,2.592,1566.4320000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,8.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",278,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,1568.1280000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",279,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,1569.824,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",280,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.048,1571.872,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",281,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,1573.568,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",282,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,160.0,32.0,2.592,1576.16,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",283,0.0,0.0,0,0,0.0,0.0,0.0,0.0,5.0,0.0,32.0,32.0,4.736,1580.8960000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",284,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.08,1582.976,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",285,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,1585.0240000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",286,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,64.0,2.848,1587.872,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,2.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",287,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,3.232,1591.104,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",288,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.08,1593.184,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",289,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.4,1595.584,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::index_elementwise_kernel<128, 4, void native::gpu_index_kernel<void native::index_kernel_impl<native::OpaqueType<8>>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>, const T1 &)::[lambda(int) (instance 1)]>(long, T3)",290,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,160.0,64.0,3.328,1598.912,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5.0,2.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",291,0.0,0.0,0,0,0.0,0.0,0.0,96.0,8.0,0.9230769230769231,128.0,32.0,2.976,1601.8880000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",292,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,128.0,128.0,2.08,1603.968,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",293,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,128.0,32.0,2.016,1605.9840000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",294,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,160.0,0.0,2.08,1608.064,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5.0,0.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",295,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,128.0,64.0,2.272,1610.336,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,2.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",296,0.0,0.0,0,0,0.0,0.0,0.0,0.0,576.0,0.0,24960.0,24576.0,7.392,1617.728,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,780.0,768.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",297,0.0,0.0,0,0,0.0,0.0,0.0,0.0,576.0,0.0,3456.0,24576.0,4.8,1622.528,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,108.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",298,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.112,1624.64,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",299,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,128.0,32.0,2.016,1626.6560000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",300,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.296,1629.9520000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",301,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.256,1634.2080000000003,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,302,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,11.904,1646.1120000000003,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",303,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.592,1648.7040000000004,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",304,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,3.968,1652.6720000000005,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",305,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.064,1656.7360000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",306,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.688,1659.4240000000007,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",307,196608.0,6537216.0,0,0,0.0,6537216.0,6537216.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,13.504,1672.9280000000006,4945920.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3840.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,308,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,8.416,1681.3440000000005,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",309,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.336,1683.6800000000005,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",310,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.08,1685.7600000000004,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",311,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.224,1689.9840000000004,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),312,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,15.2,1705.1840000000004,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",313,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.36,1708.5440000000003,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",314,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,1710.6560000000004,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",315,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.112,1712.7680000000005,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",316,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.24,1715.0080000000005,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",317,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.208,1717.2160000000006,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",318,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.048,1719.2640000000006,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",319,142914.0,359556.0,0,0,0.0,359556.0,359556.0,0.0,384.0,0.0,98304.0,98304.0,2.176,1721.4400000000005,24576.0,49152.0,142914.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",320,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.112,1723.5520000000006,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",321,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.304,1725.8560000000007,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,322,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11894144.0,172032.0,16.384,1742.2400000000007,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371692.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",323,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,3.136,1745.3760000000007,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",324,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.24,1747.6160000000007,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",325,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.224,1751.8400000000006,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,326,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.352,1764.1920000000007,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",327,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.592,1766.7840000000008,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",328,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.0,1770.7840000000008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",329,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.128,1774.9120000000007,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",330,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.688,1777.6000000000008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",331,196608.0,6537216.0,0,0,0.0,6537216.0,6537216.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,12.544,1790.144000000001,4945920.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3840.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,332,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,8.032,1798.1760000000008,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",333,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.336,1800.5120000000009,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",334,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.048,1802.5600000000009,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",335,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.288,1806.8480000000009,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),336,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.688,1821.536000000001,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",337,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.2,1824.736000000001,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",338,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,1826.848000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",339,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.08,1828.928000000001,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",340,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,1831.040000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",341,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.176,1833.216000000001,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",342,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.048,1835.264000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",343,142933.0,359594.0,0,0,0.0,359594.0,359594.0,0.0,384.0,0.0,98304.0,98304.0,2.176,1837.440000000001,24576.0,49152.0,142933.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",344,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.08,1839.520000000001,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",345,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.272,1841.7920000000008,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,346,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11896320.0,172032.0,15.712,1857.5040000000008,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371760.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",347,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,3.008,1860.5120000000009,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",348,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.08,1862.5920000000008,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",349,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.224,1866.8160000000007,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,350,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.192,1879.0080000000007,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",351,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.592,1881.6000000000008,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",352,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,3.968,1885.568000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",353,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.0,1889.568000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",354,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.688,1892.256000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",355,196608.0,6537216.0,0,0,0.0,6537216.0,6537216.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,12.608,1904.864000000001,4945920.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3840.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,356,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,7.968,1912.832000000001,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",357,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.336,1915.168000000001,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",358,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.08,1917.248000000001,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",359,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.288,1921.536000000001,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),360,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,15.232,1936.768000000001,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",361,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.36,1940.1280000000008,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",362,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.208,1942.336000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",363,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.272,1944.6080000000009,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",364,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.08,1946.6880000000008,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",365,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.176,1948.8640000000007,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",366,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.048,1950.9120000000007,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",367,142716.0,359160.0,0,0,0.0,359160.0,359160.0,0.0,384.0,0.0,98304.0,98304.0,2.176,1953.0880000000006,24576.0,49152.0,142716.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",368,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.112,1955.2000000000007,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",369,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.24,1957.4400000000007,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,370,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11894944.0,172032.0,15.744,1973.1840000000007,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371717.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",371,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,3.008,1976.1920000000007,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",372,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.112,1978.3040000000008,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",373,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.224,1982.5280000000007,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,374,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,11.968,1994.4960000000008,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",375,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.784,1997.2800000000009,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",376,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.032,2001.3120000000008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",377,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.064,2005.3760000000009,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",378,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.816,2008.192000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",379,196608.0,6537216.0,0,0,0.0,6537216.0,6537216.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,12.576,2020.768000000001,4945920.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3840.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,380,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,8.288,2029.056000000001,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",381,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.336,2031.392000000001,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",382,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.048,2033.440000000001,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",383,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.48,2037.920000000001,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),384,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.56,2052.480000000001,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",385,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.328,2055.808000000001,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",386,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,2057.920000000001,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",387,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.048,2059.9680000000008,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",388,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.144,2062.1120000000005,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",389,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.176,2064.2880000000005,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",390,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.048,2066.3360000000002,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",391,143030.0,359788.0,0,0,0.0,359788.0,359788.0,0.0,384.0,0.0,98304.0,98304.0,2.176,2068.512,24576.0,49152.0,143030.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",392,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.144,2070.656,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",393,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.304,2072.96,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,394,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11895168.0,172032.0,16.0,2088.96,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371724.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",395,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,2.912,2091.872,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",396,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.112,2093.984,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",397,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.256,2098.24,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,398,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.0,2110.24,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",399,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.656,2112.8959999999997,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",400,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.064,2116.9599999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",401,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.096,2121.0559999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",402,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.688,2123.7439999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",403,196608.0,6537216.0,0,0,0.0,6537216.0,6537216.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,12.608,2136.352,4945920.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3840.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,404,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,8.064,2144.4159999999997,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",405,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.336,2146.7519999999995,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",406,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.112,2148.8639999999996,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",407,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.192,2153.0559999999996,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),408,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.848,2167.9039999999995,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",409,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.2,2171.1039999999994,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",410,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.08,2173.1839999999993,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",411,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.048,2175.231999999999,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",412,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.08,2177.311999999999,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",413,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.24,2179.5519999999988,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",414,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.048,2181.5999999999985,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",415,142566.0,358860.0,0,0,0.0,358860.0,358860.0,0.0,384.0,0.0,98304.0,98304.0,2.208,2183.8079999999986,24576.0,49152.0,142566.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",416,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.112,2185.9199999999987,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",417,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.272,2188.1919999999986,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,418,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11899392.0,172032.0,16.096,2204.2879999999986,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371856.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",419,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,2.944,2207.2319999999986,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",420,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.112,2209.3439999999987,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",421,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.512,2213.855999999999,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,422,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.32,2226.175999999999,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",423,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.592,2228.767999999999,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",424,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.0,2232.767999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",425,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.032,2236.7999999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",426,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.72,2239.519999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",427,196608.0,6537216.0,0,0,0.0,6537216.0,6537216.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,12.544,2252.063999999999,4945920.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3840.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,428,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,7.84,2259.903999999999,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",429,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.528,2262.431999999999,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",430,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.048,2264.4799999999987,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",431,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.256,2268.7359999999985,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),432,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.848,2283.5839999999985,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",433,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.328,2286.9119999999984,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",434,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,2289.0239999999985,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",435,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.048,2291.0719999999983,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",436,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,2293.1839999999984,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",437,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.336,2295.519999999998,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",438,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.048,2297.567999999998,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",439,142837.0,359402.0,0,0,0.0,359402.0,359402.0,0.0,384.0,0.0,98304.0,98304.0,2.176,2299.743999999998,24576.0,49152.0,142837.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",440,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.112,2301.855999999998,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",441,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.24,2304.0959999999977,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,442,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11892064.0,172032.0,15.616,2319.7119999999977,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371627.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",443,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,3.04,2322.7519999999977,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",444,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.112,2324.8639999999978,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",445,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.224,2329.087999999998,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,446,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.192,2341.279999999998,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",447,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.624,2343.9039999999977,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",448,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,3.968,2347.8719999999976,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",449,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.096,2351.9679999999976,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",450,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.752,2354.7199999999975,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",451,196608.0,6537216.0,0,0,0.0,6537216.0,6537216.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,12.64,2367.3599999999974,4945920.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3840.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,452,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,8.0,2375.3599999999974,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",453,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.336,2377.695999999997,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",454,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.048,2379.743999999997,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",455,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.352,2384.095999999997,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),456,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.592,2398.687999999997,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",457,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.328,2402.015999999997,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",458,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,2404.127999999997,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",459,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.08,2406.207999999997,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",460,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.112,2408.319999999997,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",461,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.24,2410.5599999999968,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",462,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.048,2412.6079999999965,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",463,142835.0,359398.0,0,0,0.0,359398.0,359398.0,0.0,384.0,0.0,98304.0,98304.0,2.208,2414.8159999999966,24576.0,49152.0,142835.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",464,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.08,2416.8959999999965,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",465,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.336,2419.2319999999963,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,466,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11897472.0,172032.0,15.872,2435.103999999996,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371796.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",467,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,2.976,2438.0799999999963,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",468,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.112,2440.1919999999964,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",469,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.288,2444.4799999999964,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_32x32_sliced1x4_nn,470,56623104.0,114794496.0,0,0,0.0,114794496.0,114794496.0,331776.0,70848.0,0.8240343347639485,8847360.0,221184.0,12.448,2456.9279999999962,663552.0,884736.0,56623104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,276480.0,6912.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",471,0.0,92160.0,0,0,0.0,92160.0,92160.0,0.0,4608.0,0.0,230400.0,73728.0,2.592,2459.5199999999963,73728.0,18432.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7200.0,2304.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",472,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.0,2463.5199999999963,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",473,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,49152.0,49152.0,4.0,2467.5199999999963,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",474,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,2.688,2470.2079999999964,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,768.0,768.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",475,196608.0,6537216.0,0,0,0.0,6537216.0,6537216.0,51072.0,96.0,0.99812382739212,122880.0,24576.0,12.576,2482.7839999999965,4945920.0,1198080.0,196608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3840.0,768.0
ampere_sgemm_32x32_sliced1x4_nn,476,18874368.0,38436864.0,0,0,0.0,38436864.0,38436864.0,116736.0,23808.0,0.8306010928961749,2949120.0,98304.0,8.288,2491.0719999999965,294912.0,393216.0,18874368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,92160.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",477,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1728.0,0.0,101376.0,24576.0,2.336,2493.4079999999963,30720.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3168.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",478,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.08,2495.487999999996,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",479,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.256,2499.743999999996,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),480,75497472.0,151781376.0,0,0,0.0,151781376.0,151781376.0,272640.0,6144.0,0.977961432506887,10027008.0,786432.0,14.688,2514.431999999996,0.0,786432.0,75497472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,313344.0,24576.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",481,0.0,245760.0,0,0,0.0,245760.0,245760.0,0.0,9984.0,0.0,798720.0,98304.0,3.296,2517.727999999996,221184.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,24960.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",482,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.08,2519.807999999996,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",483,0.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.272,2522.079999999996,0.0,49152.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",484,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.144,2524.2239999999956,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",485,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,2.176,2526.3999999999955,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",486,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,384.0,0.0,98304.0,98304.0,2.048,2528.4479999999953,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",487,142804.0,359336.0,0,0,0.0,359336.0,359336.0,0.0,384.0,0.0,98304.0,98304.0,2.208,2530.6559999999954,24576.0,49152.0,142804.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",488,24576.0,49152.0,0,0,0.0,49152.0,49152.0,0.0,384.0,0.0,98304.0,98304.0,2.112,2532.7679999999955,0.0,0.0,24576.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",489,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,576.0,0.0,196608.0,98304.0,2.24,2535.0079999999953,0.0,24576.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0
ampere_sgemm_32x32_sliced1x4_nn,490,77070336.0,155344896.0,0,0,0.0,155344896.0,155344896.0,419328.0,93888.0,0.8170594837261503,11892000.0,172032.0,15.392,2550.399999999995,516096.0,688128.0,77070336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,371625.0,5376.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",491,0.0,55296.0,0,0,0.0,55296.0,55296.0,0.0,2304.0,0.0,175104.0,24576.0,2.944,2553.343999999995,49152.0,6144.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5472.0,768.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",492,6144.0,12288.0,0,0,0.0,12288.0,12288.0,0.0,144.0,0.0,49152.0,24576.0,2.176,2555.519999999995,0.0,0.0,6144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,768.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",493,51336.0,186856.0,0,0,0.0,186856.0,186856.0,160.0,544.0,0.22727272727272727,73728.0,25088.0,4.352,2559.871999999995,52320.0,31864.0,51336.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2304.0,784.0
ampere_sgemm_64x32_sliced1x4_tn,494,1236271104.0,2483810304.0,0,0,0.0,2483810304.0,2483810304.0,5810112.0,1369648.0,0.8092348490757352,165271552.0,1559008.0,187.552,2747.423999999995,4829184.0,6438912.0,1236271104.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5164736.0,48719.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",495,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,64.0,1.728,2749.151999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,2.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",496,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,192.0,320.0,2.592,2751.743999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6.0,10.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",497,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.08,2753.823999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",498,0.0,402056.0,0,0,0.0,402056.0,402056.0,0.0,6314.0,0.0,1608224.0,1608224.0,3.904,2757.727999999995,0.0,402056.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50257.0,50257.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",499,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.664,2759.3919999999953,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",500,0.0,0.0,0,0,0.0,0.0,0.0,6400.0,18968.0,0.2522863450015768,1622016.0,119168.0,4.352,2763.743999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50688.0,3724.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",501,0.0,0.0,0,0,0.0,0.0,0.0,29600.0,165216.0,0.15193823915900131,8338656.0,32.0,6.304,2770.047999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,260583.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",502,0.0,0.0,0,0,0.0,0.0,0.0,6400.0,18968.0,0.2522863450015768,1622016.0,115456.0,4.608,2774.6559999999954,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50688.0,3608.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",503,0.0,0.0,0,0,0.0,0.0,0.0,29600.0,167616.0,0.15008924225214992,8338656.0,0.0,6.08,2780.7359999999953,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,260583.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",504,0.0,0.0,0,0,0.0,0.0,0.0,6400.0,18968.0,0.2522863450015768,1622016.0,116352.0,4.32,2785.0559999999955,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50688.0,3636.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",505,0.0,0.0,0,0,0.0,0.0,0.0,29600.0,165816.0,0.15147173209972573,8338656.0,32.0,6.08,2791.1359999999954,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,260583.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",506,0.0,0.0,0,0,0.0,0.0,0.0,6400.0,18968.0,0.2522863450015768,1622016.0,116480.0,4.352,2795.4879999999953,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50688.0,3640.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",507,0.0,0.0,0,0,0.0,0.0,0.0,29600.0,166116.0,0.15123955118641297,8338656.0,288.0,6.304,2801.7919999999954,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,260583.0,9.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",508,0.0,0.0,0,0,0.0,0.0,0.0,0.0,39.0,0.0,12832.0,1600.0,2.752,2804.5439999999953,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,401.0,50.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",509,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.728,2806.2719999999954,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",510,0.0,0.0,0,0,0.0,0.0,0.0,497.0,34.0,0.935969868173258,1600.0,0.0,3.68,2809.951999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",511,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.728,2811.6799999999953,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",512,0.0,0.0,0,0,0.0,0.0,0.0,497.0,34.0,0.935969868173258,1600.0,0.0,3.712,2815.3919999999953,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",513,0.0,0.0,0,0,0.0,0.0,0.0,96720.0,26034.0,0.7879172980106555,1652928.0,14720.0,6.656,2822.047999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,51654.0,460.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",514,0.0,0.0,0,0,0.0,0.0,0.0,3664.0,64.0,0.9828326180257511,5120.0,992.0,6.528,2828.575999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160.0,31.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",515,0.0,0.0,0,0,0.0,0.0,0.0,0.0,37695.0,0.0,1622944.0,168544.0,4.192,2832.767999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50717.0,5267.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",516,0.0,0.0,0,0,0.0,0.0,0.0,0.0,9471.0,0.0,2010304.0,0.0,5.216,2837.983999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,62822.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",517,0.0,0.0,0,0,0.0,0.0,0.0,0.0,12565.0,0.0,0.0,3216448.0,3.648,2841.631999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,100514.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",518,0.0,0.0,0,0,0.0,0.0,0.0,98304.0,12565.0,0.8866680496802533,1608224.0,0.0,5.024,2846.655999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",519,0.0,0.0,0,0,0.0,0.0,0.0,156.0,64.0,0.7090909090909091,8192.0,0.0,2.176,2848.831999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",520,0.0,0.0,0,0,0.0,0.0,0.0,158414.0,61606.0,0.719998181983456,6673248.0,4022496.0,14.016,2862.847999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,208539.0,125703.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",521,0.0,0.0,0,0,0.0,0.0,0.0,46847.0,77853.0,0.3756776263031275,6608736.0,4379008.0,10.816,2873.6639999999948,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,206523.0,136844.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",522,0.0,0.0,0,0,0.0,0.0,0.0,45914.0,77684.0,0.37147850288839623,6611168.0,4958816.0,11.52,2885.1839999999947,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,206599.0,154963.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",523,0.0,0.0,0,0,0.0,0.0,0.0,45914.0,77882.0,0.3708843581375812,6577888.0,4249760.0,11.936,2897.119999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,205559.0,132805.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",524,0.0,0.0,0,0,0.0,0.0,0.0,14336.0,12565.0,0.5329169919333854,3216448.0,0.0,6.208,2903.327999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,100514.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",525,0.0,0.0,0,0,0.0,0.0,0.0,39.0,16.0,0.7090909090909091,2048.0,0.0,2.208,2905.535999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",526,0.0,0.0,0,0,0.0,0.0,0.0,40019.0,51204.0,0.4386941889655021,4544960.0,2866080.0,10.72,2916.255999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,142030.0,89565.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",527,0.0,0.0,0,0,0.0,0.0,0.0,0.0,50260.0,0.0,4851392.0,4824672.0,7.808,2924.063999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,151606.0,150771.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",528,5628784.0,13310088.0,0,0,0.0,13310088.0,13310088.0,1056.0,13408.0,0.07300884955752213,4437440.0,1485216.0,23.936,2947.999999999995,1650464.0,402056.0,5628784.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,138670.0,46413.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",529,0.0,2048400.0,0,0,0.0,2048400.0,2048400.0,224568.0,25136.0,0.8993368147887099,1608608.0,1194592.0,72.448,3020.447999999995,2048400.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50269.0,37331.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",530,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6314.0,0.0,1608224.0,401600.0,4.032,3024.479999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50257.0,12550.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",531,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,256.0,1.92,3026.399999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,8.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",532,0.0,0.0,0,0,0.0,0.0,0.0,0.0,37695.0,0.0,3618528.0,188992.0,8.736,3035.135999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,113079.0,5906.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",533,0.0,0.0,0,0,0.0,0.0,0.0,0.0,9471.0,0.0,2010304.0,0.0,5.536,3040.671999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,62822.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",534,5628784.0,13310088.0,0,0,0.0,13310088.0,13310088.0,1056.0,13408.0,0.07300884955752213,4437344.0,1486560.0,24.16,3064.831999999995,1650464.0,402056.0,5628784.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,138667.0,46455.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",535,0.0,0.0,0,0,0.0,0.0,0.0,4012.0,3195.0,0.5566810045788816,1608448.0,1632.0,6.88,3071.711999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50264.0,51.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",536,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.144,3073.8559999999948,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",537,0.0,0.0,0,0,0.0,0.0,0.0,4012.0,3195.0,0.5566810045788816,1608448.0,1632.0,6.816,3080.6719999999946,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50264.0,51.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",538,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.208,3082.8799999999947,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",539,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.08,3084.9599999999946,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",540,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.944,3087.9039999999945,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",541,0.0,440456.0,0,0,0.0,440456.0,440456.0,608.0,3164.0,0.16118769883351008,1608480.0,256.0,11.168,3099.0719999999947,440456.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50265.0,8.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",542,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,1.984,3101.0559999999946,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",543,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.168,3104.2239999999947,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",544,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,3106.239999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",545,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.976,3109.215999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",546,1769472.0,4343056.0,0,0,0.0,4343056.0,4343056.0,0.0,12565.0,0.0,0.0,1608224.0,4.064,3113.2799999999947,0.0,804112.0,1769472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,50257.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",547,2010280.0,4020560.0,0,0,0.0,4020560.0,4020560.0,0.0,9471.0,0.0,3216448.0,0.0,6.656,3119.9359999999947,0.0,0.0,2010280.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,100514.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",548,0.0,0.0,0,0,0.0,0.0,0.0,1216.0,3164.0,0.2776255707762557,1608960.0,256.0,16.224,3136.159999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50280.0,8.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",549,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,128.0,64.0,2.048,3138.2079999999946,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,2.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",550,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,64.0,2.016,3140.2239999999947,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,2.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",551,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,64.0,2.24,3142.4639999999945,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,2.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",552,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,128.0,64.0,2.048,3144.5119999999943,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,2.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",553,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,192.0,320.0,2.688,3147.1999999999944,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6.0,10.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",554,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,3148.9279999999944,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",555,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,3150.6559999999945,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",556,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.208,3152.8639999999946,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",557,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,3154.5599999999945,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",558,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,224.0,32.0,2.304,3156.8639999999946,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",559,0.0,0.0,0,0,0.0,0.0,0.0,0.0,5.0,0.0,32.0,32.0,4.736,3161.5999999999945,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",560,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.112,3163.7119999999945,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",561,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.08,3165.7919999999945,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",562,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,64.0,2.784,3168.5759999999946,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,2.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",563,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,3.232,3171.8079999999945,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",564,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.08,3173.8879999999945,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
