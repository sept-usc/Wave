Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.824,1.824,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",2,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,3.52,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",3,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.664,5.184,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",4,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.048,7.232,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",5,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.272,9.504,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",6,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.368,11.872,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::unrolled_elementwise_kernel<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)], std::array<char *, 2>, 8, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<1>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",7,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.424,15.296,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::sum_functor<long, long, long>::operator ()(at::TensorIterator &)::[lambda(long, long) (instance 1)]>, unsigned int, long, 4, 4>>(T3)",8,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.072,18.368,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",9,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.144,20.512,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",10,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.664,22.176000000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",11,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.664,23.840000000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void at_cuda_detail::DeviceScanInitKernel<at_cuda_detail::ScanTileState<long, 1>>(T1, int)",12,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.664,25.504000000000005,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanKernel<at_cuda_detail::DeviceScanPolicy<long, std::plus<long>>::Policy900, const long *, long *, at_cuda_detail::ScanTileState<long, 1>, std::plus<long>, at_cuda_detail::NullType, unsigned int, long, 0>(T2, T3, T4, int, T5, T6, T7)",13,0.0,0.0,0,0,0.0,0.0,0.0,129.0,6.0,0.9555555555555556,32.0,32.0,2.368,27.872000000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",14,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,29.888000000000005,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",15,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.144,32.032000000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",16,0.0,0.0,0,0,0.0,0.0,0.0,56.0,4.0,0.9333333333333333,32.0,32.0,2.496,34.528000000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",17,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,1.984,36.51200000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",18,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,38.528000000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",19,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,0.0,2.016,40.544000000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,0.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",20,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,8704.0,32768.0,3.84,44.384,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,272.0,1024.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",21,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,8704.0,32768.0,3.744,48.128,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,272.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",22,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.048,50.176,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",23,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,1.984,52.160000000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",24,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.328,55.48800000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",25,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.696,61.184000000000005,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,26,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,59.136,120.32000000000001,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",27,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.656,122.97600000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",28,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.688,125.66400000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",29,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.624,128.288,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",30,262144.0,8708096.0,0,0,0.0,8708096.0,8708096.0,68096.0,128.0,0.99812382739212,98304.0,32768.0,14.208,142.496,6586368.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,31,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18923776.0,98304.0,23.328,165.824,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591368.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",32,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,3.52,169.34400000000002,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",33,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.048,171.39200000000002,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",34,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.664,177.056,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),35,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,75.008,252.06400000000002,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",36,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.784,254.848,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",37,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.048,256.896,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",38,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.272,259.168,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",39,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,261.28000000000003,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",40,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.72,264.00000000000006,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",41,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,266.1120000000001,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",42,182928.0,464160.0,0,0,0.0,464160.0,464160.0,0.0,512.0,0.0,131072.0,131072.0,2.176,268.28800000000007,32768.0,65536.0,182928.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",43,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.272,270.56000000000006,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",44,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.432,272.9920000000001,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,45,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69272640.0,229376.0,76.672,349.6640000000001,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164770.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",46,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,3.072,352.7360000000001,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",47,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.048,354.7840000000001,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",48,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.664,360.4480000000001,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,49,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,59.52,419.9680000000001,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",50,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.912,422.88000000000005,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",51,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.656,425.53600000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",52,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.592,428.12800000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",53,262144.0,8708096.0,0,0,0.0,8708096.0,8708096.0,68096.0,128.0,0.99812382739212,98304.0,32768.0,14.112,442.24000000000007,6586368.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,54,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18924832.0,98304.0,22.976,465.21600000000007,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591401.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",55,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.688,467.90400000000005,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",56,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.08,469.98400000000004,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",57,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.792,475.776,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),58,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,75.424,551.2,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",59,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.944,554.144,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",60,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,556.256,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",61,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.24,558.496,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",62,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,560.608,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",63,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.24,562.848,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",64,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,564.9599999999999,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",65,182524.0,463352.0,0,0,0.0,463352.0,463352.0,0.0,512.0,0.0,131072.0,131072.0,2.176,567.136,32768.0,65536.0,182524.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",66,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.08,569.216,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",67,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.368,571.5840000000001,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,68,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69274976.0,229376.0,76.64,648.224,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164843.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",69,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,2.976,651.2,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",70,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.208,653.408,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",71,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.728,659.136,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,72,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,59.616,718.752,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",73,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.592,721.3439999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",74,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.72,724.064,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",75,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.56,726.6239999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",76,262144.0,8708096.0,0,0,0.0,8708096.0,8708096.0,68096.0,128.0,0.99812382739212,98304.0,32768.0,13.376,739.9999999999999,6586368.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,77,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18920608.0,98304.0,23.296,763.2959999999999,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591269.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",78,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.592,765.8879999999999,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",79,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.08,767.968,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",80,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.664,773.632,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),81,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,75.776,849.4079999999999,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",82,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.72,852.1279999999999,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",83,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.144,854.2719999999999,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",84,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.144,856.4159999999999,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",85,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,858.496,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",86,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.464,860.96,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",87,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.24,863.2,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",88,182684.0,463672.0,0,0,0.0,463672.0,463672.0,0.0,512.0,0.0,131072.0,131072.0,2.176,865.3760000000001,32768.0,65536.0,182684.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",89,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.112,867.488,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",90,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.208,869.696,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,91,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69274144.0,229376.0,76.64,946.336,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164817.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",92,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,3.104,949.44,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",93,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.08,951.5200000000001,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",94,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.696,957.2160000000001,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,95,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,59.36,1016.5760000000001,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",96,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.752,1019.3280000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",97,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.688,1022.0160000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",98,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.56,1024.576,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",99,262144.0,8708096.0,0,0,0.0,8708096.0,8708096.0,68096.0,128.0,0.99812382739212,98304.0,32768.0,13.6,1038.176,6586368.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,100,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18923296.0,98304.0,23.104,1061.28,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591353.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",101,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.528,1063.808,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",102,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.208,1066.016,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",103,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.664,1071.68,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),104,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,75.552,1147.232,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",105,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.752,1149.984,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",106,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,1152.0639999999999,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",107,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.08,1154.1439999999998,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",108,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,1156.2559999999999,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",109,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.304,1158.56,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",110,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,1160.6399999999999,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",111,182512.0,463328.0,0,0,0.0,463328.0,463328.0,0.0,512.0,0.0,131072.0,131072.0,2.24,1162.8799999999999,32768.0,65536.0,182512.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",112,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.048,1164.9279999999999,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",113,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.24,1167.168,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,114,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69273664.0,229376.0,76.48,1243.648,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164802.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",115,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,3.008,1246.656,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",116,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.112,1248.768,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",117,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.856,1254.624,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,118,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,60.064,1314.688,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",119,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.624,1317.3120000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",120,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.688,1320.0000000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",121,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.592,1322.5920000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",122,262144.0,8708096.0,0,0,0.0,8708096.0,8708096.0,68096.0,128.0,0.99812382739212,98304.0,32768.0,13.504,1336.0960000000002,6586368.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,123,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18922752.0,98304.0,23.328,1359.4240000000002,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591336.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",124,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.656,1362.0800000000002,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",125,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.112,1364.1920000000002,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",126,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.792,1369.9840000000002,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),127,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,76.064,1446.0480000000002,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",128,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.72,1448.7680000000003,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",129,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,1450.8480000000002,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",130,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.08,1452.928,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",131,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,1455.008,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",132,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.304,1457.3120000000001,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",133,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,1459.392,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",134,182844.0,463992.0,0,0,0.0,463992.0,463992.0,0.0,512.0,0.0,131072.0,131072.0,2.208,1461.6000000000001,32768.0,65536.0,182844.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",135,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.112,1463.7120000000002,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",136,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.24,1465.9520000000002,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,137,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69271520.0,229376.0,76.448,1542.4000000000003,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164735.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",138,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,2.912,1545.3120000000004,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",139,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.08,1547.3920000000003,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",140,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.76,1553.1520000000003,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,141,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,60.128,1613.2800000000002,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",142,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.624,1615.9040000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",143,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.592,1618.4960000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",144,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.72,1621.2160000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",145,262144.0,8708096.0,0,0,0.0,8708096.0,8708096.0,68096.0,128.0,0.99812382739212,98304.0,32768.0,13.504,1634.7200000000003,6586368.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,146,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18922816.0,98304.0,23.04,1657.7600000000002,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591338.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",147,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.624,1660.3840000000002,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",148,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.176,1662.5600000000002,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",149,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.696,1668.256,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),150,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,75.84,1744.096,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",151,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.752,1746.848,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",152,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,1748.9279999999999,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",153,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.176,1751.1039999999998,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",154,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,1753.1839999999997,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",155,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.4,1755.5839999999998,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",156,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.144,1757.7279999999998,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",157,183156.0,464616.0,0,0,0.0,464616.0,464616.0,0.0,512.0,0.0,131072.0,131072.0,2.176,1759.9039999999998,32768.0,65536.0,183156.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",158,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.112,1762.0159999999998,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",159,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.24,1764.2559999999999,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,160,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69274688.0,229376.0,76.576,1840.8319999999999,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164834.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",161,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,3.232,1844.0639999999999,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",162,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.048,1846.1119999999999,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",163,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.664,1851.7759999999998,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,164,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,59.168,1910.9439999999997,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",165,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.656,1913.5999999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",166,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.592,1916.1919999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",167,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.624,1918.8159999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",168,262144.0,8708096.0,0,0,0.0,8708096.0,8708096.0,68096.0,128.0,0.99812382739212,98304.0,32768.0,13.536,1932.3519999999999,6586368.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,169,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18926368.0,98304.0,23.264,1955.6159999999998,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591449.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",170,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.528,1958.1439999999998,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",171,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.08,1960.2239999999997,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",172,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.728,1965.9519999999998,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),173,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,75.392,2041.3439999999998,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",174,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.72,2044.0639999999999,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",175,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,2046.176,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",176,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.08,2048.256,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",177,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,2050.336,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",178,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.208,2052.544,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",179,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,2054.624,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",180,183088.0,464480.0,0,0,0.0,464480.0,464480.0,0.0,512.0,0.0,131072.0,131072.0,2.208,2056.832,32768.0,65536.0,183088.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",181,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.176,2059.008,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",182,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.208,2061.216,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,183,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69274112.0,229376.0,76.704,2137.92,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164816.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",184,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,2.912,2140.832,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",185,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.112,2142.944,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",186,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.792,2148.736,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,187,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,59.36,2208.096,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",188,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.656,2210.752,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",189,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.624,2213.3759999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",190,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.624,2215.9999999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",191,262144.0,8708096.0,0,0,0.0,8708096.0,8708096.0,68096.0,128.0,0.99812382739212,98304.0,32768.0,13.568,2229.5679999999998,6586368.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,192,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18925056.0,98304.0,22.944,2252.5119999999997,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591408.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",193,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.56,2255.0719999999997,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",194,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.112,2257.1839999999997,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",195,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.728,2262.912,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),196,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,75.808,2338.72,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",197,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.88,2341.6,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",198,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,2343.712,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",199,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.08,2345.792,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",200,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.048,2347.8399999999997,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",201,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.432,2350.2719999999995,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",202,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,2352.3839999999996,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",203,182996.0,464296.0,0,0,0.0,464296.0,464296.0,0.0,512.0,0.0,131072.0,131072.0,2.144,2354.5279999999993,32768.0,65536.0,182996.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",204,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.24,2356.767999999999,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",205,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.272,2359.039999999999,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,206,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69273248.0,229376.0,76.768,2435.807999999999,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164789.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",207,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,2.912,2438.719999999999,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",208,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.24,2440.9599999999987,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",209,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.728,2446.6879999999987,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_tn,210,3298332672.0,6619201536.0,0,0,0.0,6619201536.0,6619201536.0,15242112.0,3444376.0,0.8156755833412892,431863488.0,1606688.0,483.392,2930.0799999999986,9658368.0,12877824.0,3298332672.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13495734.0,50209.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",211,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.664,2931.743999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",212,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,64.0,128.0,2.848,2934.5919999999987,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,4.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",213,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,1.984,2936.5759999999987,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",214,0.0,201028.0,0,0,0.0,201028.0,201028.0,0.0,3158.0,0.0,804128.0,804128.0,3.04,2939.6159999999986,0.0,201028.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,25129.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",215,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.632,2941.2479999999987,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",216,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,58752.0,3.968,2945.2159999999985,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1836.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",217,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,82608.0,0.15193823915900131,5134592.0,0.0,5.504,2950.7199999999984,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",218,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,58368.0,3.648,2954.3679999999986,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1824.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",219,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83408.0,0.15070055392636036,5134592.0,0.0,5.472,2959.839999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",220,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,58560.0,3.616,2963.4559999999988,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1830.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",221,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83608.0,0.15039427688805787,5134592.0,0.0,5.536,2968.991999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",222,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,57600.0,3.84,2972.831999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1800.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",223,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83208.0,0.1510080809729818,5134592.0,128.0,5.536,2978.367999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,4.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",224,0.0,0.0,0,0,0.0,0.0,0.0,0.0,21.0,0.0,6432.0,800.0,2.4,2980.767999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,201.0,25.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",225,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.632,2982.399999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",226,0.0,0.0,0,0,0.0,0.0,0.0,497.0,22.0,0.9576107899807321,800.0,0.0,3.584,2985.983999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",227,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.696,2987.679999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",228,0.0,0.0,0,0,0.0,0.0,0.0,497.0,22.0,0.9576107899807321,800.0,0.0,3.552,2991.231999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",229,0.0,0.0,0,0,0.0,0.0,0.0,41088.0,13012.0,0.7594824399260628,831456.0,8608.0,5.792,2997.023999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25983.0,269.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",230,0.0,0.0,0,0,0.0,0.0,0.0,1832.0,32.0,0.9828326180257511,2560.0,544.0,6.496,3003.519999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,80.0,17.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",231,0.0,0.0,0,0,0.0,0.0,0.0,0.0,18849.0,0.0,814496.0,87296.0,3.616,3007.135999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25453.0,2728.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",232,0.0,0.0,0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,3.328,3010.463999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,31412.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",233,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6283.0,0.0,0.0,1608224.0,2.656,3013.119999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,50257.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",234,0.0,0.0,0,0,0.0,0.0,0.0,98304.0,6283.0,0.9399256121697726,804128.0,0.0,4.288,3017.407999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",235,0.0,0.0,0,0,0.0,0.0,0.0,156.0,64.0,0.7090909090909091,8192.0,0.0,2.144,3019.5519999999988,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",236,0.0,0.0,0,0,0.0,0.0,0.0,79683.0,30262.0,0.724753285733776,3062976.0,2033792.0,10.432,3029.9839999999986,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,95718.0,63556.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",237,0.0,0.0,0,0,0.0,0.0,0.0,23142.0,38194.0,0.37729881309508284,3096512.0,2479936.0,8.448,3038.4319999999984,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,96766.0,77498.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",238,0.0,0.0,0,0,0.0,0.0,0.0,23883.0,38551.0,0.38253195374315274,3099968.0,2126176.0,8.928,3047.3599999999983,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,96874.0,66443.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",239,0.0,0.0,0,0,0.0,0.0,0.0,23883.0,37864.0,0.386788022090142,3086016.0,1625216.0,8.96,3056.3199999999983,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,96438.0,50788.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",240,0.0,0.0,0,0,0.0,0.0,0.0,14336.0,6283.0,0.6952810514573937,1608224.0,0.0,4.256,3060.575999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",241,0.0,0.0,0,0,0.0,0.0,0.0,39.0,16.0,0.7090909090909091,2048.0,0.0,2.144,3062.719999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",242,0.0,0.0,0,0,0.0,0.0,0.0,20059.0,18743.0,0.5169578887686201,2098336.0,1392256.0,7.808,3070.527999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,65573.0,43508.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",243,0.0,0.0,0,0,0.0,0.0,0.0,0.0,25132.0,0.0,2427648.0,2412352.0,5.184,3075.711999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,75864.0,75386.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",244,2814392.0,6655044.0,0,0,0.0,6655044.0,6655044.0,528.0,6704.0,0.07300884955752213,2282112.0,752896.0,23.84,3099.5519999999983,825232.0,201028.0,2814392.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,71316.0,23528.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",245,0.0,1024200.0,0,0,0.0,1024200.0,1024200.0,112284.0,12568.0,0.8993368147887099,804352.0,620992.0,72.064,3171.615999999998,1024200.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25136.0,19406.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",246,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3158.0,0.0,804128.0,200800.0,3.04,3174.655999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,6275.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",247,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,128.0,1.888,3176.543999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,4.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",248,0.0,0.0,0,0,0.0,0.0,0.0,0.0,18849.0,0.0,1809280.0,85920.0,7.584,3184.127999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,56540.0,2685.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",249,0.0,0.0,0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,3.424,3187.551999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,31412.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",250,2814392.0,6655044.0,0,0,0.0,6655044.0,6655044.0,528.0,6704.0,0.07300884955752213,2276608.0,753088.0,23.68,3211.2319999999977,825232.0,201028.0,2814392.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,71144.0,23534.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",251,0.0,0.0,0,0,0.0,0.0,0.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.368,3217.5999999999976,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",252,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.112,3219.7119999999977,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",253,0.0,0.0,0,0,0.0,0.0,0.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.304,3226.015999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",254,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.08,3228.0959999999977,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",255,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.112,3230.207999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",256,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.912,3233.1199999999976,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",257,0.0,220484.0,0,0,0.0,220484.0,220484.0,320.0,1582.0,0.16824395373291273,804224.0,128.0,10.88,3243.9999999999977,220484.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25132.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",258,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.464,3246.4639999999977,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",259,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.2,3249.6639999999975,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",260,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,3251.6799999999976,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",261,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.944,3254.6239999999975,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",262,1769472.0,3941000.0,0,0,0.0,3941000.0,3941000.0,0.0,6283.0,0.0,0.0,804128.0,3.968,3258.5919999999974,0.0,402056.0,1769472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,25129.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",263,1005140.0,2010280.0,0,0,0.0,2010280.0,2010280.0,0.0,4737.0,0.0,1608256.0,0.0,4.928,3263.5199999999973,0.0,0.0,1005140.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50258.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",264,0.0,0.0,0,0,0.0,0.0,0.0,640.0,1582.0,0.28802880288028804,804448.0,128.0,15.648,3279.1679999999974,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25139.0,4.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",265,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.016,3281.1839999999975,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",266,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,1.984,3283.1679999999974,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",267,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.24,3285.407999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",268,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.048,3287.455999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",269,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,64.0,128.0,2.56,3290.015999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,4.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",270,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.664,3291.679999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",271,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.632,3293.311999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",272,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.048,3295.359999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",273,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,3297.055999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",274,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,32.0,2.432,3299.4879999999966,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",275,0.0,0.0,0,0,0.0,0.0,0.0,0.0,5.0,0.0,32.0,32.0,4.896,3304.383999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",276,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.112,3306.495999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",277,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,3308.511999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",278,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.848,3311.359999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",279,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.264,3314.623999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",280,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.08,3316.703999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",281,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.112,3318.815999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::index_elementwise_kernel<128, 4, void native::gpu_index_kernel<void native::index_kernel_impl<native::OpaqueType<8>>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>, const T1 &)::[lambda(int) (instance 1)]>(long, T3)",282,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,32.0,3.264,3322.079999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",283,0.0,0.0,0,0,0.0,0.0,0.0,56.0,4.0,0.9333333333333333,64.0,32.0,2.464,3324.543999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",284,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,64.0,2.112,3326.655999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,2.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",285,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,2.048,3328.703999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",286,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,0.0,2.016,3330.719999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,0.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",287,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,2.24,3332.959999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",288,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,25088.0,32768.0,3.936,3336.895999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,784.0,1024.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",289,0.0,0.0,0,0,0.0,0.0,0.0,0.0,768.0,0.0,8704.0,32768.0,3.424,3340.319999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,272.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",290,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.08,3342.399999999997,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",291,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,1.984,3344.383999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",292,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.36,3347.743999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",293,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.696,3353.439999999997,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,294,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,60.416,3413.855999999997,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",295,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,4.0,3417.855999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",296,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,3.936,3421.791999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",297,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.592,3424.3839999999973,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",298,262144.0,8716288.0,0,0,0.0,8716288.0,8716288.0,68096.0,128.0,0.99812382739212,163840.0,32768.0,13.696,3438.079999999997,6594560.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5120.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,299,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18927008.0,98304.0,23.104,3461.183999999997,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591469.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",300,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.592,3463.775999999997,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",301,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.08,3465.855999999997,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",302,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.696,3471.551999999997,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),303,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,75.712,3547.263999999997,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",304,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.912,3550.1759999999967,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",305,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.272,3552.4479999999967,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",306,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.176,3554.6239999999966,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",307,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,3556.7039999999965,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",308,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.24,3558.9439999999963,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",309,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,3561.0239999999962,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",310,183041.0,464386.0,0,0,0.0,464386.0,464386.0,0.0,512.0,0.0,131072.0,131072.0,2.176,3563.199999999996,32768.0,65536.0,183041.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",311,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.144,3565.343999999996,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",312,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.432,3567.7759999999957,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,313,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69271200.0,229376.0,76.384,3644.1599999999958,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164725.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",314,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,2.944,3647.1039999999957,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",315,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,4.352,3651.4559999999956,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",316,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.92,3657.3759999999957,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,317,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,58.592,3715.9679999999958,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",318,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,4.0,3719.9679999999958,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",319,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,4.064,3724.0319999999956,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",320,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.624,3726.6559999999954,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",321,262144.0,8716288.0,0,0,0.0,8716288.0,8716288.0,68096.0,128.0,0.99812382739212,163840.0,32768.0,13.568,3740.2239999999956,6594560.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5120.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,322,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18922592.0,98304.0,23.008,3763.2319999999954,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591331.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",323,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.688,3765.9199999999955,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",324,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.048,3767.9679999999953,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",325,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.888,3773.855999999995,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),326,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,76.0,3849.855999999995,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",327,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.88,3852.7359999999953,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",328,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,3854.8479999999954,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",329,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.112,3856.9599999999955,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",330,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,3859.0399999999954,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",331,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.4,3861.4399999999955,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",332,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.176,3863.6159999999954,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",333,182721.0,463746.0,0,0,0.0,463746.0,463746.0,0.0,512.0,0.0,131072.0,131072.0,2.176,3865.7919999999954,32768.0,65536.0,182721.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",334,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.08,3867.8719999999953,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",335,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.272,3870.1439999999952,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,336,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69271360.0,229376.0,76.224,3946.3679999999954,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164730.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",337,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,2.944,3949.3119999999954,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",338,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.144,3951.455999999995,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",339,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.664,3957.1199999999953,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,340,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,58.88,4015.9999999999955,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",341,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,3.936,4019.9359999999956,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",342,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,3.968,4023.9039999999954,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",343,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.624,4026.5279999999952,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",344,262144.0,8716288.0,0,0,0.0,8716288.0,8716288.0,68096.0,128.0,0.99812382739212,163840.0,32768.0,13.44,4039.9679999999953,6594560.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5120.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,345,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18921536.0,98304.0,23.648,4063.6159999999954,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591298.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",346,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.592,4066.2079999999955,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",347,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.048,4068.2559999999953,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",348,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.696,4073.951999999995,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),349,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,76.224,4150.175999999995,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",350,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.848,4153.023999999995,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",351,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.208,4155.2319999999945,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",352,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.08,4157.311999999994,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",353,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,4159.391999999994,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",354,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.24,4161.631999999994,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",355,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.272,4163.903999999994,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",356,182544.0,463392.0,0,0,0.0,463392.0,463392.0,0.0,512.0,0.0,131072.0,131072.0,2.272,4166.175999999994,32768.0,65536.0,182544.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",357,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.08,4168.255999999994,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",358,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.4,4170.655999999994,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,359,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69270816.0,229376.0,76.384,4247.039999999994,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164713.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",360,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,3.136,4250.175999999994,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",361,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.208,4252.383999999994,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",362,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.76,4258.143999999994,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,363,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,58.976,4317.1199999999935,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",364,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,4.032,4321.151999999994,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",365,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,3.968,4325.1199999999935,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",366,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.56,4327.679999999994,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",367,262144.0,8716288.0,0,0,0.0,8716288.0,8716288.0,68096.0,128.0,0.99812382739212,163840.0,32768.0,13.44,4341.1199999999935,6594560.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5120.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,368,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18926784.0,98304.0,22.88,4363.999999999994,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591462.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",369,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.528,4366.527999999994,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",370,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.08,4368.607999999994,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",371,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.696,4374.303999999994,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),372,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,75.296,4449.599999999994,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",373,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.784,4452.383999999994,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",374,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,4454.463999999994,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",375,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.112,4456.575999999994,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",376,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,4458.655999999994,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",377,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.4,4461.055999999993,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",378,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,4463.167999999993,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",379,182786.0,463876.0,0,0,0.0,463876.0,463876.0,0.0,512.0,0.0,131072.0,131072.0,2.176,4465.343999999994,32768.0,65536.0,182786.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",380,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.048,4467.3919999999935,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",381,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.432,4469.823999999993,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,382,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69271808.0,229376.0,76.448,4546.271999999994,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164744.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",383,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,3.04,4549.3119999999935,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",384,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.112,4551.423999999994,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",385,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.696,4557.1199999999935,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,386,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,59.744,4616.863999999993,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",387,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,3.968,4620.831999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",388,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,4.0,4624.831999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",389,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.656,4627.487999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",390,262144.0,8716288.0,0,0,0.0,8716288.0,8716288.0,68096.0,128.0,0.99812382739212,163840.0,32768.0,13.536,4641.023999999993,6594560.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5120.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,391,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18924992.0,98304.0,22.944,4663.9679999999935,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591406.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",392,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.56,4666.527999999994,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",393,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.112,4668.639999999994,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",394,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.824,4674.463999999994,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),395,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,76.032,4750.495999999994,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",396,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.944,4753.439999999994,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",397,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,4755.551999999994,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",398,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.144,4757.6959999999945,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",399,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.144,4759.839999999995,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",400,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.368,4762.207999999995,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",401,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,4764.319999999995,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",402,182748.0,463800.0,0,0,0.0,463800.0,463800.0,0.0,512.0,0.0,131072.0,131072.0,2.176,4766.495999999996,32768.0,65536.0,182748.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",403,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.144,4768.639999999996,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",404,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.24,4770.879999999996,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,405,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69271104.0,229376.0,76.224,4847.103999999996,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164722.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",406,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,2.944,4850.047999999996,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",407,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.336,4852.383999999996,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",408,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.664,4858.047999999996,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,409,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,61.792,4919.8399999999965,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",410,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,4.0,4923.8399999999965,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",411,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,3.936,4927.775999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",412,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.592,4930.367999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",413,262144.0,8716288.0,0,0,0.0,8716288.0,8716288.0,68096.0,128.0,0.99812382739212,163840.0,32768.0,13.568,4943.935999999996,6594560.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5120.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,414,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18923776.0,98304.0,22.688,4966.623999999996,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591368.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",415,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.592,4969.215999999996,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",416,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.08,4971.295999999996,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",417,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.856,4977.1519999999955,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),418,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,77.344,5054.495999999996,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",419,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.912,5057.407999999996,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",420,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,5059.487999999996,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",421,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.112,5061.599999999996,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",422,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.048,5063.647999999996,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",423,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.56,5066.207999999996,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",424,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,5068.287999999996,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",425,182922.0,464148.0,0,0,0.0,464148.0,464148.0,0.0,512.0,0.0,131072.0,131072.0,2.336,5070.623999999996,32768.0,65536.0,182922.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",426,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.08,5072.703999999996,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",427,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.24,5074.943999999996,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,428,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69271168.0,229376.0,76.576,5151.519999999996,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164724.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",429,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,3.136,5154.655999999996,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",430,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.048,5156.703999999996,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",431,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.696,5162.399999999996,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,432,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,58.816,5221.215999999996,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",433,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,3.968,5225.183999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",434,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,4.0,5229.183999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",435,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.656,5231.839999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",436,262144.0,8716288.0,0,0,0.0,8716288.0,8716288.0,68096.0,128.0,0.99812382739212,163840.0,32768.0,13.664,5245.503999999995,6594560.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5120.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,437,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18927200.0,98304.0,22.976,5268.479999999995,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591475.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",438,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.528,5271.007999999995,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",439,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.112,5273.119999999995,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",440,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.664,5278.783999999995,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),441,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,77.824,5356.607999999995,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",442,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.752,5359.359999999995,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",443,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,5361.439999999995,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",444,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.08,5363.519999999995,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",445,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,3.328,5366.847999999995,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",446,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.24,5369.087999999995,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",447,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.176,5371.263999999996,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",448,183019.0,464342.0,0,0,0.0,464342.0,464342.0,0.0,512.0,0.0,131072.0,131072.0,2.176,5373.439999999996,32768.0,65536.0,183019.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",449,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.08,5375.519999999996,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",450,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.4,5377.9199999999955,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,451,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69270944.0,229376.0,76.256,5454.175999999996,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164717.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",452,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,2.944,5457.119999999996,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",453,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.08,5459.199999999996,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",454,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.664,5464.863999999996,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_nn,455,402849792.0,807075840.0,0,0,0.0,807075840.0,807075840.0,1815552.0,419328.0,0.8123711340206186,53575680.0,98304.0,59.392,5524.255999999996,589824.0,786432.0,402849792.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1674240.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",456,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,3.936,5528.1919999999955,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",457,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1024.0,0.0,65536.0,65536.0,4.0,5532.1919999999955,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",458,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,2.592,5534.783999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",459,262144.0,8716288.0,0,0,0.0,8716288.0,8716288.0,68096.0,128.0,0.99812382739212,163840.0,32768.0,13.472,5548.255999999995,6594560.0,1597440.0,262144.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,5120.0,1024.0
ampere_sgemm_32x32_sliced1x4_nn,460,138412032.0,278200320.0,0,0,0.0,278200320.0,278200320.0,724992.0,148736.0,0.8297685320832112,18927552.0,98304.0,23.36,5571.6159999999945,589824.0,786432.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591486.0,3072.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",461,0.0,40960.0,0,0,0.0,40960.0,40960.0,0.0,2560.0,0.0,106496.0,32768.0,2.624,5574.239999999994,32768.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3328.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",462,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.08,5576.319999999994,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",463,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.728,5582.047999999994,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),464,536870912.0,1074528256.0,0,0,0.0,1074528256.0,1074528256.0,1624320.0,3072.0,0.998112317130722,69206016.0,393216.0,76.736,5658.783999999994,0.0,786432.0,536870912.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2162688.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",465,0.0,163840.0,0,0,0.0,163840.0,163840.0,0.0,10240.0,0.0,425984.0,131072.0,2.752,5661.535999999995,131072.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13312.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",466,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.112,5663.647999999995,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",467,0.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.304,5665.951999999995,0.0,65536.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",468,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,5668.031999999995,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",469,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,2.336,5670.367999999995,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",470,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,2.08,5672.447999999995,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",471,183078.0,464460.0,0,0,0.0,464460.0,464460.0,0.0,512.0,0.0,131072.0,131072.0,2.208,5674.6559999999945,32768.0,65536.0,183078.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",472,32768.0,65536.0,0,0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,2.08,5676.735999999994,0.0,0.0,32768.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",473,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,2.24,5678.975999999994,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0
ampere_sgemm_128x32_sliced1x4_nn,474,543162368.0,1089536000.0,0,0,0.0,1089536000.0,1089536000.0,1825152.0,542656.0,0.7708192556153202,69269760.0,229376.0,76.64,5755.6159999999945,1376256.0,1835008.0,543162368.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2164680.0,7168.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",475,0.0,73728.0,0,0,0.0,73728.0,73728.0,0.0,3584.0,0.0,237568.0,32768.0,3.072,5758.687999999995,65536.0,8192.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,7424.0,1024.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",476,8192.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,2.08,5760.767999999995,0.0,0.0,8192.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",477,51268.0,175348.0,0,0,0.0,175348.0,175348.0,80.0,712.0,0.10101010101010101,98304.0,33024.0,5.856,5766.623999999994,51760.0,21052.0,51268.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0
ampere_sgemm_64x32_sliced1x4_tn,478,3298332672.0,6619201536.0,0,0,0.0,6619201536.0,6619201536.0,15242112.0,3444376.0,0.8156755833412892,432942912.0,1600224.0,483.104,6249.727999999995,9658368.0,12877824.0,3298332672.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,13529466.0,50007.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",479,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,6251.455999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",480,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,96.0,160.0,2.56,6254.015999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,5.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",481,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,1.984,6255.999999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",482,0.0,201028.0,0,0,0.0,201028.0,201028.0,0.0,3158.0,0.0,804128.0,804128.0,2.848,6258.847999999995,0.0,201028.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,25129.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",483,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.792,6260.639999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",484,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,56640.0,3.68,6264.319999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1770.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",485,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,82608.0,0.15193823915900131,5134592.0,0.0,5.408,6269.727999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",486,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,56896.0,3.904,6273.631999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1778.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",487,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83358.0,0.15077731820126736,5134592.0,0.0,5.92,6279.551999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",488,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,55936.0,3.616,6283.167999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1748.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",489,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83408.0,0.15070055392636036,5134592.0,0.0,5.504,6288.671999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",490,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,56000.0,3.744,6292.4159999999965,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1750.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",491,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83308.0,0.15085416072083827,5134592.0,128.0,5.376,6297.791999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,4.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",492,0.0,0.0,0,0,0.0,0.0,0.0,0.0,21.0,0.0,6432.0,800.0,2.4,6300.191999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,201.0,25.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",493,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.664,6301.855999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",494,0.0,0.0,0,0,0.0,0.0,0.0,497.0,22.0,0.9576107899807321,800.0,0.0,3.616,6305.471999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",495,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.664,6307.135999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",496,0.0,0.0,0,0,0.0,0.0,0.0,497.0,22.0,0.9576107899807321,800.0,0.0,3.584,6310.719999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",497,0.0,0.0,0,0,0.0,0.0,0.0,64800.0,13016.0,0.8327336280456461,831456.0,8544.0,6.112,6316.831999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25983.0,267.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",498,0.0,0.0,0,0,0.0,0.0,0.0,1832.0,32.0,0.9828326180257511,2560.0,544.0,6.4,6323.231999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,80.0,17.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",499,0.0,0.0,0,0,0.0,0.0,0.0,0.0,18849.0,0.0,814496.0,88288.0,3.648,6326.879999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25453.0,2759.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",500,0.0,0.0,0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,3.264,6330.143999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,31412.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",501,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6283.0,0.0,0.0,1608224.0,2.624,6332.7679999999955,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,50257.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",502,0.0,0.0,0,0,0.0,0.0,0.0,98304.0,6283.0,0.9399256121697726,804128.0,0.0,4.192,6336.9599999999955,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",503,0.0,0.0,0,0,0.0,0.0,0.0,156.0,64.0,0.7090909090909091,8192.0,0.0,2.112,6339.071999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",504,0.0,0.0,0,0,0.0,0.0,0.0,80583.0,29578.0,0.7315020742367989,3112256.0,2053056.0,10.432,6349.503999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97258.0,64158.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",505,0.0,0.0,0,0,0.0,0.0,0.0,25842.0,38002.0,0.40476787168723766,3097792.0,1728512.0,8.864,6358.367999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,96806.0,54016.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",506,0.0,0.0,0,0,0.0,0.0,0.0,23883.0,38746.0,0.3813409123568954,3108800.0,1963008.0,8.928,6367.295999999995,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97150.0,61344.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",507,0.0,0.0,0,0,0.0,0.0,0.0,23883.0,38641.0,0.3819813191734374,3094976.0,1550016.0,9.056,6376.351999999994,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,96718.0,48438.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",508,0.0,0.0,0,0,0.0,0.0,0.0,14336.0,6283.0,0.6952810514573937,1608224.0,0.0,4.48,6380.831999999994,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",509,0.0,0.0,0,0,0.0,0.0,0.0,39.0,16.0,0.7090909090909091,2048.0,0.0,2.144,6382.975999999994,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",510,0.0,0.0,0,0,0.0,0.0,0.0,20059.0,17888.0,0.5286056868790682,2102176.0,1391200.0,7.968,6390.943999999994,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,65693.0,43475.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",511,0.0,0.0,0,0,0.0,0.0,0.0,0.0,25132.0,0.0,2427328.0,2412352.0,5.28,6396.223999999994,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,75854.0,75386.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",512,2814392.0,6655044.0,0,0,0.0,6655044.0,6655044.0,528.0,6704.0,0.07300884955752213,2279808.0,752704.0,23.584,6419.807999999994,825232.0,201028.0,2814392.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,71244.0,23522.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",513,0.0,1024200.0,0,0,0.0,1024200.0,1024200.0,112284.0,12568.0,0.8993368147887099,804320.0,618752.0,72.128,6491.935999999993,1024200.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25135.0,19336.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",514,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3158.0,0.0,804128.0,200800.0,3.072,6495.007999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,6275.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",515,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,128.0,1.856,6496.863999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,4.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",516,0.0,0.0,0,0,0.0,0.0,0.0,0.0,18849.0,0.0,1809280.0,86656.0,7.776,6504.639999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,56540.0,2708.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",517,0.0,0.0,0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,3.552,6508.191999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,31412.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",518,2814392.0,6655044.0,0,0,0.0,6655044.0,6655044.0,528.0,6704.0,0.07300884955752213,2278272.0,753344.0,23.776,6531.967999999993,825232.0,201028.0,2814392.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,71196.0,23542.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",519,0.0,0.0,0,0,0.0,0.0,0.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.4,6538.367999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",520,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.112,6540.479999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",521,0.0,0.0,0,0,0.0,0.0,0.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.208,6546.687999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",522,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.112,6548.799999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",523,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.016,6550.815999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",524,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.976,6553.791999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",525,0.0,220484.0,0,0,0.0,220484.0,220484.0,320.0,1582.0,0.16824395373291273,804256.0,128.0,11.072,6564.863999999991,220484.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",526,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,1.952,6566.815999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",527,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.36,6570.175999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",528,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,1.984,6572.159999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",529,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.944,6575.103999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",530,1769472.0,3941000.0,0,0,0.0,3941000.0,3941000.0,0.0,6283.0,0.0,0.0,804128.0,3.904,6579.0079999999925,0.0,402056.0,1769472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,25129.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",531,1005140.0,2010280.0,0,0,0.0,2010280.0,2010280.0,0.0,4737.0,0.0,1608256.0,0.0,4.864,6583.871999999992,0.0,0.0,1005140.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50258.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",532,0.0,0.0,0,0,0.0,0.0,0.0,640.0,1582.0,0.28802880288028804,804608.0,128.0,15.808,6599.679999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25144.0,4.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",533,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,1.984,6601.6639999999925,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",534,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,6603.679999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",535,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.208,6605.887999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",536,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.016,6607.903999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",537,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,96.0,160.0,2.56,6610.463999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,5.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",538,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.664,6612.1279999999915,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",539,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.664,6613.791999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",540,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.048,6615.839999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",541,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.632,6617.471999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",542,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,128.0,32.0,2.24,6619.71199999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",543,0.0,0.0,0,0,0.0,0.0,0.0,0.0,5.0,0.0,32.0,32.0,4.736,6624.44799999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",544,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,1.984,6626.431999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",545,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,6628.44799999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",546,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.88,6631.32799999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",547,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.232,6634.55999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",548,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,6636.57599999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
