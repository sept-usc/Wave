Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.856,1.856,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",2,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,3.552,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",3,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,5.28,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",4,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.016,7.296,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",5,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.304,9.6,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",6,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.304,11.904,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::unrolled_elementwise_kernel<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)], std::array<char *, 2>, 8, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<1>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",7,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.36,15.264,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::sum_functor<long, long, long>::operator ()(at::TensorIterator &)::[lambda(long, long) (instance 1)]>, unsigned int, long, 4, 4>>(T3)",8,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.2,18.464,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",9,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.208,20.671999999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",10,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,22.4,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",11,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,24.128,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void at_cuda_detail::DeviceScanInitKernel<at_cuda_detail::ScanTileState<long, 1>>(T1, int)",12,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.728,25.856,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanKernel<at_cuda_detail::DeviceScanPolicy<long, std::plus<long>>::Policy900, const long *, long *, at_cuda_detail::ScanTileState<long, 1>, std::plus<long>, at_cuda_detail::NullType, unsigned int, long, 0>(T2, T3, T4, int, T5, T6, T7)",13,0.0,0.0,0,0,0.0,0.0,0.0,129.0,6.0,0.9555555555555556,32.0,32.0,2.464,28.32,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",14,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,30.336,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",15,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.208,32.544,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",16,0.0,0.0,0,0,0.0,0.0,0.0,56.0,4.0,0.9333333333333333,32.0,32.0,2.496,35.04,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",17,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,37.056,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",18,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,39.104,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",19,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,0.0,2.144,41.248,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,0.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",20,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,4352.0,16384.0,3.904,45.152,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,136.0,512.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",21,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,4352.0,16384.0,4.032,49.184,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,136.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",22,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,51.263999999999996,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",23,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,53.279999999999994,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",24,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.296,56.57599999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",25,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.32,60.895999999999994,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),26,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.792,78.68799999999999,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",27,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.688,81.37599999999999,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",28,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,83.99999999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",29,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.656,86.65599999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",30,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,89.27999999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",31,131072.0,4354048.0,0,0,0.0,4354048.0,4354048.0,34048.0,64.0,0.99812382739212,49152.0,16384.0,12.8,102.07999999999998,3293184.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,32,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.632,111.71199999999999,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",33,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.4,114.112,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",34,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.208,116.32,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",35,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.224,120.544,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,36,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17390016.0,458752.0,22.528,143.072,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543438.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",37,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.136,146.208,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",38,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,148.256,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",39,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.048,150.304,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",40,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.016,152.32,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",41,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.144,154.464,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",42,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.144,156.608,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",43,94244.0,237640.0,0,0,0.0,237640.0,237640.0,0.0,256.0,0.0,65536.0,65536.0,2.176,158.784,16384.0,32768.0,94244.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",44,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.048,160.832,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",45,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.144,162.976,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,46,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18937696.0,114688.0,22.656,185.632,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591803.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",47,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.88,188.512,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",48,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.048,190.56,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",49,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.224,194.784,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),50,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,18.24,213.024,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",51,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.688,215.712,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",52,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,218.33599999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",53,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.656,220.992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",54,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.72,223.712,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",55,131072.0,4354048.0,0,0,0.0,4354048.0,4354048.0,34048.0,64.0,0.99812382739212,49152.0,16384.0,11.648,235.35999999999999,3293184.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,56,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,10.112,245.47199999999998,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",57,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.336,247.808,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",58,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.112,249.92,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",59,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,254.176,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,60,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17394624.0,458752.0,22.272,276.448,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543582.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",61,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.136,279.584,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",62,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,281.632,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",63,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.016,283.648,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",64,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.144,285.79200000000003,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",65,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.144,287.93600000000004,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",66,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.112,290.04800000000006,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",67,94056.0,237264.0,0,0,0.0,237264.0,237264.0,0.0,256.0,0.0,65536.0,65536.0,2.208,292.2560000000001,16384.0,32768.0,94056.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",68,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.08,294.33600000000007,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",69,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.176,296.51200000000006,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,70,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18938080.0,114688.0,22.592,319.10400000000004,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591815.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",71,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.88,321.98400000000004,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",72,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.112,324.09600000000006,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",73,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.224,328.32000000000005,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),74,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.856,346.17600000000004,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",75,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.688,348.86400000000003,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",76,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,351.48800000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",77,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.656,354.14400000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",78,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,356.7680000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",79,131072.0,4354048.0,0,0,0.0,4354048.0,4354048.0,34048.0,64.0,0.99812382739212,49152.0,16384.0,11.616,368.38400000000007,3293184.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,80,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.664,378.04800000000006,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",81,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.368,380.41600000000005,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",82,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.112,382.5280000000001,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",83,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.224,386.75200000000007,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,84,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17382112.0,458752.0,22.976,409.72800000000007,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543191.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",85,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.328,413.05600000000004,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",86,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.208,415.26400000000007,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",87,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.112,417.3760000000001,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",88,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,419.4240000000001,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",89,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.144,421.5680000000001,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",90,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.08,423.6480000000001,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",91,94124.0,237400.0,0,0,0.0,237400.0,237400.0,0.0,256.0,0.0,65536.0,65536.0,2.176,425.82400000000007,16384.0,32768.0,94124.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",92,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.08,427.90400000000005,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",93,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.144,430.04800000000006,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,94,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18940736.0,114688.0,22.656,452.70400000000006,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591898.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",95,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,3.008,455.71200000000005,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",96,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,457.79200000000003,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",97,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.32,462.112,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),98,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.632,479.744,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",99,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.688,482.432,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",100,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,485.05600000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",101,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.656,487.71200000000005,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",102,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,490.33600000000007,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",103,131072.0,4354048.0,0,0,0.0,4354048.0,4354048.0,34048.0,64.0,0.99812382739212,49152.0,16384.0,11.648,501.9840000000001,3293184.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,104,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.824,511.8080000000001,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",105,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.496,514.3040000000001,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",106,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.272,516.5760000000001,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",107,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,520.8320000000001,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,108,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17383168.0,458752.0,22.144,542.9760000000001,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543224.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",109,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.168,546.1440000000001,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",110,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,548.1920000000001,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",111,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.048,550.2400000000001,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",112,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,552.2880000000001,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",113,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.144,554.4320000000001,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",114,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.08,556.5120000000002,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",115,94024.0,237200.0,0,0,0.0,237200.0,237200.0,0.0,256.0,0.0,65536.0,65536.0,2.176,558.6880000000002,16384.0,32768.0,94024.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",116,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.048,560.7360000000002,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",117,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.144,562.8800000000002,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,118,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18938720.0,114688.0,22.592,585.4720000000002,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591835.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",119,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.88,588.3520000000002,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",120,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.048,590.4000000000002,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",121,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,594.6560000000002,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),122,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.664,612.3200000000002,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",123,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.88,615.2000000000002,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",124,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,617.8240000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",125,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.688,620.5120000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",126,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,623.1360000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",127,131072.0,4354048.0,0,0,0.0,4354048.0,4354048.0,34048.0,64.0,0.99812382739212,49152.0,16384.0,11.744,634.8800000000002,3293184.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,128,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.568,644.4480000000002,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",129,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.368,646.8160000000003,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",130,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.144,648.9600000000003,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",131,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,653.2160000000002,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,132,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17381664.0,458752.0,22.144,675.3600000000002,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543177.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",133,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.328,678.6880000000002,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",134,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,680.7360000000002,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",135,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.048,682.7840000000002,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",136,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,684.8320000000002,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",137,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.368,687.2000000000003,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",138,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.112,689.3120000000002,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",139,94228.0,237608.0,0,0,0.0,237608.0,237608.0,0.0,256.0,0.0,65536.0,65536.0,2.208,691.5200000000002,16384.0,32768.0,94228.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",140,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.048,693.5680000000002,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",141,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.496,696.0640000000002,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,142,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18937504.0,114688.0,22.496,718.5600000000002,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591797.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",143,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.848,721.4080000000001,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",144,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,723.4880000000002,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",145,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.224,727.7120000000002,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),146,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.728,745.4400000000002,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",147,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.688,748.1280000000002,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",148,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.656,750.7840000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",149,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.656,753.44,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",150,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.656,756.096,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",151,131072.0,4354048.0,0,0,0.0,4354048.0,4354048.0,34048.0,64.0,0.99812382739212,49152.0,16384.0,11.712,767.808,3293184.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,152,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.76,777.568,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",153,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.432,780.0,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",154,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.112,782.112,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",155,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,786.3679999999999,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,156,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17383136.0,458752.0,22.336,808.704,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543223.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",157,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.264,811.968,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",158,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,814.016,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",159,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.208,816.2239999999999,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",160,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,818.2719999999999,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",161,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.112,820.3839999999999,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",162,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.24,822.6239999999999,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",163,94308.0,237768.0,0,0,0.0,237768.0,237768.0,0.0,256.0,0.0,65536.0,65536.0,2.24,824.8639999999999,16384.0,32768.0,94308.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",164,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.048,826.9119999999999,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",165,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.208,829.1199999999999,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,166,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18938336.0,114688.0,23.2,852.3199999999999,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591823.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",167,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.976,855.2959999999999,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",168,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.112,857.4079999999999,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",169,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.288,861.6959999999999,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),170,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.792,879.4879999999999,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",171,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.848,882.3359999999999,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",172,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,884.9599999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",173,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.72,887.68,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",174,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.656,890.3359999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",175,131072.0,4354048.0,0,0,0.0,4354048.0,4354048.0,34048.0,64.0,0.99812382739212,49152.0,16384.0,11.712,902.0479999999999,3293184.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,176,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.792,911.8399999999999,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",177,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.336,914.1759999999999,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",178,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.144,916.3199999999999,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",179,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.224,920.544,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,180,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17382752.0,458752.0,22.464,943.008,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543211.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",181,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.168,946.176,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",182,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.08,948.2560000000001,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",183,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.016,950.272,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",184,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.208,952.48,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",185,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.304,954.784,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",186,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.112,956.896,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",187,94052.0,237256.0,0,0,0.0,237256.0,237256.0,0.0,256.0,0.0,65536.0,65536.0,2.176,959.072,16384.0,32768.0,94052.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",188,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.08,961.152,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",189,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.336,963.488,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,190,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18939968.0,114688.0,23.488,986.9760000000001,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591874.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",191,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.88,989.8560000000001,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",192,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.048,991.9040000000001,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",193,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.224,996.1280000000002,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),194,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.632,1013.7600000000001,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",195,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.688,1016.4480000000001,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",196,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,1019.0720000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",197,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.688,1021.7600000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",198,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.592,1024.352,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",199,131072.0,4354048.0,0,0,0.0,4354048.0,4354048.0,34048.0,64.0,0.99812382739212,49152.0,16384.0,11.744,1036.096,3293184.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,200,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.888,1045.984,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",201,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.336,1048.32,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",202,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.112,1050.432,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",203,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.224,1054.656,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,204,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17384160.0,458752.0,22.592,1077.248,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543255.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",205,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.232,1080.48,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",206,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,1082.528,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",207,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.016,1084.544,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",208,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.112,1086.6560000000002,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",209,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.272,1088.928,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",210,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.08,1091.008,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",211,94172.0,237496.0,0,0,0.0,237496.0,237496.0,0.0,256.0,0.0,65536.0,65536.0,2.176,1093.184,16384.0,32768.0,94172.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",212,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.08,1095.264,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",213,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.112,1097.376,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,214,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18939360.0,114688.0,22.656,1120.032,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591855.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",215,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.912,1122.944,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",216,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,1125.024,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",217,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,1129.28,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_64x32_sliced1x4_tn,218,1648361472.0,3307991040.0,0,0,0.0,3307991040.0,3307991040.0,7621056.0,1715116.0,0.8162934444652477,213691776.0,770144.0,246.816,1376.096,4829184.0,6438912.0,1648361472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6677868.0,24067.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",219,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.728,1377.824,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",220,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,64.0,128.0,2.592,1380.4160000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,4.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",221,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,1382.4640000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",222,0.0,201028.0,0,0,0.0,201028.0,201028.0,0.0,3158.0,0.0,804128.0,804128.0,3.008,1385.4720000000002,0.0,201028.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,25129.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",223,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,1387.1680000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",224,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,56576.0,3.712,1390.88,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1768.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",225,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,82608.0,0.15193823915900131,5134592.0,0.0,5.76,1396.64,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",226,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,57280.0,3.648,1400.288,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1790.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",227,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83608.0,0.15039427688805787,5134592.0,0.0,5.76,1406.048,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",228,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,56192.0,3.84,1409.888,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1756.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",229,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83608.0,0.15039427688805787,5134592.0,0.0,5.76,1415.648,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",230,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,55424.0,3.712,1419.36,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1732.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",231,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83208.0,0.1510080809729818,5134592.0,128.0,5.6,1424.9599999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,4.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",232,0.0,0.0,0,0,0.0,0.0,0.0,0.0,21.0,0.0,6432.0,800.0,2.432,1427.3919999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,201.0,25.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",233,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.728,1429.12,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",234,0.0,0.0,0,0,0.0,0.0,0.0,497.0,22.0,0.9576107899807321,800.0,0.0,3.52,1432.6399999999999,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",235,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.696,1434.3359999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",236,0.0,0.0,0,0,0.0,0.0,0.0,497.0,22.0,0.9576107899807321,800.0,0.0,3.584,1437.9199999999998,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",237,0.0,0.0,0,0,0.0,0.0,0.0,65664.0,13020.0,0.8345279853591582,831456.0,7968.0,6.016,1443.936,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25983.0,249.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",238,0.0,0.0,0,0,0.0,0.0,0.0,1832.0,32.0,0.9828326180257511,2560.0,544.0,6.4,1450.336,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,80.0,17.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",239,0.0,0.0,0,0,0.0,0.0,0.0,0.0,18849.0,0.0,814496.0,89664.0,3.776,1454.112,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25453.0,2802.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",240,0.0,0.0,0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,3.456,1457.568,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,31412.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",241,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6283.0,0.0,0.0,1608224.0,2.656,1460.224,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,50257.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",242,0.0,0.0,0,0,0.0,0.0,0.0,98304.0,6283.0,0.9399256121697726,804128.0,0.0,4.352,1464.576,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",243,0.0,0.0,0,0,0.0,0.0,0.0,156.0,64.0,0.7090909090909091,8192.0,0.0,2.176,1466.752,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",244,0.0,0.0,0,0,0.0,0.0,0.0,81483.0,30646.0,0.7266897947899295,3072320.0,2035104.0,10.368,1477.12,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,96010.0,63597.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",245,0.0,0.0,0,0,0.0,0.0,0.0,23142.0,35896.0,0.39198482333412377,3123008.0,2479936.0,8.576,1485.696,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97594.0,77498.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",246,0.0,0.0,0,0,0.0,0.0,0.0,23883.0,35504.0,0.4021587216057386,3114176.0,2479936.0,8.96,1494.656,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97318.0,77498.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",247,0.0,0.0,0,0,0.0,0.0,0.0,23883.0,34863.0,0.4065468287202533,3110848.0,1888864.0,8.832,1503.488,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97214.0,59027.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",248,0.0,0.0,0,0,0.0,0.0,0.0,14336.0,6283.0,0.6952810514573937,1608224.0,0.0,4.448,1507.9360000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",249,0.0,0.0,0,0,0.0,0.0,0.0,39.0,16.0,0.7090909090909091,2048.0,0.0,2.272,1510.208,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",250,0.0,0.0,0,0,0.0,0.0,0.0,20059.0,18284.0,0.5231463370106669,2110880.0,1392640.0,8.096,1518.304,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,65965.0,43520.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",251,0.0,0.0,0,0,0.0,0.0,0.0,0.0,25132.0,0.0,2427360.0,2412352.0,5.056,1523.3600000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,75855.0,75386.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",252,2814392.0,6655044.0,0,0,0.0,6655044.0,6655044.0,528.0,6704.0,0.07300884955752213,2276480.0,754304.0,23.616,1546.976,825232.0,201028.0,2814392.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,71140.0,23572.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",253,0.0,1024200.0,0,0,0.0,1024200.0,1024200.0,112284.0,12568.0,0.8993368147887099,804608.0,624864.0,72.448,1619.4240000000002,1024200.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25144.0,19527.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",254,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3158.0,0.0,804128.0,200800.0,3.392,1622.8160000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,6275.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",255,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,128.0,1.92,1624.7360000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,4.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",256,0.0,0.0,0,0,0.0,0.0,0.0,0.0,18849.0,0.0,1809280.0,89600.0,7.872,1632.6080000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,56540.0,2800.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",257,0.0,0.0,0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,3.648,1636.2560000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,31412.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",258,2814392.0,6655044.0,0,0,0.0,6655044.0,6655044.0,528.0,6704.0,0.07300884955752213,2280960.0,752640.0,24.064,1660.3200000000004,825232.0,201028.0,2814392.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,71280.0,23520.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",259,0.0,0.0,0,0,0.0,0.0,0.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.176,1666.4960000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",260,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.144,1668.6400000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",261,0.0,0.0,0,0,0.0,0.0,0.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.304,1674.9440000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",262,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.144,1677.0880000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",263,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.112,1679.2000000000005,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",264,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.976,1682.1760000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",265,0.0,220484.0,0,0,0.0,220484.0,220484.0,320.0,1582.0,0.16824395373291273,804224.0,128.0,10.88,1693.0560000000007,220484.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25132.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",266,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,1695.0720000000008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",267,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.648,1698.7200000000007,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",268,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,1700.7360000000008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",269,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.944,1703.6800000000007,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",270,1769472.0,3941000.0,0,0,0.0,3941000.0,3941000.0,0.0,6283.0,0.0,0.0,804128.0,3.968,1707.6480000000008,0.0,402056.0,1769472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,25129.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",271,1005140.0,2010280.0,0,0,0.0,2010280.0,2010280.0,0.0,4737.0,0.0,1608256.0,0.0,4.8,1712.4480000000008,0.0,0.0,1005140.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50258.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",272,0.0,0.0,0,0,0.0,0.0,0.0,640.0,1582.0,0.28802880288028804,804608.0,128.0,15.808,1728.2560000000008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25144.0,4.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",273,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.016,1730.2720000000008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",274,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.08,1732.3520000000008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",275,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.24,1734.5920000000008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",276,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.016,1736.6080000000009,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",277,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,64.0,128.0,2.592,1739.200000000001,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,4.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",278,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,1740.8960000000009,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",279,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.792,1742.6880000000008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",280,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.048,1744.7360000000008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",281,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,1746.4320000000007,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",282,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,32.0,2.304,1748.7360000000008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",283,0.0,0.0,0,0,0.0,0.0,0.0,0.0,5.0,0.0,32.0,32.0,4.704,1753.4400000000007,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",284,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.08,1755.5200000000007,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",285,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.08,1757.6000000000006,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",286,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.848,1760.4480000000005,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",287,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.168,1763.6160000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",288,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.08,1765.6960000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",289,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.24,1767.9360000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::index_elementwise_kernel<128, 4, void native::gpu_index_kernel<void native::index_kernel_impl<native::OpaqueType<8>>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>, const T1 &)::[lambda(int) (instance 1)]>(long, T3)",290,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,32.0,3.392,1771.3280000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",291,0.0,0.0,0,0,0.0,0.0,0.0,56.0,4.0,0.9333333333333333,64.0,32.0,2.528,1773.8560000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",292,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,64.0,2.048,1775.9040000000005,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,2.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",293,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,2.08,1777.9840000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",294,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,0.0,2.08,1780.0640000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,0.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",295,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,2.24,1782.3040000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",296,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,16640.0,16384.0,5.408,1787.7120000000002,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,520.0,512.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",297,0.0,0.0,0,0,0.0,0.0,0.0,0.0,384.0,0.0,4352.0,16384.0,3.392,1791.1040000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,136.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",298,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.016,1793.1200000000003,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",299,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,2.08,1795.2000000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",300,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.52,1798.7200000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",301,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.672,1803.3920000000003,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),302,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.6,1820.9920000000002,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",303,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.784,1823.7760000000003,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",304,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,3.968,1827.7440000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",305,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.0,1831.7440000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",306,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,1834.3680000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",307,131072.0,4358144.0,0,0,0.0,4358144.0,4358144.0,34048.0,64.0,0.99812382739212,81920.0,16384.0,12.64,1847.0080000000005,3297280.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2560.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,308,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.696,1856.7040000000004,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",309,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.304,1859.0080000000005,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",310,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,1861.0880000000004,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",311,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,1865.3440000000005,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,312,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17392736.0,458752.0,22.56,1887.9040000000005,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543523.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",313,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.168,1891.0720000000003,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",314,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.112,1893.1840000000004,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",315,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.08,1895.2640000000004,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",316,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.08,1897.3440000000003,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",317,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.208,1899.5520000000004,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",318,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,1901.6000000000004,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",319,94171.0,237494.0,0,0,0.0,237494.0,237494.0,0.0,256.0,0.0,65536.0,65536.0,2.176,1903.7760000000003,16384.0,32768.0,94171.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",320,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.08,1905.8560000000002,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",321,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.144,1908.0000000000002,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,322,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18937504.0,114688.0,22.72,1930.7200000000003,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591797.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",323,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.848,1933.5680000000002,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",324,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.048,1935.6160000000002,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",325,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,1939.8720000000003,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),326,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,18.24,1958.1120000000003,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",327,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.624,1960.7360000000003,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",328,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.0,1964.7360000000003,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",329,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,3.968,1968.7040000000004,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",330,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.592,1971.2960000000005,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",331,131072.0,4358144.0,0,0,0.0,4358144.0,4358144.0,34048.0,64.0,0.99812382739212,81920.0,16384.0,11.872,1983.1680000000006,3297280.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2560.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,332,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.6,1992.7680000000005,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",333,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.336,1995.1040000000005,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",334,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,1997.1840000000004,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",335,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,2001.4400000000005,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,336,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17393376.0,458752.0,22.24,2023.6800000000005,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543543.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",337,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.136,2026.8160000000005,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",338,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.272,2029.0880000000004,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",339,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.08,2031.1680000000003,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",340,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.08,2033.2480000000003,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",341,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.304,2035.5520000000004,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",342,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,2037.6000000000004,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",343,94219.0,237590.0,0,0,0.0,237590.0,237590.0,0.0,256.0,0.0,65536.0,65536.0,2.144,2039.7440000000004,16384.0,32768.0,94219.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",344,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.08,2041.8240000000003,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",345,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.08,2043.9040000000002,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,346,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18937408.0,114688.0,22.88,2066.784,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591794.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",347,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.816,2069.6,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",348,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,2071.68,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",349,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.288,2075.968,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),350,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.728,2093.696,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",351,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.656,2096.352,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",352,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.032,2100.384,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",353,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,3.968,2104.352,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",354,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.656,2107.008,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",355,131072.0,4358144.0,0,0,0.0,4358144.0,4358144.0,34048.0,64.0,0.99812382739212,81920.0,16384.0,11.872,2118.8799999999997,3297280.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2560.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,356,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,10.208,2129.0879999999997,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",357,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.336,2131.4239999999995,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",358,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,2133.5039999999995,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",359,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,2137.7599999999993,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,360,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17379936.0,458752.0,22.592,2160.3519999999994,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543123.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",361,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.104,2163.455999999999,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",362,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.08,2165.535999999999,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",363,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.048,2167.583999999999,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",364,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.112,2169.695999999999,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",365,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.208,2171.903999999999,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",366,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.08,2173.983999999999,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",367,94206.0,237564.0,0,0,0.0,237564.0,237564.0,0.0,256.0,0.0,65536.0,65536.0,2.176,2176.159999999999,16384.0,32768.0,94206.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",368,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.208,2178.367999999999,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",369,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.144,2180.511999999999,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,370,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18937600.0,114688.0,22.976,2203.487999999999,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591800.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",371,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.848,2206.335999999999,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",372,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.048,2208.3839999999987,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",373,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,2212.6399999999985,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),374,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.92,2230.5599999999986,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",375,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.624,2233.1839999999984,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",376,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.032,2237.2159999999985,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",377,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.032,2241.2479999999987,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",378,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.624,2243.8719999999985,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",379,131072.0,4358144.0,0,0,0.0,4358144.0,4358144.0,34048.0,64.0,0.99812382739212,81920.0,16384.0,11.84,2255.7119999999986,3297280.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2560.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,380,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.6,2265.3119999999985,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",381,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.336,2267.6479999999983,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",382,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,2269.7279999999982,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",383,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.32,2274.0479999999984,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,384,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17382336.0,458752.0,22.24,2296.287999999998,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543198.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",385,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.328,2299.615999999998,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",386,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.016,2301.6319999999982,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",387,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.048,2303.679999999998,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",388,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.208,2305.887999999998,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",389,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.176,2308.063999999998,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",390,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,2310.111999999998,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",391,94219.0,237590.0,0,0,0.0,237590.0,237590.0,0.0,256.0,0.0,65536.0,65536.0,2.176,2312.2879999999977,16384.0,32768.0,94219.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",392,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.08,2314.3679999999977,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",393,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.176,2316.5439999999976,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,394,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18940256.0,114688.0,22.656,2339.1999999999975,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591883.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",395,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.848,2342.0479999999975,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",396,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,2344.1279999999974,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",397,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,2348.3839999999973,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),398,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.792,2366.175999999997,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",399,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.656,2368.831999999997,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",400,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.064,2372.895999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",401,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.0,2376.895999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",402,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.592,2379.487999999997,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",403,131072.0,4358144.0,0,0,0.0,4358144.0,4358144.0,34048.0,64.0,0.99812382739212,81920.0,16384.0,11.936,2391.4239999999972,3297280.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2560.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,404,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.664,2401.0879999999975,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",405,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.336,2403.4239999999972,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",406,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,2405.503999999997,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",407,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,2409.759999999997,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,408,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17380800.0,458752.0,22.24,2431.999999999997,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543150.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",409,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.2,2435.1999999999966,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",410,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.016,2437.2159999999967,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",411,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.08,2439.2959999999966,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",412,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.112,2441.4079999999967,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",413,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.208,2443.615999999997,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",414,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,2445.6639999999966,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",415,94237.0,237626.0,0,0,0.0,237626.0,237626.0,0.0,256.0,0.0,65536.0,65536.0,2.176,2447.8399999999965,16384.0,32768.0,94237.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",416,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.08,2449.9199999999964,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",417,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.208,2452.1279999999965,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,418,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18936928.0,114688.0,22.72,2474.8479999999963,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591779.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",419,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.88,2477.7279999999964,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",420,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.048,2479.775999999996,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",421,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,2484.031999999996,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),422,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.728,2501.759999999996,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",423,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.624,2504.383999999996,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",424,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.0,2508.383999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",425,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.16,2512.543999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",426,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.688,2515.231999999996,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",427,131072.0,4358144.0,0,0,0.0,4358144.0,4358144.0,34048.0,64.0,0.99812382739212,81920.0,16384.0,11.808,2527.039999999996,3297280.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2560.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,428,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,10.24,2537.2799999999957,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",429,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.336,2539.6159999999954,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",430,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,2541.6959999999954,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",431,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,2545.951999999995,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,432,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17382272.0,458752.0,22.048,2567.999999999995,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543196.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",433,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.072,2571.071999999995,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",434,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.016,2573.087999999995,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",435,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.08,2575.167999999995,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",436,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.08,2577.247999999995,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",437,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.208,2579.455999999995,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",438,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.144,2581.599999999995,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",439,94175.0,237502.0,0,0,0.0,237502.0,237502.0,0.0,256.0,0.0,65536.0,65536.0,2.24,2583.8399999999947,16384.0,32768.0,94175.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",440,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.112,2585.9519999999948,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",441,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.176,2588.1279999999947,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,442,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18938208.0,114688.0,22.72,2610.8479999999945,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591819.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",443,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.816,2613.6639999999943,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",444,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,2615.7439999999942,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",445,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,2619.999999999994,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),446,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.856,2637.8559999999943,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",447,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.752,2640.6079999999943,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",448,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.032,2644.6399999999944,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",449,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.0,2648.6399999999944,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",450,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.688,2651.3279999999945,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",451,131072.0,4358144.0,0,0,0.0,4358144.0,4358144.0,34048.0,64.0,0.99812382739212,81920.0,16384.0,12.16,2663.4879999999944,3297280.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2560.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,452,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.952,2673.4399999999946,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",453,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.336,2675.7759999999944,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",454,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,2677.8559999999943,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",455,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,2682.111999999994,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,456,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17380480.0,458752.0,22.272,2704.383999999994,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543140.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",457,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.264,2707.647999999994,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",458,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.112,2709.7599999999943,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",459,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.048,2711.807999999994,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",460,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.048,2713.855999999994,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",461,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.208,2716.063999999994,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",462,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.112,2718.175999999994,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",463,94250.0,237652.0,0,0,0.0,237652.0,237652.0,0.0,256.0,0.0,65536.0,65536.0,2.176,2720.351999999994,16384.0,32768.0,94250.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",464,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.304,2722.655999999994,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",465,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.176,2724.831999999994,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,466,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18938048.0,114688.0,22.88,2747.711999999994,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591814.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",467,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.848,2750.559999999994,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",468,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.112,2752.671999999994,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",469,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,2756.927999999994,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),470,100663296.0,202113024.0,0,0,0.0,202113024.0,202113024.0,346368.0,3072.0,0.9912087912087912,12976128.0,393216.0,17.792,2774.719999999994,0.0,786432.0,100663296.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,405504.0,12288.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",471,0.0,122880.0,0,0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.816,2777.5359999999937,110592.0,12288.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",472,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.0,2781.5359999999937,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",473,0.0,0.0,0,0,0.0,0.0,0.0,0.0,512.0,0.0,32768.0,32768.0,4.0,2785.5359999999937,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",474,0.0,0.0,0,0,0.0,0.0,0.0,0.0,256.0,0.0,16384.0,16384.0,2.592,2788.127999999994,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,512.0,512.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",475,131072.0,4358144.0,0,0,0.0,4358144.0,4358144.0,34048.0,64.0,0.99812382739212,81920.0,16384.0,11.872,2799.9999999999936,3297280.0,798720.0,131072.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2560.0,512.0
ampere_sgemm_32x32_sliced1x4_nn,476,33554432.0,68026368.0,0,0,0.0,68026368.0,68026368.0,196608.0,37376.0,0.8402625820568927,4718592.0,65536.0,9.6,2809.5999999999935,393216.0,524288.0,33554432.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,147456.0,2048.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",477,0.0,24576.0,0,0,0.0,24576.0,24576.0,0.0,1408.0,0.0,69632.0,16384.0,2.496,2812.0959999999936,20480.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2176.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",478,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.08,2814.1759999999936,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",479,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,2818.4319999999934,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_128x32_nn,480,138412032.0,277741568.0,0,0,0.0,277741568.0,277741568.0,754432.0,138880.0,0.8445336008024072,17380768.0,458752.0,22.112,2840.5439999999935,0.0,917504.0,138412032.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,543149.0,14336.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",481,0.0,147456.0,0,0,0.0,147456.0,147456.0,0.0,7168.0,0.0,475136.0,65536.0,3.072,2843.6159999999936,131072.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,14848.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",482,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.016,2845.6319999999937,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",483,0.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.144,2847.7759999999935,0.0,32768.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",484,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.208,2849.9839999999936,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",485,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,2.176,2852.1599999999935,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",486,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,2.08,2854.2399999999934,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",487,94058.0,237268.0,0,0,0.0,237268.0,237268.0,0.0,256.0,0.0,65536.0,65536.0,2.112,2856.3519999999935,16384.0,32768.0,94058.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",488,16384.0,32768.0,0,0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,2.304,2858.6559999999936,0.0,0.0,16384.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",489,0.0,16384.0,0,0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,2.304,2860.9599999999937,0.0,16384.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0
ampere_sgemm_32x32_sliced1x4_nn,490,139460608.0,280526848.0,0,0,0.0,280526848.0,280526848.0,738304.0,148992.0,0.8320830929024813,18935904.0,114688.0,22.688,2883.6479999999938,688128.0,917504.0,139460608.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,591747.0,3584.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",491,0.0,36864.0,0,0,0.0,36864.0,36864.0,0.0,1792.0,0.0,118784.0,16384.0,2.848,2886.4959999999937,32768.0,4096.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3712.0,512.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",492,4096.0,8192.0,0,0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,2.144,2888.6399999999935,0.0,0.0,4096.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",493,30788.0,109812.0,0,0,0.0,109812.0,109812.0,80.0,360.0,0.18181818181818182,49152.0,16640.0,4.256,2892.8959999999934,31280.0,16956.0,30788.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0
ampere_sgemm_64x32_sliced1x4_tn,494,1648361472.0,3307991040.0,0,0,0.0,3307991040.0,3307991040.0,7621056.0,1715116.0,0.8162934444652477,214144768.0,773952.0,244.928,3137.8239999999932,4829184.0,6438912.0,1648361472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,6692024.0,24186.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",495,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,3139.519999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",496,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,96.0,160.0,2.656,3142.175999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,5.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",497,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,3144.223999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",498,0.0,201028.0,0,0,0.0,201028.0,201028.0,0.0,3158.0,0.0,804128.0,804128.0,2.944,3147.167999999993,0.0,201028.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,25129.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",499,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,3148.8639999999928,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",500,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,59136.0,3.712,3152.5759999999927,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1848.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",501,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,82608.0,0.15193823915900131,5134592.0,0.0,5.504,3158.0799999999927,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",502,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,57664.0,3.744,3161.823999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1802.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",503,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,83608.0,0.15039427688805787,5134592.0,0.0,5.92,3167.743999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",504,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,58880.0,3.904,3171.647999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1840.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",505,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,82808.0,0.1516269158265716,5134592.0,0.0,5.504,3177.1519999999928,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",506,0.0,0.0,0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813024.0,57664.0,3.872,3181.0239999999926,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25407.0,1802.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",507,0.0,0.0,0,0,0.0,0.0,0.0,14800.0,82958.0,0.15139425929335706,5134592.0,128.0,5.472,3186.495999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,160456.0,4.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",508,0.0,0.0,0,0,0.0,0.0,0.0,0.0,21.0,0.0,6432.0,800.0,2.56,3189.0559999999928,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,201.0,25.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",509,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.728,3190.783999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",510,0.0,0.0,0,0,0.0,0.0,0.0,497.0,22.0,0.9576107899807321,800.0,0.0,3.648,3194.431999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",511,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,1.696,3196.127999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",512,0.0,0.0,0,0,0.0,0.0,0.0,497.0,22.0,0.9576107899807321,800.0,0.0,3.616,3199.743999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",513,0.0,0.0,0,0,0.0,0.0,0.0,55632.0,13018.0,0.8103714493809177,831456.0,8640.0,6.112,3205.855999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25983.0,270.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",514,0.0,0.0,0,0,0.0,0.0,0.0,1832.0,32.0,0.9828326180257511,2560.0,544.0,6.496,3212.351999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,80.0,17.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",515,0.0,0.0,0,0,0.0,0.0,0.0,0.0,18849.0,0.0,814496.0,86816.0,3.68,3216.031999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25453.0,2713.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",516,0.0,0.0,0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,3.36,3219.391999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,31412.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",517,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6283.0,0.0,0.0,1608224.0,2.688,3222.079999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,50257.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",518,0.0,0.0,0,0,0.0,0.0,0.0,98304.0,6283.0,0.9399256121697726,804128.0,0.0,4.16,3226.239999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",519,0.0,0.0,0,0,0.0,0.0,0.0,156.0,64.0,0.7090909090909091,8192.0,0.0,2.176,3228.415999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",520,0.0,0.0,0,0,0.0,0.0,0.0,79842.0,29483.0,0.7303178595929568,3092288.0,2063424.0,10.464,3238.879999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,96634.0,64482.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",521,0.0,0.0,0,0,0.0,0.0,0.0,24042.0,37198.0,0.39258654474199867,3128128.0,2044128.0,8.8,3247.679999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97754.0,63879.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",522,0.0,0.0,0,0,0.0,0.0,0.0,23883.0,35506.0,0.40214517840004044,3119040.0,2479936.0,8.832,3256.511999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97470.0,77498.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",523,0.0,0.0,0,0,0.0,0.0,0.0,23883.0,35835.0,0.3999296694463981,3126080.0,1887072.0,8.992,3265.503999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,97690.0,58971.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",524,0.0,0.0,0,0,0.0,0.0,0.0,14336.0,6283.0,0.6952810514573937,1608224.0,0.0,4.352,3269.855999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",525,0.0,0.0,0,0,0.0,0.0,0.0,39.0,16.0,0.7090909090909091,2048.0,0.0,2.24,3272.0959999999927,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",526,0.0,0.0,0,0,0.0,0.0,0.0,20059.0,17754.0,0.5304789358157248,2108832.0,1392128.0,8.032,3280.127999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,65901.0,43504.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",527,0.0,0.0,0,0,0.0,0.0,0.0,0.0,25132.0,0.0,2427872.0,2412352.0,5.248,3285.375999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,75871.0,75386.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",528,2814392.0,6655044.0,0,0,0.0,6655044.0,6655044.0,528.0,6704.0,0.07300884955752213,2277632.0,753024.0,23.616,3308.991999999993,825232.0,201028.0,2814392.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,71176.0,23532.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",529,0.0,1024200.0,0,0,0.0,1024200.0,1024200.0,112284.0,12568.0,0.8993368147887099,804544.0,626208.0,72.48,3381.471999999993,1024200.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25142.0,19569.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",530,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3158.0,0.0,804128.0,200800.0,3.008,3384.4799999999927,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25129.0,6275.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",531,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,128.0,1.92,3386.399999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,4.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",532,0.0,0.0,0,0,0.0,0.0,0.0,0.0,18849.0,0.0,1809280.0,92352.0,7.776,3394.1759999999927,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,56540.0,2886.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",533,0.0,0.0,0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,3.36,3397.535999999993,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,31412.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",534,2814392.0,6655044.0,0,0,0.0,6655044.0,6655044.0,528.0,6704.0,0.07300884955752213,2279808.0,753024.0,24.096,3421.631999999993,825232.0,201028.0,2814392.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,71244.0,23532.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",535,0.0,0.0,0,0,0.0,0.0,0.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.24,3427.8719999999926,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",536,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.176,3430.0479999999925,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",537,0.0,0.0,0,0,0.0,0.0,0.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,6.08,3436.1279999999924,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",538,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.176,3438.3039999999924,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",539,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.368,3440.6719999999923,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",540,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,3.104,3443.775999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",541,0.0,220484.0,0,0,0.0,220484.0,220484.0,320.0,1582.0,0.16824395373291273,804224.0,128.0,10.848,3454.623999999992,220484.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25132.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",542,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,1.984,3456.607999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",543,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.264,3459.871999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",544,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.048,3461.919999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",545,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,2.976,3464.895999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",546,1769472.0,3941000.0,0,0,0.0,3941000.0,3941000.0,0.0,6283.0,0.0,0.0,804128.0,4.0,3468.895999999992,0.0,402056.0,1769472.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,25129.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",547,1005140.0,2010280.0,0,0,0.0,2010280.0,2010280.0,0.0,4737.0,0.0,1608256.0,0.0,5.056,3473.951999999992,0.0,0.0,1005140.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,50258.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",548,0.0,0.0,0,0,0.0,0.0,0.0,640.0,1582.0,0.28802880288028804,804448.0,128.0,15.872,3489.823999999992,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,25139.0,4.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",549,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.048,3491.8719999999917,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",550,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.016,3493.8879999999917,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",551,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.24,3496.1279999999915,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",552,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.048,3498.1759999999913,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",553,0.0,0.0,0,0,0.0,0.0,0.0,0.0,6.0,0.0,96.0,160.0,2.624,3500.799999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,3.0,5.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",554,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.76,3502.5599999999913,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",555,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,3504.255999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",556,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.048,3506.303999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",557,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,1.696,3507.999999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",558,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,128.0,32.0,2.24,3510.2399999999907,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,4.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",559,0.0,0.0,0,0,0.0,0.0,0.0,0.0,5.0,0.0,32.0,32.0,4.8,3515.039999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",560,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.08,3517.119999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",561,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.08,3519.1999999999907,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",562,0.0,0.0,0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,2.784,3521.983999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",563,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.232,3525.215999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",564,0.0,0.0,0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,2.112,3527.327999999991,0.0,0.0,0.0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,1.0,1.0
