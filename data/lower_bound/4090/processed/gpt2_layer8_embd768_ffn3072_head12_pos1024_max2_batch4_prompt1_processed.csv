Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum,flops_log,flops_threshold,is_matmul_candidate,is_attention_candidate,elementwise_add_fma_ops,role
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",385.0,25668.0,93428.0,0.0,0.0,0.0,93428.0,93428.0,80.0,272.0,0.2272727272727272,36864.0,12544.0,4.236000000000001,1957.3919999999991,26160.0,15932.0,25668.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0,11.444957068529465,13.075642730030262,False,False,0,other
ampere_sgemm_32x32_sliced1x4_nn,386.0,56623104.0,114794496.0,0.0,0.0,0.0,114794496.0,114794496.0,331776.0,63072.0,0.8402625820568927,7962624.0,110592.0,12.216000000000001,1969.6079999999993,663552.0,884736.0,56623104.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,248832.0,3456.0,18.558654105160727,13.075642730030262,True,True,0,QKV
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",387.0,0.0,46080.0,0.0,0.0,0.0,46080.0,46080.0,0.0,2880.0,0.0,119808.0,36864.0,2.544,1972.151999999999,36864.0,9216.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3744.0,1152.0,10.73815599652319,13.075642730030262,False,False,0,splitKreduce
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",388.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,4.064,1976.215999999999,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,768.0,768.0,0.0,13.075642730030262,False,False,0,other
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",389.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,384.0,0.0,24576.0,24576.0,3.976,1980.191999999999,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,768.0,768.0,0.0,13.075642730030262,False,False,0,other
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",390.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,192.0,0.0,12288.0,12288.0,2.652,1982.843999999999,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,384.0,384.0,0.0,13.075642730030262,False,False,0,other
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",391.0,98304.0,3268608.0,0.0,0.0,0.0,3268608.0,3268608.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,11.604,1994.4479999999992,2472960.0,599040.0,98304.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1920.0,384.0,14.999875070109738,13.075642730030262,True,True,0,Attention
ampere_sgemm_32x32_sliced1x4_nn,392.0,23592960.0,48906240.0,0.0,0.0,0.0,48906240.0,48906240.0,176640.0,22272.0,0.888030888030888,2682232.0,122880.0,7.312,2001.7599999999989,737280.0,983040.0,23592960.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,83819.75,3840.0,17.705415574113225,13.075642730030262,True,True,0,Wo
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",393.0,0.0,36864.0,0.0,0.0,0.0,36864.0,36864.0,0.0,1632.0,0.0,125952.0,12288.0,2.968,2004.7279999999987,33792.0,3072.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3936.0,384.0,10.515017870423751,13.075642730030262,False,False,0,splitKreduce
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",394.0,3072.0,6144.0,0.0,0.0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.096,2006.823999999999,0.0,0.0,3072.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,768.0,384.0,8.723394022000136,13.075642730030262,False,False,3072,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",395.0,25668.0,93428.0,0.0,0.0,0.0,93428.0,93428.0,80.0,272.0,0.2272727272727272,36864.0,12544.0,4.208,2011.0319999999988,26160.0,15932.0,25668.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0,11.444957068529465,13.075642730030262,False,False,0,other
void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(T1::Params),396.0,75497472.0,151781376.0,0.0,0.0,0.0,151781376.0,151781376.0,272640.0,3072.0,0.9888579387186628,9732096.0,393216.0,14.684000000000001,2025.715999999999,0.0,786432.0,75497472.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,304128.0,12288.0,18.837951734240434,13.075642730030262,True,True,0,FFN1
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",397.0,0.0,122880.0,0.0,0.0,0.0,122880.0,122880.0,0.0,5760.0,0.0,405504.0,49152.0,2.6799999999999997,2028.3959999999988,110592.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12672.0,1536.0,11.718971686369219,13.075642730030262,False,False,0,splitKreduce
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",398.0,0.0,12288.0,0.0,0.0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.064,2030.459999999999,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,9.416459832284596,13.075642730030262,False,False,0,other
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",399.0,0.0,24576.0,0.0,0.0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.072,2032.5319999999988,0.0,24576.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,10.109566325223746,13.075642730030262,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",400.0,0.0,12288.0,0.0,0.0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.108,2034.6399999999987,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,9.416459832284596,13.075642730030262,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",401.0,12288.0,24576.0,0.0,0.0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,2.188,2036.8279999999986,0.0,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0,10.109566325223746,13.075642730030262,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",402.0,0.0,12288.0,0.0,0.0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,2.104,2038.9319999999989,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,9.416459832284596,13.075642730030262,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",403.0,71418.5,179701.0,0.0,0.0,0.0,179701.0,179701.0,0.0,192.0,0.0,49152.0,49152.0,2.16,2041.0919999999987,12288.0,24576.0,71418.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,12.09905520237026,13.075642730030262,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",404.0,12288.0,24576.0,0.0,0.0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,2.092,2043.1839999999986,0.0,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,10.109566325223746,13.075642730030262,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",405.0,0.0,12288.0,0.0,0.0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,2.152,2045.3359999999986,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0,9.416459832284596,13.075642730030262,False,False,0,other
ampere_sgemm_32x32_sliced1x4_nn,406.0,77070336.0,155344896.0,0.0,0.0,0.0,155344896.0,155344896.0,419328.0,83808.0,0.8334287349742415,10658872.0,86016.0,15.672,2061.0079999999984,516096.0,688128.0,77070336.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,333089.75,2688.0,18.861158344856985,13.075642730030262,True,True,0,FFN2
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",407.0,0.0,27648.0,0.0,0.0,0.0,27648.0,27648.0,0.0,1344.0,0.0,89088.0,12288.0,2.904,2063.9119999999984,24576.0,3072.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2784.0,384.0,10.227344839931181,13.075642730030262,False,False,0,splitKreduce
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",408.0,3072.0,6144.0,0.0,0.0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,2.116,2066.0279999999984,0.0,0.0,3072.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,768.0,384.0,8.723394022000136,13.075642730030262,False,False,3072,elementwise_add
