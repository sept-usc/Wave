Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum,flops_log,flops_threshold,is_matmul_candidate,is_attention_candidate,elementwise_add_fma_ops,role
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",386.0,315863040.0,636764160.0,2580480.0,0.0,0.0,639344640.0,639344640.0,2774400.0,2615040.0,0.5147844674029213,325650640.0,245760.0,132.72,9426.388,2703360.0,4915200.0,314572800.0,1290240.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10176582.5,7680.0,20.275954211235906,15.548136326626667,True,False,0,QKV
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",387.0,81920.0,0.0,163840.0,0.0,0.0,163840.0,163840.0,0.0,2560.0,0.0,163840.0,163840.0,6.196,9432.584,0.0,0.0,0.0,81920.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5120.0,5120.0,12.006651724330279,15.548136326626667,False,False,0,add
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",388.0,81920.0,0.0,163840.0,0.0,0.0,163840.0,163840.0,0.0,2560.0,0.0,163840.0,163840.0,6.272,9438.856000000002,0.0,0.0,0.0,81920.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5120.0,5120.0,12.006651724330279,15.548136326626667,False,False,0,add
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",389.0,20480.0,0.0,40960.0,0.0,0.0,40960.0,40960.0,0.0,1280.0,0.0,81920.0,81920.0,4.5760000000000005,9443.432,0.0,0.0,0.0,20480.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2560.0,2560.0,10.620375673477872,15.548136326626667,False,False,0,add
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",390.0,655360.0,21790720.0,0.0,0.0,193273528320.0,21790720.0,193295319040.0,170240.0,320.0,0.99812382739212,409600.0,81920.0,20.232,9463.664,16486400.0,3993600.0,655360.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,754974720.0,12800.0,2560.0,25.98748500683541,15.548136326626667,True,True,0,Attention
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",391.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5120.0,0.0,0.0,105600.0,3.06,9466.724,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3300.0,0.0,15.548136326626667,False,False,0,other
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",392.0,210535200.0,420659200.0,820800.0,0.0,0.0,421480000.0,421480000.0,4202400.0,1356635.875,0.755972914972254,111817944.0,1663272.0,87.232,9553.956,0.0,409600.0,210124800.0,410400.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3494310.75,51977.25,19.859282887068193,15.548136326626667,True,True,0,Wo
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",393.0,0.0,20480.0,0.0,0.0,0.0,20480.0,20480.0,0.0,6400.0,0.0,102400.0,0.0,4.288,9558.244000000002,20480.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3200.0,0.0,9.92725290608639,15.548136326626667,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",394.0,20480.0,40960.0,0.0,0.0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,3.448,9561.692000000001,0.0,0.0,20480.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5120.0,2560.0,10.620375673477872,15.548136326626667,False,False,20480,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",395.0,117316.0,371956.0,9216.0,0.0,0.0,381172.0,381172.0,80.0,1768.0,0.0432900432900432,245760.0,82176.0,15.100000000000001,9576.792000000001,113200.0,33340.0,112708.0,4608.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7680.0,2568.0,12.85100861929252,15.548136326626667,False,False,0,other
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",396.0,421150720.0,849018880.0,3440640.0,0.0,0.0,852459520.0,852459520.0,3699200.0,3486720.0,0.5147844674029213,430818816.0,327680.0,155.192,9731.984,3604480.0,6553600.0,419430400.0,1720320.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,13463088.0,10240.0,20.56363628329666,15.548136326626667,True,False,0,FFN1
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",397.0,0.0,81920.0,0.0,0.0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,3.5,9735.484,0.0,81920.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10240.0,10240.0,11.31351064723008,15.548136326626667,False,False,0,other
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",398.0,0.0,163840.0,0.0,0.0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,3.596,9739.080000000002,0.0,163840.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10240.0,10240.0,12.006651724330279,15.548136326626667,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",399.0,0.0,81920.0,0.0,0.0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,3.544,9742.624000000003,0.0,81920.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10240.0,10240.0,11.31351064723008,15.548136326626667,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",400.0,81920.0,163840.0,0.0,0.0,0.0,163840.0,163840.0,0.0,1920.0,0.0,655360.0,327680.0,3.612,9746.236,0.0,0.0,81920.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,20480.0,10240.0,12.006651724330279,15.548136326626667,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",401.0,0.0,81920.0,0.0,0.0,0.0,81920.0,81920.0,0.0,1280.0,0.0,327680.0,327680.0,3.54,9749.776,0.0,81920.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10240.0,10240.0,11.31351064723008,15.548136326626667,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",402.0,462269.125,1149818.25,40960.0,0.0,0.0,1190778.25,1190778.25,0.0,1280.0,0.0,327680.0,327680.0,3.596,9753.372000000001,81920.0,184320.0,441789.125,20480.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10240.0,10240.0,13.990118482711608,15.548136326626667,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",403.0,81920.0,163840.0,0.0,0.0,0.0,163840.0,163840.0,0.0,1280.0,0.0,327680.0,327680.0,3.484,9756.856000000002,0.0,0.0,81920.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10240.0,10240.0,12.006651724330279,15.548136326626667,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",404.0,0.0,81920.0,0.0,0.0,0.0,81920.0,81920.0,0.0,1920.0,0.0,655360.0,327680.0,3.596,9760.452000000001,0.0,81920.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,20480.0,10240.0,11.31351064723008,15.548136326626667,False,False,0,other
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",405.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5120.0,0.0,0.0,105520.0,3.152,9763.604000000001,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3297.5,0.0,15.548136326626667,False,False,0,other
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",406.0,840172800.0,1679687680.0,1313280.0,0.0,0.0,1681000960.0,1681000960.0,16800000.0,5842652.875,0.74197506253066,442666504.0,2661516.0,215.276,9978.880000000001,0.0,655360.0,839516160.0,656640.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,13833328.25,83172.375,21.24265526305625,15.548136326626667,True,True,0,FFN2
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",407.0,0.0,20480.0,0.0,0.0,0.0,20480.0,20480.0,0.0,6400.0,0.0,102400.0,0.0,4.5,9983.380000000001,20480.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3200.0,0.0,9.92725290608639,15.548136326626667,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",408.0,20480.0,40960.0,0.0,0.0,0.0,40960.0,40960.0,0.0,480.0,0.0,163840.0,81920.0,3.476,9986.856000000002,0.0,0.0,20480.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5120.0,2560.0,10.620375673477872,15.548136326626667,False,False,20480,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",409.0,117316.0,371956.0,9216.0,0.0,0.0,381172.0,381172.0,80.0,1768.0,0.0432900432900432,245760.0,82176.0,15.283999999999999,10002.140000000003,113200.0,33340.0,112708.0,4608.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7680.0,2568.0,12.85100861929252,15.548136326626667,False,False,0,other
