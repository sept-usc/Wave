Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum,flops_log,flops_threshold,is_matmul_candidate,is_attention_candidate,elementwise_add_fma_ops,role
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",363.0,7123968.0,14376960.0,92160.0,0.0,0.0,14469120.0,14469120.0,64512.0,59328.0,0.5209302325581395,7529472.0,36864.0,19.408,2521.868,110592.0,110592.0,7077888.0,46080.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,235296.0,1152.0,16.487527350390728,14.164951636467679,True,False,0,QKV
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",364.0,12288.0,0.0,24576.0,0.0,0.0,24576.0,24576.0,0.0,384.0,0.0,24576.0,24576.0,5.444,2527.312,0.0,0.0,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,768.0,768.0,10.109566325223746,14.164951636467679,False,False,0,add
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",365.0,12288.0,0.0,24576.0,0.0,0.0,24576.0,24576.0,0.0,384.0,0.0,24576.0,24576.0,5.444,2532.7560000000003,0.0,0.0,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,768.0,768.0,10.109566325223746,14.164951636467679,False,False,0,add
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",366.0,3072.0,0.0,6144.0,0.0,0.0,6144.0,6144.0,0.0,192.0,0.0,12288.0,12288.0,4.408,2537.1639999999998,0.0,0.0,0.0,3072.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,384.0,384.0,8.723394022000136,14.164951636467679,False,False,0,add
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",367.0,98304.0,3268608.0,0.0,0.0,28991029248.0,3268608.0,28994297856.0,25536.0,48.0,0.99812382739212,61440.0,12288.0,12.496,2549.66,2472960.0,599040.0,98304.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,113246208.0,1920.0,384.0,24.090365021978844,14.164951636467679,True,True,0,Attention
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),368.0,37748736.0,75890688.0,0.0,0.0,0.0,75890688.0,75890688.0,136320.0,768.0,0.9943977591036416,2506752.0,98304.0,9.079999999999998,2558.7400000000002,0.0,393216.0,37748736.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,78336.0,3072.0,18.144804560268913,14.164951636467679,True,True,0,Wo
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",369.0,15360.0,30720.0,30720.0,0.0,0.0,61440.0,61440.0,0.0,1440.0,0.0,101376.0,12288.0,4.152,2562.892,27648.0,3072.0,0.0,15360.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3168.0,384.0,11.025832643730768,14.164951636467679,False,False,0,splitKreduce
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",370.0,3072.0,6144.0,0.0,0.0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,3.46,2566.352,0.0,0.0,3072.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,768.0,384.0,8.723394022000136,14.164951636467679,False,False,3072,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",371.0,30276.0,93428.0,9216.0,0.0,0.0,102644.0,102644.0,80.0,272.0,0.2272727272727272,36864.0,12544.0,6.588,2572.9399999999996,26160.0,15932.0,25668.0,4608.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0,11.539031712054708,14.164951636467679,False,False,0,other
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",372.0,9498624.0,19169280.0,122880.0,0.0,0.0,19292160.0,19292160.0,86016.0,79104.0,0.5209302325581395,10039296.0,49152.0,19.572000000000003,2592.5119999999997,147456.0,147456.0,9437184.0,61440.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,313728.0,1536.0,16.775209405564333,14.164951636467679,True,False,0,FFN1
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",373.0,0.0,12288.0,0.0,0.0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,3.38,2595.892,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,9.416459832284596,14.164951636467679,False,False,0,other
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",374.0,0.0,24576.0,0.0,0.0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,3.372,2599.2639999999997,0.0,24576.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,10.109566325223746,14.164951636467679,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",375.0,0.0,12288.0,0.0,0.0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,3.3720000000000003,2602.6359999999995,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,9.416459832284596,14.164951636467679,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",376.0,12288.0,24576.0,0.0,0.0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.476,2606.112,0.0,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0,10.109566325223746,14.164951636467679,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",377.0,0.0,12288.0,0.0,0.0,0.0,12288.0,12288.0,0.0,192.0,0.0,49152.0,49152.0,3.3680000000000003,2609.4799999999996,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,9.416459832284596,14.164951636467679,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",378.0,74511.5,182815.0,6144.0,0.0,0.0,188959.0,188959.0,0.0,192.0,0.0,49152.0,49152.0,3.5,2612.9799999999996,12288.0,27648.0,71439.5,3072.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,12.149290631431189,14.164951636467679,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",379.0,12288.0,24576.0,0.0,0.0,0.0,24576.0,24576.0,0.0,192.0,0.0,49152.0,49152.0,3.372,2616.3519999999994,0.0,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,1536.0,10.109566325223746,14.164951636467679,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",380.0,0.0,12288.0,0.0,0.0,0.0,12288.0,12288.0,0.0,288.0,0.0,98304.0,49152.0,3.472,2619.8239999999996,0.0,12288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3072.0,1536.0,9.416459832284596,14.164951636467679,False,False,0,other
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),381.0,150994944.0,302776320.0,0.0,0.0,0.0,302776320.0,302776320.0,493824.0,1536.0,0.9968992248062016,10027008.0,196608.0,19.016,2638.8399999999992,0.0,786432.0,150994944.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,313344.0,6144.0,19.528504876337678,14.164951636467679,True,True,0,FFN2
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",382.0,15360.0,55296.0,30720.0,0.0,0.0,86016.0,86016.0,0.0,2208.0,0.0,199680.0,12288.0,4.616,2643.455999999999,52224.0,3072.0,0.0,15360.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6240.0,384.0,11.362300230119235,14.164951636467679,False,False,0,splitKreduce
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",383.0,3072.0,6144.0,0.0,0.0,0.0,6144.0,6144.0,0.0,72.0,0.0,24576.0,12288.0,3.484,2646.9399999999996,0.0,0.0,3072.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,768.0,384.0,8.723394022000136,14.164951636467679,False,False,3072,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",384.0,30276.0,93428.0,9216.0,0.0,0.0,102644.0,102644.0,80.0,272.0,0.2272727272727272,36864.0,12544.0,6.5600000000000005,2653.4999999999995,26160.0,15932.0,25668.0,4608.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1152.0,392.0,11.539031712054708,14.164951636467679,False,False,0,other
