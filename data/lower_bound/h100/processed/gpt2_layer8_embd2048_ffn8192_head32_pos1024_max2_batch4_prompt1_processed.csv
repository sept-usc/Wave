Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum,flops_log,flops_threshold,is_matmul_candidate,is_attention_candidate,elementwise_add_fma_ops,role
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",363.0,50552832.0,101941248.0,442368.0,0.0,0.0,102383616.0,102383616.0,446208.0,419328.0,0.515527950310559,53501952.0,98304.0,46.464,4594.408000000003,491520.0,786432.0,50331648.0,221184.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1671936.0,3072.0,18.444237267535502,14.779390290285328,True,False,0,QKV
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",364.0,32768.0,0.0,65536.0,0.0,0.0,65536.0,65536.0,0.0,1024.0,0.0,65536.0,65536.0,6.036,4600.444000000003,0.0,0.0,0.0,32768.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,11.090370147631774,14.779390290285328,False,False,0,add
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",365.0,32768.0,0.0,65536.0,0.0,0.0,65536.0,65536.0,0.0,1024.0,0.0,65536.0,65536.0,6.008,4606.452000000004,0.0,0.0,0.0,32768.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,11.090370147631774,14.779390290285328,False,False,0,add
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",366.0,8192.0,0.0,16384.0,0.0,0.0,16384.0,16384.0,0.0,512.0,0.0,32768.0,32768.0,4.4,4610.8520000000035,0.0,0.0,0.0,8192.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0,9.704121561132915,14.779390290285328,False,False,0,add
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",367.0,262144.0,8716288.0,0.0,0.0,77309411328.0,8716288.0,77318127616.0,68096.0,128.0,0.99812382739212,163840.0,32768.0,13.124,4623.976000000003,6594560.0,1597440.0,262144.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,301989888.0,5120.0,1024.0,25.071194274969013,14.779390290285328,True,True,0,Attention
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",368.0,16850944.0,33980416.0,147456.0,0.0,0.0,34127872.0,34127872.0,148736.0,139776.0,0.515527950310559,17833984.0,32768.0,43.124,4667.100000000004,163840.0,262144.0,16777216.0,73728.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,557312.0,1024.0,17.345624998401767,14.779390290285328,True,False,0,Wo
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",369.0,8192.0,16384.0,0.0,0.0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,3.3480000000000003,4670.448000000004,0.0,0.0,8192.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0,9.704121561132915,14.779390290285328,False,False,8192,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",370.0,55364.0,175348.0,8192.0,0.0,0.0,183540.0,183540.0,80.0,712.0,0.101010101010101,98304.0,33024.0,8.816,4679.264000000004,51760.0,21052.0,51268.0,4096.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0,12.12019335476178,14.779390290285328,False,False,0,other
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",371.0,67403776.0,135921664.0,589824.0,0.0,0.0,136511488.0,136511488.0,594944.0,559104.0,0.515527950310559,71335936.0,131072.0,46.647999999999996,4725.912000000004,655360.0,1048576.0,67108864.0,294912.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2229248.0,4096.0,18.731919337545488,14.779390290285328,True,False,0,FFN1
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",372.0,0.0,32768.0,0.0,0.0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,3.468,4729.380000000004,0.0,32768.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0,10.397238225511654,14.779390290285328,False,False,0,other
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",373.0,0.0,65536.0,0.0,0.0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,3.396,4732.7760000000035,0.0,65536.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0,11.090370147631774,14.779390290285328,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",374.0,0.0,32768.0,0.0,0.0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,3.44,4736.216000000004,0.0,32768.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0,10.397238225511654,14.779390290285328,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",375.0,32768.0,65536.0,0.0,0.0,0.0,65536.0,65536.0,0.0,768.0,0.0,262144.0,131072.0,3.46,4739.676000000003,0.0,0.0,32768.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0,11.090370147631774,14.779390290285328,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",376.0,0.0,32768.0,0.0,0.0,0.0,32768.0,32768.0,0.0,512.0,0.0,131072.0,131072.0,3.404,4743.080000000004,0.0,32768.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0,10.397238225511654,14.779390290285328,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",377.0,191141.5,472395.0,16384.0,0.0,0.0,488779.0,488779.0,0.0,512.0,0.0,131072.0,131072.0,3.5439999999999996,4746.624000000003,32768.0,73728.0,182949.5,8192.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0,13.09966776947214,14.779390290285328,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",378.0,32768.0,65536.0,0.0,0.0,0.0,65536.0,65536.0,0.0,512.0,0.0,131072.0,131072.0,3.3280000000000003,4749.952000000004,0.0,0.0,32768.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0,11.090370147631774,14.779390290285328,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",379.0,0.0,32768.0,0.0,0.0,0.0,32768.0,32768.0,0.0,768.0,0.0,262144.0,131072.0,3.516,4753.468000000004,0.0,32768.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8192.0,4096.0,10.397238225511654,14.779390290285328,False,False,0,other
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",380.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,0.0,0.0,43072.0,2.964,4756.432000000003,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1346.0,0.0,14.779390290285328,False,False,0,other
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",381.0,134742528.0,269221888.0,525312.0,0.0,0.0,269747200.0,269747200.0,2689536.0,870719.25,0.7554475153957746,72539604.0,1064216.0,95.72800000000001,4852.1600000000035,0.0,262144.0,134479872.0,262656.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2266862.625,33256.75,19.412995785774356,14.779390290285328,True,True,0,FFN2
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",382.0,0.0,8192.0,0.0,0.0,0.0,8192.0,8192.0,0.0,2560.0,0.0,40960.0,0.0,4.208,4856.368000000004,8192.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1280.0,0.0,9.011035410141815,14.779390290285328,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",383.0,8192.0,16384.0,0.0,0.0,0.0,16384.0,16384.0,0.0,192.0,0.0,65536.0,32768.0,3.3920000000000003,4859.760000000004,0.0,0.0,8192.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,1024.0,9.704121561132915,14.779390290285328,False,False,8192,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",384.0,55364.0,175348.0,8192.0,0.0,0.0,183540.0,183540.0,80.0,712.0,0.101010101010101,98304.0,33024.0,8.836,4868.596000000003,51760.0,21052.0,51268.0,4096.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3072.0,1032.0,12.12019335476178,14.779390290285328,False,False,0,other
