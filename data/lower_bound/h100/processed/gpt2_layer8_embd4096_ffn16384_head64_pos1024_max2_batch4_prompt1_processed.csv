Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum,flops_log,flops_threshold,is_matmul_candidate,is_attention_candidate,elementwise_add_fma_ops,role
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",386.0,202162176.0,407568384.0,1671168.0,0.0,0.0,409239552.0,409239552.0,1777152.0,1674240.0,0.5149087672452158,210089824.0,196608.0,100.09200000000001,7715.668000000003,1769472.0,3145728.0,201326592.0,835584.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6565307.0,6144.0,19.82981124670582,15.263567893303577,True,False,0,QKV
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",387.0,65536.0,0.0,131072.0,0.0,0.0,131072.0,131072.0,0.0,2048.0,0.0,131072.0,131072.0,5.763999999999999,7721.432000000003,0.0,0.0,0.0,65536.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0,11.783509698884497,15.263567893303577,False,False,0,add
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",388.0,65536.0,0.0,131072.0,0.0,0.0,131072.0,131072.0,0.0,2048.0,0.0,131072.0,131072.0,5.788,7727.220000000004,0.0,0.0,0.0,65536.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,4096.0,11.783509698884497,15.263567893303577,False,False,0,add
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",389.0,16384.0,0.0,32768.0,0.0,0.0,32768.0,32768.0,0.0,1024.0,0.0,65536.0,65536.0,4.536,7731.756000000005,0.0,0.0,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,10.397238225511654,15.263567893303577,False,False,0,add
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",390.0,524288.0,17432576.0,0.0,0.0,154618822656.0,17432576.0,154636255232.0,136192.0,256.0,0.99812382739212,327680.0,65536.0,16.244,7748.000000000004,13189120.0,3194880.0,524288.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,603979776.0,10240.0,2048.0,25.764341455522494,15.263567893303577,True,True,0,Attention
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",391.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,0.0,0.0,86832.0,3.072,7751.072000000004,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2713.5,0.0,15.263567893303577,False,False,0,other
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",392.0,134873728.0,269418496.0,656640.0,0.0,0.0,270075136.0,270075136.0,2690176.0,848312.875,0.760264960633648,71986316.0,1329920.0,74.684,7825.756000000005,0.0,327680.0,134545408.0,328320.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2249572.375,41560.0,19.414210763433694,15.263567893303577,True,True,0,Wo
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",393.0,0.0,16384.0,0.0,0.0,0.0,16384.0,16384.0,0.0,5120.0,0.0,81920.0,0.0,4.264,7830.020000000004,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2560.0,0.0,9.704121561132915,15.263567893303577,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",394.0,16384.0,32768.0,0.0,0.0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,3.424,7833.444000000004,0.0,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0,10.397238225511654,15.263567893303577,False,False,16384,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",395.0,96324.0,306420.0,8192.0,0.0,0.0,314612.0,314612.0,80.0,1416.0,0.053475935828877,196608.0,65792.0,13.080000000000002,7846.524000000004,92720.0,29244.0,92228.0,4096.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,2056.0,12.659098591066622,15.263567893303577,False,False,0,other
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",396.0,269549568.0,543424512.0,2228224.0,0.0,0.0,545652736.0,545652736.0,2369536.0,2232320.0,0.5149087672452158,277223456.0,262144.0,109.256,7955.780000000004,2359296.0,4194304.0,268435456.0,1114112.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8663233.0,8192.0,20.117493318546714,15.263567893303577,True,False,0,FFN1
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",397.0,0.0,65536.0,0.0,0.0,0.0,65536.0,65536.0,0.0,1024.0,0.0,262144.0,262144.0,3.54,7959.320000000005,0.0,65536.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8192.0,8192.0,11.090370147631774,15.263567893303577,False,False,0,other
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",398.0,0.0,131072.0,0.0,0.0,0.0,131072.0,131072.0,0.0,1024.0,0.0,262144.0,262144.0,3.532,7962.852000000004,0.0,131072.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8192.0,8192.0,11.783509698884497,15.263567893303577,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",399.0,0.0,65536.0,0.0,0.0,0.0,65536.0,65536.0,0.0,1024.0,0.0,262144.0,262144.0,3.428,7966.280000000006,0.0,65536.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8192.0,8192.0,11.090370147631774,15.263567893303577,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",400.0,65536.0,131072.0,0.0,0.0,0.0,131072.0,131072.0,0.0,1536.0,0.0,524288.0,262144.0,3.572,7969.852000000006,0.0,0.0,65536.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,16384.0,8192.0,11.783509698884497,15.263567893303577,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",401.0,0.0,65536.0,0.0,0.0,0.0,65536.0,65536.0,0.0,1024.0,0.0,262144.0,262144.0,3.416,7973.2680000000055,0.0,65536.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8192.0,8192.0,11.090370147631774,15.263567893303577,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",402.0,372488.25,925200.5,32768.0,0.0,0.0,957968.5,957968.5,0.0,1024.0,0.0,262144.0,262144.0,3.504,7976.772000000005,65536.0,147456.0,356104.25,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8192.0,8192.0,13.772571219285433,15.263567893303577,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",403.0,65536.0,131072.0,0.0,0.0,0.0,131072.0,131072.0,0.0,1024.0,0.0,262144.0,262144.0,3.444,7980.216000000006,0.0,0.0,65536.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8192.0,8192.0,11.783509698884497,15.263567893303577,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",404.0,0.0,65536.0,0.0,0.0,0.0,65536.0,65536.0,0.0,1536.0,0.0,524288.0,262144.0,3.54,7983.756000000005,0.0,65536.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,16384.0,8192.0,11.090370147631774,15.263567893303577,False,False,0,other
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",405.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,0.0,0.0,87200.0,3.064,7986.820000000005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2725.0,0.0,15.263567893303577,False,False,0,other
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",406.0,537920512.0,1075314688.0,1050624.0,0.0,0.0,1076365312.0,1076365312.0,10753024.0,3540158.75,0.7523319054083348,284268856.0,2128588.0,175.904,8162.7240000000065,0.0,524288.0,537395200.0,525312.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8883401.75,66518.375,20.796855751288497,15.263567893303577,True,True,0,FFN2
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",407.0,0.0,16384.0,0.0,0.0,0.0,16384.0,16384.0,0.0,5120.0,0.0,81920.0,0.0,4.308,8167.0320000000065,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2560.0,0.0,9.704121561132915,15.263567893303577,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",408.0,16384.0,32768.0,0.0,0.0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,3.456,8170.488000000007,0.0,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0,10.397238225511654,15.263567893303577,False,False,16384,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",409.0,96324.0,306420.0,8192.0,0.0,0.0,314612.0,314612.0,80.0,1416.0,0.053475935828877,196608.0,65792.0,13.068,8183.556000000006,92720.0,29244.0,92228.0,4096.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,2056.0,12.659098591066622,15.263567893303577,False,False,0,other
