Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum,flops_log,flops_threshold,is_matmul_candidate,is_attention_candidate,elementwise_add_fma_ops,role
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",363.0,12644352.0,25509888.0,122880.0,0.0,0.0,25632768.0,25632768.0,112512.0,105216.0,0.5167548500881834,13381632.0,49152.0,24.112,2772.84,147456.0,196608.0,12582912.0,61440.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,418176.0,1536.0,17.059382129960735,14.524173799334966,True,False,0,QKV
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",364.0,16384.0,0.0,32768.0,0.0,0.0,32768.0,32768.0,0.0,512.0,0.0,32768.0,32768.0,5.516,2778.356,0.0,0.0,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0,10.397238225511654,14.524173799334966,False,False,0,add
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",365.0,16384.0,0.0,32768.0,0.0,0.0,32768.0,32768.0,0.0,512.0,0.0,32768.0,32768.0,5.4639999999999995,2783.82,0.0,0.0,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1024.0,1024.0,10.397238225511654,14.524173799334966,False,False,0,add
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",366.0,4096.0,0.0,8192.0,0.0,0.0,8192.0,8192.0,0.0,256.0,0.0,16384.0,16384.0,4.34,2788.16,0.0,0.0,0.0,4096.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,512.0,512.0,9.011035410141815,14.524173799334966,False,False,0,add
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",367.0,131072.0,4358144.0,0.0,0.0,38654705664.0,4358144.0,38659063808.0,34048.0,64.0,0.99812382739212,81920.0,16384.0,12.620000000000001,2800.78,3297280.0,798720.0,131072.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,150994944.0,2560.0,512.0,24.378047094422,14.524173799334966,True,True,0,Attention
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),368.0,67108864.0,134742016.0,0.0,0.0,0.0,134742016.0,134742016.0,230912.0,1024.0,0.9955849889624724,4456448.0,131072.0,10.368,2811.148,0.0,524288.0,67108864.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,139264.0,4096.0,18.71887252295577,14.524173799334966,True,True,0,Wo
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",369.0,20480.0,40960.0,40960.0,0.0,0.0,81920.0,81920.0,0.0,1920.0,0.0,135168.0,16384.0,4.184,2815.3320000000003,36864.0,4096.0,0.0,20480.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4224.0,512.0,11.31351064723008,14.524173799334966,False,False,0,splitKreduce
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",370.0,4096.0,8192.0,0.0,0.0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,3.3200000000000003,2818.652,0.0,0.0,4096.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0,9.011035410141815,14.524173799334966,False,False,4096,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",371.0,35396.0,109812.0,9216.0,0.0,0.0,119028.0,119028.0,80.0,360.0,0.1818181818181818,49152.0,16640.0,6.724,2825.376,31280.0,16956.0,30788.0,4608.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0,11.68712243988325,14.524173799334966,False,False,0,other
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",372.0,16859136.0,34013184.0,163840.0,0.0,0.0,34177024.0,34177024.0,150016.0,140288.0,0.5167548500881834,17842176.0,65536.0,23.784,2849.16,196608.0,262144.0,16777216.0,81920.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,557568.0,2048.0,17.347064192659374,14.524173799334966,True,False,0,FFN1
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",373.0,0.0,16384.0,0.0,0.0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,3.38,2852.54,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,9.704121561132915,14.524173799334966,False,False,0,other
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",374.0,0.0,32768.0,0.0,0.0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,3.404,2855.944,0.0,32768.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,10.397238225511654,14.524173799334966,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",375.0,0.0,16384.0,0.0,0.0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,3.3240000000000003,2859.268,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,9.704121561132915,14.524173799334966,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",376.0,16384.0,32768.0,0.0,0.0,0.0,32768.0,32768.0,0.0,384.0,0.0,131072.0,65536.0,3.468,2862.736,0.0,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0,10.397238225511654,14.524173799334966,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",377.0,0.0,16384.0,0.0,0.0,0.0,16384.0,16384.0,0.0,256.0,0.0,65536.0,65536.0,3.388,2866.124,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,9.704121561132915,14.524173799334966,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",378.0,98275.625,241607.25,8192.0,0.0,0.0,249799.25,249799.25,0.0,256.0,0.0,65536.0,65536.0,3.46,2869.584,16384.0,36864.0,94179.625,4096.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,12.428416877473754,14.524173799334966,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",379.0,16384.0,32768.0,0.0,0.0,0.0,32768.0,32768.0,0.0,256.0,0.0,65536.0,65536.0,3.3840000000000003,2872.968,0.0,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2048.0,2048.0,10.397238225511654,14.524173799334966,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",380.0,0.0,16384.0,0.0,0.0,0.0,16384.0,16384.0,0.0,384.0,0.0,131072.0,65536.0,3.448,2876.4159999999997,0.0,16384.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4096.0,2048.0,9.704121561132915,14.524173799334966,False,False,0,other
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),381.0,268435456.0,537722880.0,0.0,0.0,0.0,537722880.0,537722880.0,842176.0,1664.0,0.9980280621918848,17825792.0,212992.0,27.624000000000002,2904.04,0.0,851968.0,268435456.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,557056.0,6656.0,20.10285389434301,14.524173799334966,True,True,0,FFN2
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",382.0,20480.0,61440.0,40960.0,0.0,0.0,102400.0,102400.0,0.0,2560.0,0.0,217088.0,16384.0,5.0120000000000005,2909.0519999999997,57344.0,4096.0,0.0,20480.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6784.0,512.0,11.536651757164861,14.524173799334966,False,False,0,splitKreduce
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",383.0,4096.0,8192.0,0.0,0.0,0.0,8192.0,8192.0,0.0,96.0,0.0,32768.0,16384.0,3.348,2912.3999999999996,0.0,0.0,4096.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1024.0,512.0,9.011035410141815,14.524173799334966,False,False,4096,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",384.0,35396.0,109812.0,9216.0,0.0,0.0,119028.0,119028.0,80.0,360.0,0.1818181818181818,49152.0,16640.0,6.5760000000000005,2918.9759999999997,31280.0,16956.0,30788.0,4608.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1536.0,520.0,11.68712243988325,14.524173799334966,False,False,0,other
