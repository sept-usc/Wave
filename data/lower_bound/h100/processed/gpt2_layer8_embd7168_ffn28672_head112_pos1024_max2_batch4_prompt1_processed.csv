Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum,flops_log,flops_threshold,is_matmul_candidate,is_attention_candidate,elementwise_add_fma_ops,role
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",387.0,619057152.0,1247920128.0,4988928.0,0.0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632937568.0,344064.0,221.756,14952.39200000001,5160960.0,9633792.0,616562688.0,2494464.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19779299.0,10752.0,20.94873393001877,15.979159843192733,True,False,0,QKV
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",388.0,114688.0,0.0,229376.0,0.0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.208,14958.600000000013,0.0,0.0,0.0,114688.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7168.0,7168.0,12.343122217099008,15.979159843192733,False,False,0,add
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",389.0,114688.0,0.0,229376.0,0.0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.284000000000001,14964.884000000013,0.0,0.0,0.0,114688.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7168.0,7168.0,12.343122217099008,15.979159843192733,False,False,0,add
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",390.0,28672.0,0.0,57344.0,0.0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.656000000000001,14969.540000000012,0.0,0.0,0.0,28672.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3584.0,3584.0,10.956840934798622,15.979159843192733,False,False,0,add
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",391.0,917504.0,30507008.0,0.0,0.0,270582939648.0,30507008.0,270613446656.0,238336.0,448.0,0.99812382739212,573440.0,114688.0,29.252000000000002,14998.792000000012,23080960.0,5591040.0,917504.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1056964608.0,17920.0,3584.0,26.323957243455144,15.979159843192733,True,True,0,Attention
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",392.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,140368.0,3.08,15001.872000000014,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4386.5,0.0,15.979159843192733,False,False,0,other
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",393.0,411960192.0,823459840.0,919296.0,0.0,0.0,824379136.0,824379136.0,8233344.0,2507150.5,0.7665849660184758,215536172.0,1862832.0,130.276,15132.148000000012,0.0,458752.0,411500544.0,459648.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6735505.375,58213.5,20.530141099770493,15.979159843192733,True,True,0,Wo
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",394.0,0.0,28672.0,0.0,0.0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.612,15136.760000000011,28672.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4480.0,0.0,10.263711192398603,15.979159843192733,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",395.0,28672.0,57344.0,0.0,0.0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.484,15140.244000000013,0.0,0.0,28672.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7168.0,3584.0,10.956840934798622,15.979159843192733,False,False,28672,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",396.0,158276.0,503028.0,9216.0,0.0,0.0,512244.0,512244.0,80.0,2472.0,0.0313479623824451,344064.0,114944.0,19.728,15159.972000000012,154160.0,41532.0,153668.0,4608.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10752.0,3592.0,13.146558305194553,15.979159843192733,False,False,0,other
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",397.0,825409536.0,1663893504.0,6651904.0,0.0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839146832.0,458752.0,277.264,15437.236000000012,6881280.0,12845056.0,822083584.0,3325952.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,26223338.5,14336.0,21.236416002271014,15.979159843192733,True,False,0,FFN1
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",398.0,0.0,114688.0,0.0,0.0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.48,15440.716000000011,0.0,114688.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,14336.0,14336.0,11.64997939616457,15.979159843192733,False,False,0,other
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",399.0,0.0,229376.0,0.0,0.0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.468,15444.184000000012,0.0,229376.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,14336.0,14336.0,12.343122217099008,15.979159843192733,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",400.0,0.0,114688.0,0.0,0.0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.556,15447.740000000013,0.0,114688.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,14336.0,14336.0,11.64997939616457,15.979159843192733,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",401.0,114688.0,229376.0,0.0,0.0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.6719999999999997,15451.412000000013,0.0,0.0,114688.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,28672.0,14336.0,12.343122217099008,15.979159843192733,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",402.0,0.0,114688.0,0.0,0.0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.484,15454.896000000012,0.0,114688.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,14336.0,14336.0,11.64997939616457,15.979159843192733,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",403.0,640604.375,1596600.75,57344.0,0.0,0.0,1653944.75,1653944.75,0.0,1792.0,0.0,458752.0,458752.0,3.6719999999999997,15458.568000000012,114688.0,258048.0,611932.375,28672.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,14336.0,14336.0,14.31867435475338,15.979159843192733,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",404.0,114688.0,229376.0,0.0,0.0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.532,15462.100000000013,0.0,0.0,114688.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,14336.0,14336.0,12.343122217099008,15.979159843192733,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",405.0,0.0,114688.0,0.0,0.0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.716,15465.816000000012,0.0,114688.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,28672.0,14336.0,11.64997939616457,15.979159843192733,False,False,0,other
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",406.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,138848.0,3.052,15468.868000000013,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4339.0,0.0,15.979159843192733,False,False,0,other
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",407.0,1662635632.0,3323371520.0,3792096.0,0.0,0.0,3327163616.0,3327163616.0,33227824.0,12433571.75,0.7277977076592154,874620016.0,7685024.0,387.824,15856.692000000014,0.0,1892352.0,1660739584.0,1896048.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,27331875.5,240157.0,21.92538601131275,15.979159843192733,True,True,0,FFN2
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",408.0,0.0,28672.0,0.0,0.0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.5680000000000005,15861.260000000013,28672.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4480.0,0.0,10.263711192398603,15.979159843192733,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",409.0,28672.0,57344.0,0.0,0.0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.432,15864.692000000012,0.0,0.0,28672.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7168.0,3584.0,10.956840934798622,15.979159843192733,False,False,28672,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",410.0,158276.0,503028.0,9216.0,0.0,0.0,512244.0,512244.0,80.0,2472.0,0.0313479623824451,344064.0,114944.0,19.508,15884.200000000012,154160.0,41532.0,153668.0,4608.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10752.0,3592.0,13.146558305194553,15.979159843192733,False,False,0,other
