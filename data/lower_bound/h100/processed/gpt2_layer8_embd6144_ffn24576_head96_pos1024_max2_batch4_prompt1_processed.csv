Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum,flops_log,flops_threshold,is_matmul_candidate,is_attention_candidate,elementwise_add_fma_ops,role
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",386.0,454828032.0,916881408.0,3686400.0,0.0,0.0,920567808.0,920567808.0,3992832.0,3764736.0,0.5147015147015147,466590928.0,294912.0,174.98399999999998,11839.000000000011,3833856.0,7077888.0,452984832.0,1843200.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,14580966.5,9216.0,20.640501221323483,15.780795381667076,True,False,0,QKV
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",387.0,98304.0,0.0,196608.0,0.0,0.0,196608.0,196608.0,0.0,3072.0,0.0,196608.0,196608.0,5.852,11844.85200000001,0.0,0.0,0.0,98304.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,6144.0,12.18897226387732,15.780795381667076,False,False,0,add
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",388.0,98304.0,0.0,196608.0,0.0,0.0,196608.0,196608.0,0.0,3072.0,0.0,196608.0,196608.0,5.811999999999999,11850.664000000012,0.0,0.0,0.0,98304.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,6144.0,12.18897226387732,15.780795381667076,False,False,0,add
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",389.0,24576.0,0.0,49152.0,0.0,0.0,49152.0,49152.0,0.0,1536.0,0.0,98304.0,98304.0,4.616,11855.28000000001,0.0,0.0,0.0,24576.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3072.0,3072.0,10.802693161352469,15.780795381667076,False,False,0,add
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",390.0,786432.0,26148864.0,0.0,0.0,231928233984.0,26148864.0,231954382848.0,204288.0,384.0,0.99812382739212,491520.0,98304.0,20.311999999999998,11875.592000000011,19783680.0,4792320.0,786432.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,905969664.0,15360.0,3072.0,26.1698065636285,15.780795381667076,True,True,0,Attention
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",391.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,0.0,0.0,124992.0,3.144,11878.736000000012,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3906.0,0.0,15.780795381667076,False,False,0,other
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",392.0,302678688.0,605011968.0,689472.0,0.0,0.0,605701440.0,605701440.0,6049056.0,1813274.0,0.769383225055825,158331724.0,1396944.0,123.70400000000001,12002.440000000011,0.0,344064.0,302333952.0,344736.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4947866.375,43654.5,20.22189775101389,15.780795381667076,True,True,0,Wo
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",393.0,0.0,24576.0,0.0,0.0,0.0,24576.0,24576.0,0.0,7680.0,0.0,122880.0,0.0,4.392,12006.83200000001,24576.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3840.0,0.0,10.109566325223746,15.780795381667076,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",394.0,24576.0,49152.0,0.0,0.0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,3.396,12010.228000000012,0.0,0.0,24576.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0,10.802693161352469,15.780795381667076,False,False,24576,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",395.0,137284.0,437492.0,8192.0,0.0,0.0,445684.0,445684.0,80.0,2120.0,0.0363636363636363,294912.0,98560.0,17.704,12027.932000000012,133680.0,37436.0,133188.0,4096.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,9216.0,3080.0,13.007367703443471,15.780795381667076,False,False,0,other
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",396.0,606437376.0,1222508544.0,4915200.0,0.0,0.0,1227423744.0,1227423744.0,5323776.0,5019648.0,0.5147015147015147,617792816.0,393216.0,209.26,12237.19200000001,5111808.0,9437184.0,603979776.0,2457600.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19306025.5,12288.0,20.928183293503693,15.780795381667076,True,False,0,FFN1
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",397.0,0.0,98304.0,0.0,0.0,0.0,98304.0,98304.0,0.0,1536.0,0.0,393216.0,393216.0,3.552,12240.744000000012,0.0,98304.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12288.0,12288.0,11.495830169541591,15.780795381667076,False,False,0,other
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",398.0,0.0,196608.0,0.0,0.0,0.0,196608.0,196608.0,0.0,1536.0,0.0,393216.0,393216.0,3.584,12244.32800000001,0.0,196608.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12288.0,12288.0,12.18897226387732,15.780795381667076,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",399.0,0.0,98304.0,0.0,0.0,0.0,98304.0,98304.0,0.0,1536.0,0.0,393216.0,393216.0,3.488,12247.816000000013,0.0,98304.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12288.0,12288.0,11.495830169541591,15.780795381667076,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",400.0,98304.0,196608.0,0.0,0.0,0.0,196608.0,196608.0,0.0,2304.0,0.0,786432.0,393216.0,3.7199999999999998,12251.536000000011,0.0,0.0,98304.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,24576.0,12288.0,12.18897226387732,15.780795381667076,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",401.0,0.0,98304.0,0.0,0.0,0.0,98304.0,98304.0,0.0,1536.0,0.0,393216.0,393216.0,3.484,12255.020000000011,0.0,98304.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12288.0,12288.0,11.495830169541591,15.780795381667076,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",402.0,551466.25,1373268.5,49152.0,0.0,0.0,1422420.5,1422420.5,0.0,1536.0,0.0,393216.0,393216.0,3.528,12258.548000000013,98304.0,221184.0,526890.25,24576.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12288.0,12288.0,14.16787125892563,15.780795381667076,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",403.0,98304.0,196608.0,0.0,0.0,0.0,196608.0,196608.0,0.0,1536.0,0.0,393216.0,393216.0,3.496,12262.044000000013,0.0,0.0,98304.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12288.0,12288.0,12.18897226387732,15.780795381667076,False,False,0,add
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",404.0,0.0,98304.0,0.0,0.0,0.0,98304.0,98304.0,0.0,2304.0,0.0,786432.0,393216.0,3.58,12265.624000000013,0.0,98304.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,24576.0,12288.0,11.495830169541591,15.780795381667076,False,False,0,other
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",405.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,0.0,0.0,124256.0,3.072,12268.69600000001,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3883.0,0.0,15.780795381667076,False,False,0,other
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",406.0,1209533952.0,2418278400.0,1575936.0,0.0,0.0,2419854336.0,2419854336.0,24190464.0,9295785.125,0.7224296571498648,636467788.0,3193872.0,278.18,12546.876000000011,0.0,786432.0,1208745984.0,787968.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19889618.375,99808.5,21.606973183981122,15.780795381667076,True,True,0,FFN2
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",407.0,0.0,24576.0,0.0,0.0,0.0,24576.0,24576.0,0.0,7680.0,0.0,122880.0,0.0,4.444000000000001,12551.320000000012,24576.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3840.0,0.0,10.109566325223746,15.780795381667076,False,False,0,other
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",408.0,24576.0,49152.0,0.0,0.0,0.0,49152.0,49152.0,0.0,576.0,0.0,196608.0,98304.0,3.448,12554.768000000015,0.0,0.0,24576.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6144.0,3072.0,10.802693161352469,15.780795381667076,False,False,24576,elementwise_add
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",409.0,137284.0,437492.0,8192.0,0.0,0.0,445684.0,445684.0,80.0,2120.0,0.0363636363636363,294912.0,98560.0,17.688000000000002,12572.456000000013,133680.0,37436.0,133188.0,4096.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,9216.0,3080.0,13.007367703443471,15.780795381667076,False,False,0,other
