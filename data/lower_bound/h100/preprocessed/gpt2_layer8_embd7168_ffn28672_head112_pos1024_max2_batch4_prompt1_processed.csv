Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",1,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.784,2.784,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",2,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.624,5.4079999999999995,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",3,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.72,8.128,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",4,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.616,11.744,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",5,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.648,15.392,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",6,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,64.0,32.0,4.096,19.488,0.0,0.0,0.0,8.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::unrolled_elementwise_kernel<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)], std::array<char *, 2>, 8, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<1>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",7,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,4.32,23.808,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::sum_functor<long, long, long>::operator ()(at::TensorIterator &)::[lambda(long, long) (instance 1)]>, unsigned int, long, 4, 4>>(T3)",8,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,5.344,29.152,0.0,0.0,0.0,8.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",9,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.904,33.056,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",10,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.752,35.808,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",11,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.688,38.496,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void at_cuda_detail::DeviceScanInitKernel<at_cuda_detail::ScanTileState<long, 1>>(T1, int)",12,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,3.04,41.536,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanKernel<at_cuda_detail::DeviceScanPolicy<long, std::plus<long>>::Policy900, const long *, long *, at_cuda_detail::ScanTileState<long, 1>, std::plus<long>, at_cuda_detail::NullType, unsigned int, long, 0>(T2, T3, T4, int, T5, T6, T7)",13,0.0,0.0,0.0,0,0.0,0.0,0.0,129.0,6.0,0.9555555555555556,32.0,32.0,4.128,45.664,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",14,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.296,48.96,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",15,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.488,52.448,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",16,0.0,0.0,0.0,0,0.0,0.0,0.0,56.0,4.0,0.9333333333333333,32.0,32.0,4.224,56.672,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",17,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.296,59.967999999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",18,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.232,63.199999999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",19,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,0.0,3.456,66.65599999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,0.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",20,7168.0,0.0,14336.0,0,0.0,14336.0,14336.0,0.0,2688.0,0.0,30464.0,114688.0,6.208,72.86399999999999,0.0,0.0,0.0,7168.0,0,0,0,0,0,0,0,0.0,0.0,0.0,952.0,3584.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",21,7168.0,0.0,14336.0,0,0.0,14336.0,14336.0,0.0,2688.0,0.0,30464.0,114688.0,5.856,78.71999999999998,0.0,0.0,0.0,7168.0,0,0,0,0,0,0,0,0.0,0.0,0.0,952.0,3584.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",22,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.488,82.20799999999998,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",23,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.488,85.69599999999998,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",24,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,2.0,0.0,32.0,32.0,5.024,90.71999999999998,0.0,0.0,0.0,4.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",25,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.232,109.95199999999998,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",26,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632925824.0,344064.0,223.616,333.568,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19778932.0,10752.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",27,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.832,338.4,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",28,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.576,342.976,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",29,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.672,347.648,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",30,917504.0,30478336.0,0.0,0,270582939648.0,30478336.0,270613417984.0,238336.0,448.0,0.99812382739212,344064.0,114688.0,30.144,377.79200000000003,23052288.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,10752.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",31,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,135680.0,3.072,380.86400000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4240.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",32,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2664663.0,0.7554907975375681,216003840.0,1862944.0,130.048,510.91200000000003,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6750120.0,58217.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",33,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.832,515.744,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",34,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.392,519.1360000000001,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",35,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.584,538.72,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",36,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839193088.0,458752.0,278.144,816.864,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26224784.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",37,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.68,820.544,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",38,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.712,824.256,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",39,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.488,827.744,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",40,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.936,831.6800000000001,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",41,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,835.1360000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",42,640408.0,1596208.0,57344.0,0,0.0,1653552.0,1653552.0,0.0,1792.0,0.0,458752.0,458752.0,3.712,838.8480000000001,114688.0,258048.0,611736.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",43,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,842.3040000000001,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",44,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.904,846.2080000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",45,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,137856.0,3.04,849.248,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4308.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",46,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,12440643.0,0.7275878999835926,873849248.0,7685536.0,386.24,1235.488,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27307789.0,240173.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",47,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.8,1240.288,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",48,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.392,1243.68,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",49,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.872,1263.5520000000001,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",50,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632975488.0,344064.0,221.024,1484.576,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19780484.0,10752.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",51,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.64,1489.2160000000001,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",52,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.576,1493.7920000000001,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",53,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.576,1498.3680000000002,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",54,917504.0,30478336.0,0.0,0,270582939648.0,30478336.0,270613417984.0,238336.0,448.0,0.99812382739212,344064.0,114688.0,29.312,1527.68,23052288.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,10752.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",55,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,139264.0,3.072,1530.752,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4352.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",56,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2610022.0,0.7592978047591495,215306272.0,1862592.0,130.88,1661.632,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6728321.0,58206.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",57,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.512,1666.144,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",58,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.488,1669.632,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",59,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.776,1689.4080000000001,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",60,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839131008.0,458752.0,278.944,1968.352,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26222844.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",61,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,1971.872,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",62,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,1975.392,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",63,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.552,1978.944,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",64,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.936,1982.8799999999999,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",65,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,1986.3359999999998,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",66,640532.0,1596456.0,57344.0,0,0.0,1653800.0,1653800.0,0.0,1792.0,0.0,458752.0,458752.0,3.648,1989.9839999999997,114688.0,258048.0,611860.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",67,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,1993.4399999999996,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",68,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.744,1997.1839999999995,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",69,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,136320.0,3.104,2000.2879999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4260.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",70,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,12399274.0,0.7282475865548145,874340320.0,7684640.0,381.664,2381.9519999999993,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27323135.0,240145.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",71,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.704,2386.6559999999995,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",72,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.36,2390.0159999999996,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",73,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.36,2409.3759999999997,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",74,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632892544.0,344064.0,222.816,2632.1919999999996,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19777892.0,10752.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",75,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.576,2636.7679999999996,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",76,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.576,2641.3439999999996,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",77,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.544,2645.8879999999995,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",78,917504.0,30478336.0,0.0,0,270582939648.0,30478336.0,270613417984.0,238336.0,448.0,0.99812382739212,344064.0,114688.0,29.6,2675.4879999999994,23052288.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,10752.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",79,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,139648.0,3.072,2678.5599999999995,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4364.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",80,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2668999.0,0.7551903292714236,215602112.0,1862656.0,130.24,2808.7999999999993,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6737566.0,58208.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",81,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.384,2813.1839999999993,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",82,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.296,2816.479999999999,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",83,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.552,2836.0319999999992,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",84,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839266560.0,458752.0,277.728,3113.7599999999993,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26227080.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",85,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,3117.2159999999994,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",86,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.616,3120.8319999999994,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",87,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,3124.3519999999994,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",88,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.712,3128.0639999999994,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",89,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.584,3131.6479999999992,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",90,640716.0,1596824.0,57344.0,0,0.0,1654168.0,1654168.0,0.0,1792.0,0.0,458752.0,458752.0,3.584,3135.231999999999,114688.0,258048.0,612044.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",91,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,3138.751999999999,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",92,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.744,3142.495999999999,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",93,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,137216.0,3.04,3145.535999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4288.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",94,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,12994341.0,0.7188720822575057,873075744.0,7685664.0,382.528,3528.0639999999994,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27283617.0,240177.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",95,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.672,3532.7359999999994,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",96,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.488,3536.2239999999993,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",97,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.808,3556.0319999999992,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",98,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632836224.0,344064.0,221.792,3777.823999999999,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19776132.0,10752.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",99,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.512,3782.3359999999993,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",100,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.576,3786.9119999999994,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",101,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.416,3791.3279999999995,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",102,917504.0,30478336.0,0.0,0,270582939648.0,30478336.0,270613417984.0,238336.0,448.0,0.99812382739212,344064.0,114688.0,29.024,3820.3519999999994,23052288.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,10752.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",103,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,140544.0,3.072,3823.4239999999995,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4392.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",104,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2634360.0,0.7575973729133587,215346816.0,1863008.0,129.632,3953.0559999999996,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6729588.0,58219.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",105,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.576,3957.6319999999996,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",106,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.392,3961.0239999999994,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",107,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.488,3980.5119999999993,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",108,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839164544.0,458752.0,278.624,4259.1359999999995,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26223892.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",109,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.424,4262.5599999999995,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",110,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.584,4266.143999999999,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",111,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.552,4269.695999999999,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",112,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.776,4273.471999999999,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",113,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.36,4276.8319999999985,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",114,640876.0,1597144.0,57344.0,0,0.0,1654488.0,1654488.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,4280.287999999999,114688.0,258048.0,612204.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",115,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.392,4283.6799999999985,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",116,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.584,4287.263999999998,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",117,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,139008.0,3.04,4290.303999999998,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4344.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",118,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,12480238.0,0.7269576207365781,873489824.0,7685504.0,374.816,4665.119999999998,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27296557.0,240172.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",119,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.608,4669.727999999998,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",120,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.712,4673.439999999999,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",121,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.072,4692.511999999999,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",122,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632851072.0,344064.0,222.72,4915.231999999999,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19776596.0,10752.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",123,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.576,4919.807999999999,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",124,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.448,4924.255999999999,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",125,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.608,4928.864,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",126,917504.0,30478336.0,0.0,0,270582939648.0,30478336.0,270613417984.0,238336.0,448.0,0.99812382739212,344064.0,114688.0,29.216,4958.08,23052288.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,10752.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",127,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,141184.0,3.072,4961.152,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4412.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",128,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2411395.0,0.7734660286175171,215360352.0,1862848.0,130.944,5092.0960000000005,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6730011.0,58214.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",129,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.608,5096.704000000001,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",130,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.456,5100.160000000001,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",131,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.68,5119.840000000001,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",132,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839099008.0,458752.0,279.744,5399.584000000001,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26221844.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",133,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.584,5403.168000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",134,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.584,5406.752,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",135,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,5410.272000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",136,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.616,5413.888000000001,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",137,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.392,5417.280000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",138,640840.0,1597072.0,57344.0,0,0.0,1654416.0,1654416.0,0.0,1792.0,0.0,458752.0,458752.0,3.584,5420.8640000000005,114688.0,258048.0,612168.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",139,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,5424.320000000001,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",140,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.616,5427.936000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",141,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,139904.0,3.072,5431.008000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4372.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",142,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,11682949.0,0.7398631058966632,873042176.0,7685568.0,375.808,5806.816000000001,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27282568.0,240174.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",143,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.576,5811.392000000001,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",144,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.68,5815.072000000001,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",145,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.552,5834.624000000001,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",146,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,633101568.0,344064.0,224.064,6058.688000000001,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19784424.0,10752.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",147,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.608,6063.296000000001,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",148,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.576,6067.872000000001,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",149,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.64,6072.5120000000015,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",150,917504.0,30478336.0,0.0,0,270582939648.0,30478336.0,270613417984.0,238336.0,448.0,0.99812382739212,344064.0,114688.0,29.568,6102.080000000002,23052288.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,10752.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",151,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,139008.0,3.104,6105.184000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4344.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",152,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2520250.0,0.7656364932505356,215287968.0,1862560.0,130.112,6235.296000000002,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6727749.0,58205.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",153,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.48,6239.776000000002,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",154,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.328,6243.104000000002,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",155,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.776,6262.880000000002,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",156,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839221760.0,458752.0,277.664,6540.544000000002,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26225680.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",157,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.84,6544.384000000002,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",158,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.552,6547.9360000000015,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",159,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.424,6551.3600000000015,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",160,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.68,6555.040000000002,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",161,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,6558.496000000002,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",162,640480.0,1596352.0,57344.0,0,0.0,1653696.0,1653696.0,0.0,1792.0,0.0,458752.0,458752.0,3.648,6562.144000000002,114688.0,258048.0,611808.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",163,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.36,6565.504000000002,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",164,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.744,6569.248000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",165,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,138624.0,3.04,6572.288000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4332.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",166,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,12773283.0,0.7223266170529331,874981440.0,7685152.0,389.76,6962.048000000002,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27343170.0,240161.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",167,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.512,6966.560000000001,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",168,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.488,6970.048000000002,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",169,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.616,6989.664000000002,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",170,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632919168.0,344064.0,223.2,7212.864000000001,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19778724.0,10752.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",171,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.544,7217.408000000001,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",172,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.448,7221.856000000002,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",173,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.48,7226.336000000001,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",174,917504.0,30478336.0,0.0,0,270582939648.0,30478336.0,270613417984.0,238336.0,448.0,0.99812382739212,344064.0,114688.0,29.696,7256.032000000001,23052288.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,10752.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",175,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,141824.0,3.104,7259.136000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4432.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",176,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2626645.0,0.7581355745387955,215849088.0,1862720.0,132.128,7391.264000000001,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6745284.0,58210.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",177,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.512,7395.776000000001,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",178,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.456,7399.232000000001,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",179,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.136,7418.368000000001,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",180,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839289216.0,458752.0,278.592,7696.960000000001,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26227788.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",181,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,7700.480000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",182,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,7704.000000000002,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",183,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.488,7707.488000000002,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",184,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.776,7711.264000000002,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",185,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.488,7714.752000000002,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",186,640828.0,1597048.0,57344.0,0,0.0,1654392.0,1654392.0,0.0,1792.0,0.0,458752.0,458752.0,3.584,7718.336000000002,114688.0,258048.0,612156.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",187,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.488,7721.824000000002,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",188,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.648,7725.4720000000025,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",189,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,141184.0,3.104,7728.576000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4412.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",190,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,12660971.0,0.7240944984500028,874427584.0,7685024.0,385.216,8113.792000000003,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27325862.0,240157.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",191,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.352,8118.144000000003,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",192,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.712,8121.856000000003,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",193,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.488,8141.344000000004,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",194,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632790400.0,344064.0,221.312,8362.656000000004,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19774700.0,10752.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",195,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.896,8367.552000000005,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",196,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.544,8372.096000000005,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",197,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.64,8376.736000000004,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",198,917504.0,30478336.0,0.0,0,270582939648.0,30478336.0,270613417984.0,238336.0,448.0,0.99812382739212,344064.0,114688.0,29.056,8405.792000000005,23052288.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,10752.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",199,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,138240.0,3.296,8409.088000000005,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4320.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",200,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2703290.0,0.7528224863335465,215352128.0,1862624.0,130.688,8539.776000000005,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6729754.0,58207.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",201,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.8,8544.576000000005,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",202,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.488,8548.064000000004,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",203,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.68,8567.744000000004,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",204,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839044224.0,458752.0,278.528,8846.272000000004,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26220132.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",205,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,8849.792000000005,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",206,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.552,8853.344000000005,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",207,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.68,8857.024000000005,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",208,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.744,8860.768000000005,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",209,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.488,8864.256000000005,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",210,641080.0,1597552.0,57344.0,0,0.0,1654896.0,1654896.0,0.0,1792.0,0.0,458752.0,458752.0,3.488,8867.744000000004,114688.0,258048.0,612408.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",211,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,8871.264000000005,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",212,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.712,8874.976000000004,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",213,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,139648.0,3.04,8878.016000000005,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4364.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",214,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,11869315.0,0.7368055876005792,874671552.0,7684704.0,384.128,9262.144000000006,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27333486.0,240147.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",215,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.896,9267.040000000006,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",216,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.648,9270.688000000006,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",217,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.392,9290.080000000005,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void scal_64addr_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",218,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,74637.0,0.0,0.0,2015520.0,3.808,9293.888000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,62985.0
"void sgemm_largek_lds64<1, 0, 6, 3, 4, 5, 2, 66>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",219,2891077776.0,5778923520.0,6451488.0,0,0.0,5785375008.0,5785375008.0,57780432.0,12429089.0,0.8229714599534157,1492974048.0,14064160.0,764.192,10058.080000000005,0.0,3219456.0,2887852032.0,3225744.0,0,0,0,0,0,0,0,0.0,0.0,0.0,46655439.0,439505.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",220,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.784,10060.864000000005,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",221,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,6.0,0.0,64.0,128.0,4.48,10065.344000000005,0.0,0.0,0.0,256.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,4.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",222,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.552,10068.896000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",223,128.0,201728.0,256.0,0,0.0,201984.0,201984.0,0.0,3158.0,0.0,804128.0,804128.0,3.712,10072.608000000004,0.0,201728.0,0.0,128.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25129.0,25129.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",224,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,3.136,10075.744000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",225,0.0,0.0,0.0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813152.0,55104.0,6.208,10081.952000000005,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25411.0,1722.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",226,96000.0,0.0,192000.0,0,0.0,192000.0,192000.0,13200.0,82608.0,0.1377755511022044,5134848.0,0.0,8.0,10089.952000000005,0.0,0.0,0.0,96000.0,0,0,0,0,0,0,0,0.0,0.0,0.0,160464.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",227,0.0,0.0,0.0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813152.0,54912.0,5.952,10095.904000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25411.0,1716.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",228,96000.0,0.0,192000.0,0,0.0,192000.0,192000.0,13200.0,82608.0,0.1377755511022044,5134848.0,0.0,8.32,10104.224000000004,0.0,0.0,0.0,96000.0,0,0,0,0,0,0,0,0.0,0.0,0.0,160464.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",229,0.0,0.0,0.0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813152.0,54656.0,5.92,10110.144000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25411.0,1708.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",230,89600.0,0.0,179200.0,0,0.0,179200.0,179200.0,13200.0,82808.0,0.13748854262144822,5134848.0,0.0,8.032,10118.176000000003,0.0,0.0,0.0,89600.0,0,0,0,0,0,0,0,0.0,0.0,0.0,160464.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",231,0.0,0.0,0.0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813152.0,53760.0,5.984,10124.160000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25411.0,1680.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",232,83200.0,0.0,166400.0,0,0.0,166400.0,166400.0,13200.0,83008.0,0.13720272742391484,5134848.0,128.0,8.16,10132.320000000003,0.0,0.0,0.0,83200.0,0,0,0,0,0,0,0,0.0,0.0,0.0,160464.0,4.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",233,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,21.0,0.0,6432.0,800.0,3.744,10136.064000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,201.0,25.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",234,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.816,10138.880000000005,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",235,0.0,0.0,0.0,0,0.0,0.0,0.0,640.0,23.0,0.9653092006033183,800.0,0.0,5.312,10144.192000000005,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",236,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,3.04,10147.232000000005,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",237,0.0,0.0,0.0,0,0.0,0.0,0.0,640.0,23.0,0.9653092006033183,800.0,0.0,5.472,10152.704000000005,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",238,51200.0,0.0,102400.0,0,0.0,102400.0,102400.0,61336.0,13012.0,0.8249852047129714,831584.0,8608.0,9.056,10161.760000000006,0.0,0.0,0.0,51200.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25987.0,269.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",239,0.0,0.0,0.0,0,0.0,0.0,0.0,1832.0,32.0,0.9828326180257511,2560.0,0.0,8.736,10170.496000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,80.0,0.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",240,402056.0,0.0,804112.0,0,0.0,804112.0,804112.0,0.0,18849.0,0.0,816544.0,67520.0,6.048,10176.544000000007,0.0,0.0,0.0,402056.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25517.0,2110.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",241,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,4.512,10181.056000000008,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,31412.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",242,201028.0,0.0,402056.0,0,0.0,402056.0,402056.0,0.0,6283.0,0.0,0.0,1608224.0,4.192,10185.248000000007,0.0,0.0,0.0,201028.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,50257.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",243,0.0,0.0,0.0,0,0.0,0.0,0.0,135168.0,6283.0,0.9555817915744674,804128.0,0.0,6.016,10191.264000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25129.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",244,0.0,0.0,0.0,0,0.0,0.0,0.0,128.0,64.0,0.6666666666666666,8192.0,0.0,3.36,10194.624000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",245,0.0,0.0,0.0,0,0.0,0.0,0.0,65855.0,28330.0,0.6992090035568297,2729920.0,2012864.0,14.944,10209.568000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,85310.0,62902.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",246,0.0,0.0,0.0,0,0.0,0.0,0.0,14210.0,28330.0,0.3340385519511048,2734656.0,2451264.0,11.424,10220.992000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,85458.0,76602.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",247,0.0,0.0,0.0,0,0.0,0.0,0.0,15311.0,28231.0,0.3516374994258417,2719936.0,2451040.0,12.704,10233.696000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,84998.0,76595.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",248,0.0,0.0,0.0,0,0.0,0.0,0.0,15311.0,28243.0,0.35154061624649857,2732736.0,1904096.0,12.928,10246.624000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,85398.0,59503.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",249,0.0,0.0,0.0,0,0.0,0.0,0.0,14784.0,6283.0,0.7017610480846822,1608224.0,0.0,4.864,10251.488000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",250,0.0,0.0,0.0,0,0.0,0.0,0.0,32.0,16.0,0.6666666666666666,2048.0,0.0,3.488,10254.976000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",251,0.0,0.0,0.0,0,0.0,0.0,0.0,14833.0,15198.0,0.4939229462888349,1870368.0,1336640.0,9.632,10264.608000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,58449.0,41770.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",252,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,25132.0,0.0,2428640.0,2412352.0,5.312,10269.920000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,75895.0,75386.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",253,3122040.0,6655044.0,615296.0,0,0.0,7270340.0,7270340.0,528.0,6704.0,0.07300884955752213,1085632.0,753344.0,32.16,10302.080000000005,825232.0,201028.0,2814392.0,307648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,33926.0,23542.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",254,0.0,1024200.0,0.0,0,0.0,1024200.0,1024200.0,112284.0,12568.0,0.8993368147887099,804256.0,611520.0,97.888,10399.968000000006,1024200.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25133.0,19110.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",255,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3158.0,0.0,804128.0,200928.0,3.712,10403.680000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25129.0,6279.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",256,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,128.0,3.2,10406.880000000006,0.0,0.0,0.0,4.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",257,402056.0,0.0,804112.0,0,0.0,804112.0,804112.0,0.0,18849.0,0.0,1809280.0,91296.0,12.16,10419.040000000006,0.0,0.0,0.0,402056.0,0,0,0,0,0,0,0,0.0,0.0,0.0,56540.0,2853.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",258,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,4.768,10423.808000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,31412.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",259,3122056.0,6655044.0,615328.0,0,0.0,7270372.0,7270372.0,528.0,6704.0,0.07300884955752213,1083840.0,753408.0,32.256,10456.064000000006,825232.0,201028.0,2814392.0,307664.0,0,0,0,0,0,0,0,0.0,0.0,0.0,33870.0,23544.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",260,38912.0,0.0,77824.0,0,0.0,77824.0,77824.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,9.376,10465.440000000006,0.0,0.0,0.0,38912.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",261,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.36,10468.800000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",262,38912.0,0.0,77824.0,0,0.0,77824.0,77824.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,9.632,10478.432000000006,0.0,0.0,0.0,38912.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",263,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.456,10481.888000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",264,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.424,10485.312000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",265,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,4.768,10490.080000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",266,4096.0,220484.0,8192.0,0,0.0,228676.0,228676.0,320.0,1582.0,0.16824395373291273,804224.0,128.0,16.0,10506.080000000007,220484.0,0.0,0.0,4096.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25132.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",267,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.552,10509.632000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",268,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,2.0,0.0,32.0,32.0,4.96,10514.592000000006,0.0,0.0,0.0,4.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",269,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.392,10517.984000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",270,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,4.608,10522.592000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",271,2213376.0,4023944.0,804864.0,0,0.0,4828808.0,4828808.0,0.0,6283.0,0.0,0.0,804128.0,5.984,10528.576000000006,0.0,402056.0,1810944.0,402432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,25129.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",272,1256416.0,2010280.0,502552.0,0,0.0,2512832.0,2512832.0,0.0,4737.0,0.0,1608256.0,0.0,5.344,10533.920000000006,0.0,0.0,1005140.0,251276.0,0,0,0,0,0,0,0,0.0,0.0,0.0,50258.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",273,2560.0,0.0,5120.0,0,0.0,5120.0,5120.0,640.0,1582.0,0.28802880288028804,804288.0,128.0,22.624,10556.544000000005,0.0,0.0,0.0,2560.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25134.0,4.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",274,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.488,10560.032000000005,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",275,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.456,10563.488000000005,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",276,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,3.0,0.0,64.0,32.0,3.808,10567.296000000006,0.0,0.0,0.0,4.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",277,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.2,10570.496000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",278,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,6.0,0.0,64.0,128.0,4.384,10574.880000000006,0.0,0.0,0.0,256.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,4.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",279,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.784,10577.664000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",280,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.752,10580.416000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",281,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.36,10583.776000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",282,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.848,10586.624000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",283,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,96.0,32.0,4.096,10590.720000000007,0.0,0.0,0.0,8.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",284,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,5.0,0.0,32.0,32.0,6.944,10597.664000000006,0.0,0.0,0.0,2.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",285,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.424,10601.088000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",286,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.36,10604.448000000008,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",287,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.936,10608.384000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",288,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,4.768,10613.152000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",289,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.424,10616.576000000008,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",290,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.456,10620.032000000008,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::index_elementwise_kernel<128, 4, void native::gpu_index_kernel<void native::index_kernel_impl<native::OpaqueType<8>>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>, const T1 &)::[lambda(int) (instance 1)]>(long, T3)",291,128.0,0.0,256.0,0,0.0,256.0,256.0,0.0,3.0,0.0,96.0,32.0,4.864,10624.896000000008,0.0,0.0,0.0,128.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",292,0.0,0.0,0.0,0,0.0,0.0,0.0,56.0,4.0,0.9333333333333333,64.0,32.0,4.096,10628.992000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",293,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,64.0,3.232,10632.224000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,2.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",294,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,3.264,10635.488000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",295,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,96.0,0.0,3.232,10638.720000000007,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3.0,0.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",296,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,2.0,0.0,64.0,32.0,3.904,10642.624000000007,0.0,0.0,0.0,4.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",297,7168.0,0.0,14336.0,0,0.0,14336.0,14336.0,0.0,2688.0,0.0,116480.0,114688.0,6.848,10649.472000000007,0.0,0.0,0.0,7168.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3640.0,3584.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",298,7168.0,0.0,14336.0,0,0.0,14336.0,14336.0,0.0,2688.0,0.0,30464.0,114688.0,5.76,10655.232000000007,0.0,0.0,0.0,7168.0,0,0,0,0,0,0,0,0.0,0.0,0.0,952.0,3584.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",299,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.296,10658.528000000008,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",300,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,3.36,10661.888000000008,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",301,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,2.0,0.0,32.0,32.0,5.184,10667.072000000007,0.0,0.0,0.0,8.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",302,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.328,10686.400000000007,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",303,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632856576.0,344064.0,220.64,10907.040000000006,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19776768.0,10752.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",304,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.144,10913.184000000007,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",305,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.272,10919.456000000007,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",306,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.608,10924.064000000008,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",307,917504.0,30507008.0,0.0,0,270582939648.0,30507008.0,270613446656.0,238336.0,448.0,0.99812382739212,573440.0,114688.0,29.12,10953.184000000008,23080960.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,17920.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",308,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,140800.0,3.2,10956.38400000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4400.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",309,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2448165.0,0.7708034510854225,214792256.0,1863072.0,129.6,11085.98400000001,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6712258.0,58221.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",310,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.416,11090.400000000009,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",311,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.36,11093.76000000001,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",312,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,20.352,11114.11200000001,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",313,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839062144.0,458752.0,276.96,11391.07200000001,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26220692.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",314,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,11394.52800000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",315,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,11397.98400000001,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",316,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,11401.44000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",317,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.744,11405.18400000001,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",318,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,11408.70400000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",319,640382.0,1596156.0,57344.0,0,0.0,1653500.0,1653500.0,0.0,1792.0,0.0,458752.0,458752.0,3.712,11412.41600000001,114688.0,258048.0,611710.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",320,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.584,11416.000000000011,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",321,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.872,11419.87200000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",322,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,136576.0,3.04,11422.912000000011,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4268.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",323,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,12097687.0,0.7330932021924695,875853536.0,7685216.0,395.744,11818.656000000012,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27370423.0,240163.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",324,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.416,11823.072000000011,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",325,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.456,11826.528000000011,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",326,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.616,11846.144000000011,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",327,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,633099776.0,344064.0,221.376,12067.520000000011,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19784368.0,10752.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",328,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.208,12073.728000000012,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",329,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.208,12079.936000000012,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",330,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.8,12084.736000000012,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",331,917504.0,30507008.0,0.0,0,270582939648.0,30507008.0,270613446656.0,238336.0,448.0,0.99812382739212,573440.0,114688.0,29.184,12113.920000000011,23080960.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,17920.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",332,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,140928.0,3.008,12116.92800000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4404.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",333,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2511067.0,0.7662908650832512,215748416.0,1862880.0,129.92,12246.84800000001,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6742138.0,58215.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",334,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.832,12251.680000000011,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",335,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.392,12255.072000000011,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",336,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.616,12274.688000000011,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",337,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839101824.0,458752.0,276.8,12551.48800000001,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26221932.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",338,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,12554.94400000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",339,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.424,12558.368000000011,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",340,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,12561.824000000011,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",341,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.616,12565.440000000011,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",342,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,12568.896000000012,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",343,640581.0,1596554.0,57344.0,0,0.0,1653898.0,1653898.0,0.0,1792.0,0.0,458752.0,458752.0,3.584,12572.480000000012,114688.0,258048.0,611909.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",344,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,12576.000000000013,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",345,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.712,12579.712000000012,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",346,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,138112.0,3.008,12582.720000000012,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4316.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",347,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,12009413.0,0.7345237287591194,876364352.0,7685088.0,399.136,12981.856000000013,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27386386.0,240159.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",348,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.48,12986.336000000012,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",349,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.488,12989.824000000011,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",350,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.648,13009.47200000001,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",351,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632981504.0,344064.0,221.984,13231.456000000011,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19780672.0,10752.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",352,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.112,13237.56800000001,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",353,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.304,13243.87200000001,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",354,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.64,13248.51200000001,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",355,917504.0,30507008.0,0.0,0,270582939648.0,30507008.0,270613446656.0,238336.0,448.0,0.99812382739212,573440.0,114688.0,29.152,13277.66400000001,23080960.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,17920.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",356,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,138496.0,3.04,13280.70400000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4328.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",357,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2554906.0,0.7631769749495979,215662656.0,1862848.0,131.072,13411.77600000001,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6739458.0,58214.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",358,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.544,13416.32000000001,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",359,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.392,13419.71200000001,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",360,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.296,13439.00800000001,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",361,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839069184.0,458752.0,276.704,13715.71200000001,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26220912.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",362,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.488,13719.20000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",363,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.424,13722.62400000001,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",364,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.712,13726.33600000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",365,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.776,13730.11200000001,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",366,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.552,13733.66400000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",367,640510.0,1596412.0,57344.0,0,0.0,1653756.0,1653756.0,0.0,1792.0,0.0,458752.0,458752.0,3.68,13737.34400000001,114688.0,258048.0,611838.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",368,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.584,13740.92800000001,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",369,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.712,13744.64000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",370,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,139136.0,3.04,13747.680000000011,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4348.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",371,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,12481329.0,0.7269402694904454,876313376.0,7685056.0,389.696,14137.376000000011,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27384793.0,240158.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",372,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.672,14142.048000000012,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",373,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.456,14145.504000000012,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",374,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.616,14165.120000000012,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",375,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632813696.0,344064.0,222.048,14387.168000000012,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19775428.0,10752.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",376,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.144,14393.312000000013,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",377,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.336,14399.648000000012,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",378,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.448,14404.096000000012,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",379,917504.0,30507008.0,0.0,0,270582939648.0,30507008.0,270613446656.0,238336.0,448.0,0.99812382739212,573440.0,114688.0,29.44,14433.536000000013,23080960.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,17920.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",380,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,140928.0,3.04,14436.576000000014,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4404.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",381,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2439241.0,0.7714479669171058,215797856.0,1862496.0,128.288,14564.864000000014,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6743683.0,58203.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",382,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.448,14569.312000000014,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",383,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.36,14572.672000000015,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",384,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.872,14592.544000000014,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",385,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839198592.0,458752.0,276.832,14869.376000000015,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26224956.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",386,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,14872.832000000015,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",387,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.392,14876.224000000015,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",388,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.488,14879.712000000014,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",389,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.808,14883.520000000015,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",390,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,14886.976000000015,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",391,640787.0,1596966.0,57344.0,0,0.0,1654310.0,1654310.0,0.0,1792.0,0.0,458752.0,458752.0,3.68,14890.656000000015,114688.0,258048.0,612115.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",392,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.552,14894.208000000015,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",393,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.648,14897.856000000014,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",394,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,139776.0,3.008,14900.864000000014,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4368.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",395,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,11602073.0,0.7411978662364538,873250848.0,7685120.0,386.048,15286.912000000015,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27289089.0,240160.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",396,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.416,15291.328000000014,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",397,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.424,15294.752000000015,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",398,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.36,15314.112000000016,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",399,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,633017600.0,344064.0,220.768,15534.880000000016,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19781800.0,10752.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",400,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.336,15541.216000000015,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",401,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.336,15547.552000000014,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",402,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.608,15552.160000000014,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",403,917504.0,30507008.0,0.0,0,270582939648.0,30507008.0,270613446656.0,238336.0,448.0,0.99812382739212,573440.0,114688.0,29.184,15581.344000000014,23080960.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,17920.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",404,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,140032.0,3.04,15584.384000000015,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4376.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",405,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2550142.0,0.7635141363377298,215407392.0,1862752.0,129.856,15714.240000000014,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6731481.0,58211.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",406,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.576,15718.816000000013,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",407,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.584,15722.400000000014,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",408,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.712,15742.112000000014,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",409,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839158912.0,458752.0,277.504,16019.616000000015,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26223716.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",410,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.392,16023.008000000014,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",411,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.488,16026.496000000014,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",412,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.584,16030.080000000014,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",413,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.616,16033.696000000014,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",414,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.552,16037.248000000014,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",415,641041.0,1597474.0,57344.0,0,0.0,1654818.0,1654818.0,0.0,1792.0,0.0,458752.0,458752.0,3.68,16040.928000000014,114688.0,258048.0,612369.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",416,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.424,16044.352000000015,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",417,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.744,16048.096000000016,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",418,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,138496.0,3.2,16051.296000000017,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4328.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",419,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,13271156.0,0.7145925351480829,873009984.0,7684864.0,382.656,16433.952000000016,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27281562.0,240152.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",420,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.672,16438.624000000014,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",421,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.328,16441.952000000016,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",422,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.392,16461.344000000016,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",423,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632863616.0,344064.0,221.76,16683.104000000014,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19776988.0,10752.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",424,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.08,16689.184000000016,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",425,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.144,16695.328000000016,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",426,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.768,16700.096000000016,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",427,917504.0,30507008.0,0.0,0,270582939648.0,30507008.0,270613446656.0,238336.0,448.0,0.99812382739212,573440.0,114688.0,29.632,16729.728000000017,23080960.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,17920.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",428,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,141824.0,3.008,16732.73600000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4432.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",429,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2497082.0,0.7672895745238819,215397216.0,1862688.0,131.584,16864.320000000018,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6731163.0,58209.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",430,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.608,16868.928000000018,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",431,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.584,16872.512000000017,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",432,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.52,16892.032000000017,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",433,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839142912.0,458752.0,277.056,17169.088000000018,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26223216.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",434,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.424,17172.512000000017,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",435,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.584,17176.096000000016,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",436,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.744,17179.840000000015,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",437,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.552,17183.392000000014,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",438,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.488,17186.880000000016,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",439,640527.0,1596446.0,57344.0,0,0.0,1653790.0,1653790.0,0.0,1792.0,0.0,458752.0,458752.0,3.68,17190.560000000016,114688.0,258048.0,611855.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",440,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,17194.080000000016,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",441,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.744,17197.824000000015,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",442,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,139264.0,3.04,17200.864000000016,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4352.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",443,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,12459826.0,0.7272824056391607,873135584.0,7684672.0,378.048,17578.912000000015,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27285487.0,240146.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",444,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.704,17583.616000000016,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",445,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.296,17586.912000000015,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",446,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.52,17606.432000000015,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",447,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,632864768.0,344064.0,224.064,17830.496000000014,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19777024.0,10752.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",448,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.24,17836.736000000015,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",449,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.368,17843.104000000014,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",450,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.8,17847.904000000013,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",451,917504.0,30507008.0,0.0,0,270582939648.0,30507008.0,270613446656.0,238336.0,448.0,0.99812382739212,573440.0,114688.0,29.216,17877.120000000014,23080960.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,17920.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",452,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,141184.0,3.168,17880.288000000015,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4412.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",453,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2575374.0,0.7617317798466016,215301568.0,1862816.0,130.624,18010.912000000015,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6728174.0,58213.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",454,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.864,18015.776000000016,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",455,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.584,18019.360000000015,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",456,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.552,18038.912000000015,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",457,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839192576.0,458752.0,279.104,18318.016000000014,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26224768.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",458,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,18321.536000000015,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",459,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,18325.056000000015,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",460,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,18328.512000000013,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",461,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.712,18332.224000000013,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",462,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,18335.68000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",463,640580.0,1596552.0,57344.0,0,0.0,1653896.0,1653896.0,0.0,1792.0,0.0,458752.0,458752.0,3.808,18339.488000000012,114688.0,258048.0,611908.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",464,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.552,18343.04000000001,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",465,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.712,18346.75200000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",466,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,141184.0,3.04,18349.792000000012,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4412.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",467,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,12378315.0,0.7285822638921484,872266048.0,7685024.0,378.208,18728.00000000001,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27258314.0,240157.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",468,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.736,18732.73600000001,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",469,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.52,18736.256000000012,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",470,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.808,18756.064000000013,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",471,619057152.0,1247920128.0,4988928.0,0,0.0,1252909056.0,1252909056.0,5432448.0,5123328.0,0.5146422205245734,633003008.0,344064.0,221.408,18977.472000000012,5160960.0,9633792.0,616562688.0,2494464.0,0,0,0,0,0,0,0,0.0,0.0,0.0,19781344.0,10752.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",472,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.4,18983.872000000014,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",473,114688.0,0.0,229376.0,0,0.0,229376.0,229376.0,0.0,3584.0,0.0,229376.0,229376.0,6.304,18990.176000000014,0.0,0.0,0.0,114688.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,7168.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",474,28672.0,0.0,57344.0,0,0.0,57344.0,57344.0,0.0,1792.0,0.0,114688.0,114688.0,4.576,18994.752000000015,0.0,0.0,0.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3584.0,3584.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params)",475,917504.0,30507008.0,0.0,0,270582939648.0,30507008.0,270613446656.0,238336.0,448.0,0.99812382739212,573440.0,114688.0,29.088,19023.840000000015,23080960.0,5591040.0,917504.0,0.0,0,0,0,0,0,0,0,0.0,0.0,1056964608.0,17920.0,3584.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",476,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,138752.0,3.136,19026.976000000013,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4336.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",477,411960192.0,823459840.0,919296.0,0,0.0,824379136.0,824379136.0,8233344.0,2481227.0,0.7684249794042151,216182016.0,1863104.0,131.264,19158.240000000013,0.0,458752.0,411500544.0,459648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6755688.0,58222.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",478,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.608,19162.848000000013,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",479,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.616,19166.464000000014,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",480,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.904,19186.368000000013,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",481,825409536.0,1663893504.0,6651904.0,0,0.0,1670545408.0,1670545408.0,7243264.0,6831104.0,0.5146422205245734,839248512.0,458752.0,277.152,19463.52000000001,6881280.0,12845056.0,822083584.0,3325952.0,0,0,0,0,0,0,0,0.0,0.0,0.0,26226516.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",482,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.648,19467.168000000012,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",483,0.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.456,19470.62400000001,0.0,229376.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",484,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.552,19474.17600000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",485,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,2688.0,0.0,917504.0,458752.0,3.552,19477.72800000001,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",486,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,1792.0,0.0,458752.0,458752.0,3.392,19481.12000000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",487,640427.0,1596246.0,57344.0,0,0.0,1653590.0,1653590.0,0.0,1792.0,0.0,458752.0,458752.0,3.552,19484.67200000001,114688.0,258048.0,611755.0,28672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",488,114688.0,229376.0,0.0,0,0.0,229376.0,229376.0,0.0,1792.0,0.0,458752.0,458752.0,3.52,19488.19200000001,0.0,0.0,114688.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,14336.0,14336.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",489,0.0,114688.0,0.0,0,0.0,114688.0,114688.0,0.0,2688.0,0.0,917504.0,458752.0,3.584,19491.77600000001,0.0,114688.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,28672.0,14336.0
"void scal_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",490,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,7168.0,0.0,0.0,138240.0,3.04,19494.81600000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4320.0
"void sgemm_largek_lds64<0, 0, 6, 3, 4, 5, 2, 64>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",491,1662635632.0,3323371520.0,3792096.0,0,0.0,3327163616.0,3327163616.0,33227824.0,13168775.0,0.7161693899158428,876766400.0,7685152.0,393.056,19887.87200000001,0.0,1892352.0,1660739584.0,1896048.0,0,0,0,0,0,0,0,0.0,0.0,0.0,27398950.0,240161.0
"void cublasLt::globalKernel<8, 32, float, float, float, 1, 1, 1>(int, int, long, T3 *, cublasLtEpilogue_t, int, T4 *, long, void *, long, long, long, T5 *, long, int *)",492,0.0,28672.0,0.0,0,0.0,28672.0,28672.0,0.0,8960.0,0.0,143360.0,0.0,4.448,19892.32000000001,28672.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4480.0,0.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",493,28672.0,57344.0,0.0,0,0.0,57344.0,57344.0,0.0,672.0,0.0,229376.0,114688.0,3.488,19895.80800000001,0.0,0.0,28672.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,7168.0,3584.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",494,158276.0,503028.0,9216.0,0,0.0,512244.0,512244.0,80.0,2472.0,0.03134796238244514,344064.0,114944.0,19.104,19914.91200000001,154160.0,41532.0,153668.0,4608.0,0,0,0,0,0,0,0,0.0,0.0,0.0,10752.0,3592.0
"void scal_64addr_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *)",495,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,74637.0,0.0,0.0,2004608.0,3.936,19918.848000000013,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,62644.0
"void sgemm_largek_lds64<1, 0, 6, 3, 4, 5, 2, 66>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *)",496,2891077776.0,5778923520.0,6451488.0,0,0.0,5785375008.0,5785375008.0,57780432.0,12471660.0,0.8224727599570985,1493719328.0,14064640.0,759.264,20678.112000000012,0.0,3219456.0,2887852032.0,3225744.0,0,0,0,0,0,0,0,0.0,0.0,0.0,46678729.0,439520.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",497,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.752,20680.864000000012,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",498,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,6.0,0.0,96.0,160.0,3.968,20684.832000000013,0.0,0.0,0.0,256.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3.0,5.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",499,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.36,20688.192000000014,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",500,128.0,201728.0,256.0,0,0.0,201984.0,201984.0,0.0,3158.0,0.0,804128.0,804128.0,3.68,20691.872000000014,0.0,201728.0,0.0,128.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25129.0,25129.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",501,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.912,20694.784000000014,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",502,0.0,0.0,0.0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813152.0,55104.0,5.92,20700.704000000012,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25411.0,1722.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",503,96000.0,0.0,192000.0,0,0.0,192000.0,192000.0,13200.0,82608.0,0.1377755511022044,5134848.0,0.0,8.096,20708.800000000014,0.0,0.0,0.0,96000.0,0,0,0,0,0,0,0,0.0,0.0,0.0,160464.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",504,0.0,0.0,0.0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813152.0,54784.0,5.92,20714.720000000012,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25411.0,1712.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",505,96000.0,0.0,192000.0,0,0.0,192000.0,192000.0,13200.0,82608.0,0.1377755511022044,5134848.0,0.0,8.128,20722.848000000013,0.0,0.0,0.0,96000.0,0,0,0,0,0,0,0,0.0,0.0,0.0,160464.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",506,0.0,0.0,0.0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813152.0,54464.0,5.888,20728.73600000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25411.0,1702.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",507,80000.0,0.0,160000.0,0,0.0,160000.0,160000.0,13200.0,83108.0,0.13706026498317897,5134848.0,0.0,8.128,20736.864000000012,0.0,0.0,0.0,80000.0,0,0,0,0,0,0,0,0.0,0.0,0.0,160464.0,0.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",508,0.0,0.0,0.0,0,0.0,0.0,0.0,3200.0,9484.0,0.2522863450015768,813152.0,55168.0,5.888,20742.75200000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25411.0,1724.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",509,76800.0,0.0,153600.0,0,0.0,153600.0,153600.0,13200.0,83208.0,0.13691809808314662,5134848.0,128.0,8.128,20750.880000000012,0.0,0.0,0.0,76800.0,0,0,0,0,0,0,0,0.0,0.0,0.0,160464.0,4.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",510,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,21.0,0.0,6432.0,800.0,3.968,20754.848000000013,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,201.0,25.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",511,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,3.072,20757.920000000013,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",512,0.0,0.0,0.0,0,0.0,0.0,0.0,640.0,23.0,0.9653092006033183,800.0,0.0,5.664,20763.584000000013,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",513,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.816,20766.400000000012,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",514,0.0,0.0,0.0,0,0.0,0.0,0.0,640.0,23.0,0.9653092006033183,800.0,0.0,5.792,20772.192000000014,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",515,51200.0,0.0,102400.0,0,0.0,102400.0,102400.0,51004.0,13014.0,0.7967134243494017,831584.0,8768.0,8.896,20781.088000000014,0.0,0.0,0.0,51200.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25987.0,274.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",516,0.0,0.0,0.0,0,0.0,0.0,0.0,1832.0,32.0,0.9828326180257511,2560.0,0.0,8.256,20789.344000000016,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,80.0,0.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",517,402056.0,0.0,804112.0,0,0.0,804112.0,804112.0,0.0,18849.0,0.0,816544.0,68896.0,6.272,20795.616000000016,0.0,0.0,0.0,402056.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25517.0,2153.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",518,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,4.064,20799.680000000015,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,31412.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",519,201028.0,0.0,402056.0,0,0.0,402056.0,402056.0,0.0,6283.0,0.0,0.0,1608224.0,3.936,20803.616000000016,0.0,0.0,0.0,201028.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,50257.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",520,0.0,0.0,0.0,0,0.0,0.0,0.0,135168.0,6283.0,0.9555817915744674,804128.0,0.0,5.984,20809.600000000017,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25129.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",521,0.0,0.0,0.0,0,0.0,0.0,0.0,128.0,64.0,0.6666666666666666,8192.0,0.0,3.456,20813.056000000015,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",522,0.0,0.0,0.0,0,0.0,0.0,0.0,65855.0,28292.0,0.699491221175396,2732608.0,2018464.0,14.88,20827.936000000016,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,85394.0,63077.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",523,0.0,0.0,0.0,0,0.0,0.0,0.0,14210.0,28341.0,0.3339521985382247,2733504.0,2451264.0,11.584,20839.520000000015,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,85422.0,76602.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",524,0.0,0.0,0.0,0,0.0,0.0,0.0,15311.0,28223.0,0.3517021178848716,2717376.0,1667040.0,12.32,20851.840000000015,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,84918.0,52095.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",525,0.0,0.0,0.0,0,0.0,0.0,0.0,15311.0,28241.0,0.3515567597354886,2722624.0,2152384.0,12.512,20864.352000000014,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,85082.0,67262.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",526,0.0,0.0,0.0,0,0.0,0.0,0.0,14784.0,6283.0,0.7017610480846822,1608224.0,0.0,4.864,20869.216000000015,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,50257.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",527,0.0,0.0,0.0,0,0.0,0.0,0.0,32.0,16.0,0.6666666666666666,2048.0,0.0,3.456,20872.672000000013,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",528,0.0,0.0,0.0,0,0.0,0.0,0.0,14833.0,15185.0,0.49413685122259976,1861664.0,1339552.0,9.632,20882.304000000015,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,58177.0,41861.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",529,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,25132.0,0.0,2429728.0,2412352.0,5.376,20887.680000000015,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,75929.0,75386.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",530,3122040.0,6655044.0,615296.0,0,0.0,7270340.0,7270340.0,528.0,6704.0,0.07300884955752213,1056928.0,753536.0,31.424,20919.104000000014,825232.0,201028.0,2814392.0,307648.0,0,0,0,0,0,0,0,0.0,0.0,0.0,33029.0,23548.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",531,0.0,1024200.0,0.0,0,0.0,1024200.0,1024200.0,112284.0,12568.0,0.8993368147887099,804224.0,610656.0,96.416,21015.520000000015,1024200.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25132.0,19083.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",532,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3158.0,0.0,804128.0,200800.0,3.744,21019.264000000014,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25129.0,6275.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",533,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,1.0,0.0,0.0,128.0,3.232,21022.496000000014,0.0,0.0,0.0,4.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",534,402056.0,0.0,804112.0,0,0.0,804112.0,804112.0,0.0,18849.0,0.0,1809280.0,91872.0,11.904,21034.400000000012,0.0,0.0,0.0,402056.0,0,0,0,0,0,0,0,0.0,0.0,0.0,56540.0,2871.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",535,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,4737.0,0.0,1005184.0,0.0,4.448,21038.848000000013,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,31412.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",536,3122058.0,6655044.0,615332.0,0,0.0,7270376.0,7270376.0,528.0,6704.0,0.07300884955752213,1076768.0,753472.0,31.968,21070.816000000013,825232.0,201028.0,2814392.0,307666.0,0,0,0,0,0,0,0,0.0,0.0,0.0,33649.0,23546.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",537,38912.0,0.0,77824.0,0,0.0,77824.0,77824.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,9.312,21080.128000000015,0.0,0.0,0.0,38912.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",538,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.392,21083.520000000015,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",539,38912.0,0.0,77824.0,0,0.0,77824.0,77824.0,2037.0,1598.0,0.560385144429161,804256.0,832.0,9.216,21092.736000000015,0.0,0.0,0.0,38912.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25133.0,26.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",540,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.648,21096.384000000016,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",541,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.52,21099.904000000017,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",542,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,4.512,21104.416000000016,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",543,4096.0,220484.0,8192.0,0,0.0,228676.0,228676.0,320.0,1582.0,0.16824395373291273,804224.0,128.0,15.872,21120.288000000015,220484.0,0.0,0.0,4096.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25132.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",544,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.36,21123.648000000016,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",545,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,2.0,0.0,32.0,32.0,5.184,21128.832000000017,0.0,0.0,0.0,4.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",546,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.2,21132.032000000017,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",547,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,4.704,21136.73600000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",548,2213376.0,4023944.0,804864.0,0,0.0,4828808.0,4828808.0,0.0,6283.0,0.0,0.0,804128.0,6.048,21142.784000000018,0.0,402056.0,1810944.0,402432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,25129.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",549,1256418.0,2010280.0,502556.0,0,0.0,2512836.0,2512836.0,0.0,4737.0,0.0,1608256.0,0.0,5.536,21148.320000000018,0.0,0.0,1005140.0,251278.0,0,0,0,0,0,0,0,0.0,0.0,0.0,50258.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",550,2560.0,0.0,5120.0,0,0.0,5120.0,5120.0,640.0,1582.0,0.28802880288028804,804224.0,128.0,21.728,21170.048000000017,0.0,0.0,0.0,2560.0,0,0,0,0,0,0,0,0.0,0.0,0.0,25132.0,4.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",551,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.168,21173.21600000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",552,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.392,21176.60800000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",553,4.0,0.0,8.0,0,0.0,8.0,8.0,0.0,3.0,0.0,64.0,32.0,3.808,21180.41600000002,0.0,0.0,0.0,4.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",554,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.488,21183.90400000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",555,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,6.0,0.0,96.0,160.0,3.968,21187.87200000002,0.0,0.0,0.0,256.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3.0,5.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",556,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.848,21190.720000000023,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",557,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.848,21193.568000000025,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",558,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.392,21196.960000000025,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",559,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.848,21199.808000000026,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",560,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,3.0,0.0,128.0,32.0,4.064,21203.872000000025,0.0,0.0,0.0,8.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",561,2.0,0.0,4.0,0,0.0,4.0,4.0,0.0,5.0,0.0,32.0,32.0,6.88,21210.752000000026,0.0,0.0,0.0,2.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",562,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.616,21214.368000000028,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",563,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.488,21217.85600000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",564,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,4.032,21221.888000000028,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",565,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,4.64,21226.528000000028,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",566,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.36,21229.888000000028,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
