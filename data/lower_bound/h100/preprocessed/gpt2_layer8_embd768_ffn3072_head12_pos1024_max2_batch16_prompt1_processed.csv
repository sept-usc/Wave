Kernel Name,index,total_fma_ops,float_flops,half_flops,double_flops,tensor_flops,total_flops,total_flops_with_tensor,shared_ops,external_memory_ops,shared_ratio,global_op_ld_lookup_miss_bytes,global_op_st_lookup_miss_bytes,kernel_duration,accumulated_time,smsp__sass_thread_inst_executed_op_fadd_pred_on.sum,smsp__sass_thread_inst_executed_op_fmul_pred_on.sum,smsp__sass_thread_inst_executed_op_ffma_pred_on.sum,smsp__sass_thread_inst_executed_op_hfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hadd_pred_on.sum,smsp__sass_thread_inst_executed_op_hmul_pred_on.sum,sm__ops_path_tensor_src_fp16_dst_fp16.sum,sm__ops_path_tensor_src_fp16_dst_fp32.sum,sm__ops_path_tensor_src_bf16_dst_fp32.sum,sm__ops_path_tensor_src_fp8.sum,sm__ops_path_tensor_src_tf32_dst_fp32.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_miss.sum,l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_miss.sum
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",1,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,128.0,2.88,2.88,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",2,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,128.0,2.592,5.4719999999999995,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",3,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.72,8.192,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",4,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.296,11.488,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",5,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.616,15.104,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",6,32.0,0.0,64.0,0,0.0,64.0,64.0,0.0,3.0,0.0,160.0,32.0,3.808,18.912,0.0,0.0,0.0,32.0,0,0,0,0,0,0,0,0.0,0.0,0.0,5.0,1.0
"void native::unrolled_elementwise_kernel<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)], std::array<char *, 2>, 8, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<1>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",7,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,128.0,4.448,23.36,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,4.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::sum_functor<long, long, long>::operator ()(at::TensorIterator &)::[lambda(long, long) (instance 1)]>, unsigned int, long, 4, 4>>(T3)",8,32.0,0.0,64.0,0,0.0,64.0,64.0,0.0,2.0,0.0,128.0,32.0,5.12,28.48,0.0,0.0,0.0,32.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",9,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.52,32.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",10,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,128.0,2.752,34.752,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",11,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.72,37.472,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void at_cuda_detail::DeviceScanInitKernel<at_cuda_detail::ScanTileState<long, 1>>(T1, int)",12,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,3.2,40.672000000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanKernel<at_cuda_detail::DeviceScanPolicy<long, std::plus<long>>::Policy900, const long *, long *, at_cuda_detail::ScanTileState<long, 1>, std::plus<long>, at_cuda_detail::NullType, unsigned int, long, 0>(T2, T3, T4, int, T5, T6, T7)",13,0.0,0.0,0.0,0,0.0,0.0,0.0,129.0,6.0,0.9555555555555556,32.0,32.0,4.064,44.736000000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",14,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.296,48.032000000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",15,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.264,51.29600000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",16,0.0,0.0,0.0,0,0.0,0.0,0.0,176.0,16.0,0.9166666666666666,128.0,32.0,4.16,55.456,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4.0,1.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",17,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,128.0,128.0,3.328,58.784000000000006,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4.0,4.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",18,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,128.0,32.0,3.456,62.24000000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",19,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,160.0,0.0,3.456,65.69600000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,5.0,0.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",20,768.0,0.0,1536.0,0,0.0,1536.0,1536.0,0.0,1152.0,0.0,3840.0,49152.0,10.208,75.90400000000001,0.0,0.0,0.0,768.0,0,0,0,0,0,0,0,0.0,0.0,0.0,120.0,1536.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",21,768.0,0.0,1536.0,0,0.0,1536.0,1536.0,0.0,1152.0,0.0,3840.0,49152.0,9.664,85.56800000000001,0.0,0.0,0.0,768.0,0,0,0,0,0,0,0,0.0,0.0,0.0,120.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",22,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.456,89.02400000000002,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",23,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,128.0,32.0,3.488,92.51200000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",24,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,2.0,0.0,32.0,32.0,5.024,97.53600000000002,0.0,0.0,0.0,16.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",25,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.336,103.87200000000001,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",26,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.432,122.30400000000002,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",27,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.544,126.84800000000001,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",28,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.48,131.328,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",29,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.512,135.84,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",30,393216.0,13062144.0,0.0,0,115964116992.0,13062144.0,115977179136.0,102144.0,192.0,0.99812382739212,147456.0,49152.0,16.864,152.704,9879552.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,4608.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),31,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.656,163.36,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",32,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,5.056,168.41600000000003,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",33,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.296,171.71200000000002,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",34,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.24,177.95200000000003,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",35,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,19.104,197.05600000000004,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",36,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.392,200.44800000000004,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",37,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.424,203.87200000000004,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",38,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.936,207.80800000000005,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",39,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.456,211.26400000000004,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",40,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.424,214.68800000000005,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",41,298112.0,731392.0,24576.0,0,0.0,755968.0,755968.0,0.0,768.0,0.0,196608.0,196608.0,3.456,218.14400000000003,49152.0,110592.0,285824.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",42,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.328,221.47200000000004,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",43,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.456,224.92800000000003,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),44,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,615044.0,0.4699377070542366,14748704.0,791936.0,90.688,315.61600000000004,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,460897.0,24748.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",45,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.424,319.04,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",46,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.496,325.536,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",47,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.432,343.968,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",48,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.32,348.288,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",49,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.384,352.672,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",50,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.544,357.216,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",51,393216.0,13062144.0,0.0,0,115964116992.0,13062144.0,115977179136.0,102144.0,192.0,0.99812382739212,147456.0,49152.0,15.424,372.64,9879552.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,4608.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),52,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.592,383.23199999999997,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",53,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,4.992,388.224,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",54,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.616,391.84,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",55,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.72,398.56,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",56,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,18.72,417.28,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",57,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.296,420.57599999999996,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",58,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.328,423.90399999999994,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",59,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.232,427.13599999999997,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",60,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.328,430.46399999999994,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",61,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.36,433.82399999999996,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",62,298240.0,731648.0,24576.0,0,0.0,756224.0,756224.0,0.0,768.0,0.0,196608.0,196608.0,3.648,437.472,49152.0,110592.0,285952.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",63,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.552,441.024,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",64,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.456,444.48,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),65,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,619906.0,0.46797678653880154,14764160.0,797440.0,90.56,535.04,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,461380.0,24920.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",66,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.424,538.4639999999999,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",67,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.528,544.992,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",68,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.272,563.264,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",69,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.512,567.776,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",70,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.48,572.256,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",71,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.48,576.736,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",72,393216.0,13062144.0,0.0,0,115964116992.0,13062144.0,115977179136.0,102144.0,192.0,0.99812382739212,147456.0,49152.0,15.2,591.936,9879552.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,4608.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),73,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.432,602.368,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",74,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,4.96,607.3280000000001,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",75,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.392,610.7200000000001,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",76,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.656,617.3760000000001,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",77,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,19.104,636.4800000000001,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",78,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.52,640.0000000000001,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",79,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.552,643.5520000000001,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",80,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.488,647.0400000000002,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",81,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.392,650.4320000000002,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",82,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.68,654.1120000000002,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",83,297488.0,730144.0,24576.0,0,0.0,754720.0,754720.0,0.0,768.0,0.0,196608.0,196608.0,3.424,657.5360000000002,49152.0,110592.0,285200.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",84,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.36,660.8960000000002,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",85,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.552,664.4480000000002,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),86,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,616067.0,0.469523751299138,14779360.0,797184.0,91.008,755.4560000000002,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,461855.0,24912.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",87,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.424,758.8800000000002,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",88,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.4,765.2800000000002,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",89,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.4,783.6800000000002,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",90,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.48,788.1600000000002,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",91,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.256,792.4160000000002,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",92,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.352,796.7680000000001,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",93,393216.0,13062144.0,0.0,0,115964116992.0,13062144.0,115977179136.0,102144.0,192.0,0.99812382739212,147456.0,49152.0,15.52,812.2880000000001,9879552.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,4608.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),94,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.464,822.7520000000002,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",95,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,5.088,827.8400000000001,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",96,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.648,831.4880000000002,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",97,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.176,837.6640000000002,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",98,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,18.688,856.3520000000002,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",99,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.584,859.9360000000001,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",100,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.424,863.3600000000001,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",101,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.456,866.8160000000001,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",102,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.488,870.3040000000002,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",103,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.424,873.7280000000002,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",104,298000.0,731168.0,24576.0,0,0.0,755744.0,755744.0,0.0,768.0,0.0,196608.0,196608.0,3.52,877.2480000000002,49152.0,110592.0,285712.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",105,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.296,880.5440000000002,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",106,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.552,884.0960000000002,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),107,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,618734.0,0.46844797399343996,14760960.0,785792.0,90.784,974.8800000000002,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,461280.0,24556.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",108,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.616,978.4960000000002,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",109,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.208,984.7040000000002,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",110,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.304,1003.0080000000002,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",111,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.64,1007.6480000000001,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",112,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.576,1012.2240000000002,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",113,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.544,1016.7680000000001,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",114,393216.0,13062144.0,0.0,0,115964116992.0,13062144.0,115977179136.0,102144.0,192.0,0.99812382739212,147456.0,49152.0,15.424,1032.1920000000002,9879552.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,4608.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),115,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.4,1042.5920000000003,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",116,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,5.024,1047.6160000000002,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",117,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.616,1051.2320000000002,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",118,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.656,1057.8880000000001,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",119,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,19.008,1076.8960000000002,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",120,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.424,1080.3200000000002,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",121,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.392,1083.7120000000002,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",122,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.392,1087.1040000000003,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",123,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.52,1090.6240000000003,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",124,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.328,1093.9520000000002,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",125,297664.0,730496.0,24576.0,0,0.0,755072.0,755072.0,0.0,768.0,0.0,196608.0,196608.0,3.52,1097.4720000000002,49152.0,110592.0,285376.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",126,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.36,1100.832,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",127,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.456,1104.288,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),128,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,615335.0,0.46981987997742575,14750240.0,801024.0,90.4,1194.688,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,460945.0,25032.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",129,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.424,1198.112,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",130,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.4,1204.5120000000002,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",131,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,17.984,1222.496,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",132,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.608,1227.104,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",133,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.416,1231.52,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",134,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.48,1236.0,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",135,393216.0,13062144.0,0.0,0,115964116992.0,13062144.0,115977179136.0,102144.0,192.0,0.99812382739212,147456.0,49152.0,15.456,1251.456,9879552.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,4608.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),136,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.432,1261.888,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",137,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,4.928,1266.816,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",138,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.584,1270.4,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",139,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.4,1276.8000000000002,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",140,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,18.944,1295.7440000000001,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",141,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.456,1299.2,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",142,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.52,1302.72,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",143,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.456,1306.176,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",144,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.488,1309.664,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",145,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.296,1312.96,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",146,298048.0,731264.0,24576.0,0,0.0,755840.0,755840.0,0.0,768.0,0.0,196608.0,196608.0,3.424,1316.384,49152.0,110592.0,285760.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",147,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.392,1319.776,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",148,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.456,1323.232,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),149,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,620013.0,0.46793381578710247,14742048.0,786304.0,90.112,1413.344,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,460689.0,24572.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",150,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.552,1416.896,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",151,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.272,1423.168,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",152,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.272,1441.4399999999998,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",153,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.48,1445.9199999999998,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",154,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.448,1450.368,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",155,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.48,1454.848,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",156,393216.0,13062144.0,0.0,0,115964116992.0,13062144.0,115977179136.0,102144.0,192.0,0.99812382739212,147456.0,49152.0,15.648,1470.4959999999999,9879552.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,4608.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),157,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.624,1481.12,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",158,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,5.088,1486.2079999999999,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",159,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.584,1489.792,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",160,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.464,1496.2559999999999,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",161,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,19.168,1515.4239999999998,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",162,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.328,1518.7519999999997,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",163,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.68,1522.4319999999998,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",164,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.584,1526.0159999999998,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",165,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.552,1529.5679999999998,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",166,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.52,1533.0879999999997,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",167,297792.0,730752.0,24576.0,0,0.0,755328.0,755328.0,0.0,768.0,0.0,196608.0,196608.0,3.648,1536.7359999999996,49152.0,110592.0,285504.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",168,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.36,1540.0959999999995,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",169,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.424,1543.5199999999995,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),170,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,616059.0,0.4695269856605177,14766272.0,800128.0,90.112,1633.6319999999996,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,461446.0,25004.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",171,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.36,1636.9919999999995,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",172,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.4,1643.3919999999996,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",173,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.4,1661.7919999999997,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",174,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.448,1666.2399999999998,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",175,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.448,1670.6879999999999,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",176,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.64,1675.328,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",177,393216.0,13062144.0,0.0,0,115964116992.0,13062144.0,115977179136.0,102144.0,192.0,0.99812382739212,147456.0,49152.0,15.52,1690.848,9879552.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,4608.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),178,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.432,1701.28,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",179,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,4.928,1706.208,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",180,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.392,1709.6000000000001,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",181,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.4,1716.0000000000002,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",182,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,18.88,1734.8800000000003,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",183,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.648,1738.5280000000002,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",184,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.488,1742.0160000000003,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",185,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.584,1745.6000000000004,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",186,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.328,1748.9280000000003,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",187,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.36,1752.2880000000002,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",188,298144.0,731456.0,24576.0,0,0.0,756032.0,756032.0,0.0,768.0,0.0,196608.0,196608.0,3.52,1755.8080000000002,49152.0,110592.0,285856.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",189,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.712,1759.5200000000002,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",190,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.52,1763.0400000000002,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),191,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,622581.0,0.4669048799471855,14727488.0,783360.0,88.544,1851.5840000000003,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,460234.0,24480.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",192,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.488,1855.0720000000003,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",193,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.368,1861.4400000000003,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
sm80_xmma_gemm_f32f32_f32f32_f32_tn_n_tilesize32x128x8_stage3_warpsize1x2x1_ffma_aligna4_alignc4_execute_kernel__5x_cublas,194,1237931136.0,2475761664.0,100608.0,0,0.0,2475862272.0,2475862272.0,3085050.0,25136.0,0.9919181682381697,165424640.0,2600800.0,103.776,1965.2160000000003,0.0,0.0,1237880832.0,50304.0,0,0,0,0,0,0,0,0.0,0.0,0.0,5169520.0,81275.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",195,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,128.0,2.784,1968.0000000000005,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",196,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,6.0,0.0,256.0,512.0,3.968,1971.9680000000005,0.0,0.0,0.0,256.0,0,0,0,0,0,0,0,0.0,0.0,0.0,8.0,16.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",197,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.264,1975.2320000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",198,128.0,804864.0,256.0,0,0.0,805120.0,805120.0,0.0,12578.0,0.0,3216448.0,3216448.0,5.12,1980.3520000000003,0.0,804864.0,0.0,128.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100514.0,100514.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",199,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,64.0,3.072,1983.4240000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,2.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",200,0.0,0.0,0.0,0,0.0,0.0,0.0,12800.0,37936.0,0.2522863450015768,3247296.0,233792.0,6.688,1990.1120000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,101478.0,7306.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",201,384000.0,0.0,768000.0,0,0.0,768000.0,768000.0,52800.0,330432.0,0.1377755511022044,20524224.0,192.0,9.76,1999.8720000000003,0.0,0.0,0.0,384000.0,0,0,0,0,0,0,0,0.0,0.0,0.0,641382.0,6.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",202,0.0,0.0,0.0,0,0.0,0.0,0.0,12800.0,37936.0,0.2522863450015768,3247296.0,232704.0,6.56,2006.4320000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,101478.0,7272.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",203,230400.0,0.0,460800.0,0,0.0,460800.0,460800.0,52800.0,335232.0,0.13607125185551708,20527904.0,160.0,9.984,2016.4160000000002,0.0,0.0,0.0,230400.0,0,0,0,0,0,0,0,0.0,0.0,0.0,641497.0,5.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",204,0.0,0.0,0.0,0,0.0,0.0,0.0,12800.0,37936.0,0.2522863450015768,3247296.0,232000.0,6.624,2023.0400000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,101478.0,7250.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",205,358400.0,0.0,716800.0,0,0.0,716800.0,716800.0,52800.0,331232.0,0.13748854262144822,20524768.0,96.0,9.728,2032.7680000000003,0.0,0.0,0.0,358400.0,0,0,0,0,0,0,0,0.0,0.0,0.0,641399.0,3.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",206,0.0,0.0,0.0,0,0.0,0.0,0.0,12800.0,37936.0,0.2522863450015768,3247296.0,231296.0,6.528,2039.2960000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,101478.0,7228.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",207,332800.0,0.0,665600.0,0,0.0,665600.0,665600.0,52800.0,332032.0,0.13720272742391484,20525248.0,544.0,9.664,2048.9600000000005,0.0,0.0,0.0,332800.0,0,0,0,0,0,0,0,0.0,0.0,0.0,641414.0,17.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",208,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,75.0,0.0,25664.0,3200.0,5.696,2054.6560000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,802.0,100.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",209,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,3.104,2057.76,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",210,0.0,0.0,0.0,0,0.0,0.0,0.0,640.0,59.0,0.9155937052932761,3200.0,0.0,5.888,2063.648,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",211,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.912,2066.56,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",212,0.0,0.0,0.0,0,0.0,0.0,0.0,640.0,59.0,0.9155937052932761,3200.0,0.0,5.952,2072.512,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",213,204800.0,0.0,409600.0,0,0.0,409600.0,409600.0,186960.0,52048.0,0.7822332306868389,3321024.0,37024.0,10.144,2082.656,0.0,0.0,0.0,204800.0,0,0,0,0,0,0,0,0.0,0.0,0.0,103782.0,1157.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",214,0.0,0.0,0.0,0,0.0,0.0,0.0,7328.0,128.0,0.9828326180257511,10240.0,0.0,8.704,2091.36,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,320.0,0.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",215,1608224.0,0.0,3216448.0,0,0.0,3216448.0,3216448.0,0.0,75387.0,0.0,3266912.0,256096.0,6.592,2097.952,0.0,0.0,0.0,1608224.0,0,0,0,0,0,0,0,0.0,0.0,0.0,102091.0,8003.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",216,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,18867.0,0.0,4020576.0,0.0,5.376,2103.3280000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,125643.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",217,804112.0,0.0,1608224.0,0,0.0,1608224.0,1608224.0,0.0,25129.0,0.0,0.0,6432896.0,6.272,2109.6000000000004,0.0,0.0,0.0,804112.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,201028.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",218,0.0,0.0,0.0,0,0.0,0.0,0.0,135168.0,25129.0,0.8432347455036588,3216448.0,0.0,7.392,2116.992,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100514.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",219,0.0,0.0,0.0,0,0.0,0.0,0.0,128.0,64.0,0.6666666666666666,8192.0,0.0,3.52,2120.512,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",220,0.0,0.0,0.0,0,0.0,0.0,0.0,260937.0,118496.0,0.6877024402200125,11628736.0,7957376.0,24.96,2145.472,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,363398.0,248668.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",221,0.0,0.0,0.0,0,0.0,0.0,0.0,51954.0,124105.0,0.2950942581748164,11643968.0,9802944.0,17.344,2162.8160000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,363874.0,306342.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",222,0.0,0.0,0.0,0,0.0,0.0,0.0,53145.0,126885.0,0.2952007998666889,11685184.0,9802944.0,20.032,2182.8480000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,365162.0,306342.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",223,0.0,0.0,0.0,0,0.0,0.0,0.0,53145.0,123674.0,0.3005615912317115,11673280.0,9801664.0,20.352,2203.2000000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,364790.0,306302.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",224,0.0,0.0,0.0,0,0.0,0.0,0.0,14784.0,25129.0,0.37040563225014406,6432896.0,0.0,6.592,2209.7920000000004,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,201028.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",225,0.0,0.0,0.0,0,0.0,0.0,0.0,32.0,16.0,0.6666666666666666,2048.0,0.0,3.392,2213.184,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",226,0.0,0.0,0.0,0,0.0,0.0,0.0,59326.0,68904.0,0.46265304530921003,8375936.0,5547456.0,14.56,2227.744,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,261748.0,173358.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",227,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,100516.0,0.0,9714656.0,9649344.0,9.696,2237.44,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,303583.0,301542.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",228,12488160.0,26620176.0,2461184.0,0,0.0,29081360.0,29081360.0,2112.0,26816.0,0.07300884955752213,4442624.0,2911008.0,32.16,2269.6,3300928.0,804112.0,11257568.0,1230592.0,0,0,0,0,0,0,0,0.0,0.0,0.0,138832.0,90969.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",229,0.0,4096800.0,0.0,0,0.0,4096800.0,4096800.0,449136.0,50272.0,0.8993368147887099,3217408.0,2246912.0,98.464,2368.064,4096800.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100544.0,70216.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",230,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,12578.0,0.0,3216448.0,803936.0,4.512,2372.576,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100514.0,25123.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",231,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,1.0,0.0,0.0,512.0,3.296,2375.872,0.0,0.0,0.0,16.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,16.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",232,1608224.0,0.0,3216448.0,0,0.0,3216448.0,3216448.0,0.0,75387.0,0.0,7237024.0,400640.0,11.776,2387.6479999999997,0.0,0.0,0.0,1608224.0,0,0,0,0,0,0,0,0.0,0.0,0.0,226157.0,12520.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",233,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,18867.0,0.0,4020576.0,128.0,5.472,2393.12,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,125643.0,4.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",234,12488208.0,26620176.0,2461280.0,0,0.0,29081456.0,29081456.0,2112.0,26816.0,0.07300884955752213,4469440.0,2916960.0,32.256,2425.3759999999997,3300928.0,804112.0,11257568.0,1230640.0,0,0,0,0,0,0,0,0.0,0.0,0.0,139670.0,91155.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",235,152576.0,0.0,305152.0,0,0.0,305152.0,305152.0,7883.0,6387.0,0.5524176594253679,3216864.0,3200.0,9.44,2434.816,0.0,0.0,0.0,152576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100527.0,100.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",236,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.552,2438.368,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",237,152576.0,0.0,305152.0,0,0.0,305152.0,305152.0,7883.0,6387.0,0.5524176594253679,3216864.0,3200.0,10.048,2448.4159999999997,0.0,0.0,0.0,152576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100527.0,100.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",238,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.328,2451.7439999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",239,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.456,2455.2,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",240,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,4.832,2460.0319999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",241,16384.0,877328.0,32768.0,0,0.0,910096.0,910096.0,736.0,6328.0,0.10419026047565119,3216896.0,512.0,16.704,2476.736,877328.0,0.0,0.0,16384.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100528.0,16.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",242,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,3.072,2479.808,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",243,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,2.0,0.0,32.0,32.0,5.12,2484.928,0.0,0.0,0.0,16.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",244,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.296,2488.2239999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",245,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,4.512,2492.736,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",246,2973696.0,6474272.0,1081344.0,0,0.0,7555616.0,7555616.0,0.0,25129.0,0.0,0.0,3216448.0,7.488,2500.2239999999997,0.0,1608224.0,2433024.0,540672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,100514.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",247,5025136.0,8041120.0,2009152.0,0,0.0,10050272.0,10050272.0,0.0,18867.0,0.0,6432896.0,2560.0,7.456,2507.68,0.0,0.0,4020560.0,1004576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,201028.0,80.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",248,8704.0,0.0,17408.0,0,0.0,17408.0,17408.0,1472.0,6328.0,0.18871794871794872,3216896.0,512.0,22.912,2530.5919999999996,0.0,0.0,0.0,8704.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100528.0,16.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",249,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,256.0,128.0,3.36,2533.9519999999998,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,8.0,4.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",250,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,128.0,128.0,3.328,2537.2799999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4.0,4.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",251,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,160.0,128.0,3.968,2541.2479999999996,0.0,0.0,0.0,16.0,0,0,0,0,0,0,0,0.0,0.0,0.0,5.0,4.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",252,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,256.0,128.0,3.648,2544.8959999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,8.0,4.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",253,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,6.0,0.0,256.0,512.0,3.968,2548.8639999999996,0.0,0.0,0.0,256.0,0,0,0,0,0,0,0,0.0,0.0,0.0,8.0,16.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",254,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.72,2551.5839999999994,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",255,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.848,2554.4319999999993,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",256,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.456,2557.8879999999995,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",257,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.752,2560.6399999999994,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",258,32.0,0.0,64.0,0,0.0,64.0,64.0,0.0,3.0,0.0,288.0,32.0,3.712,2564.3519999999994,0.0,0.0,0.0,32.0,0,0,0,0,0,0,0,0.0,0.0,0.0,9.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",259,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,5.0,0.0,32.0,32.0,7.328,2571.6799999999994,0.0,0.0,0.0,8.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",260,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.52,2575.1999999999994,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",261,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.264,2578.4639999999995,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",262,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,160.0,128.0,4.224,2582.6879999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,5.0,4.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",263,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,128.0,32.0,4.96,2587.6479999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",264,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.232,2590.8799999999997,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<long>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(long) (instance 1)], std::array<char *, 2>>(int, T2, T3)",265,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.232,2594.1119999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::index_elementwise_kernel<128, 4, void native::gpu_index_kernel<void native::index_kernel_impl<native::OpaqueType<8>>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(at::TensorIteratorBase &, c10::ArrayRef<long>, c10::ArrayRef<long>, const T1 &)::[lambda(int) (instance 1)]>(long, T3)",266,128.0,0.0,256.0,0,0.0,256.0,256.0,0.0,3.0,0.0,288.0,128.0,4.928,2599.0399999999995,0.0,0.0,0.0,128.0,0,0,0,0,0,0,0,0.0,0.0,0.0,9.0,4.0
"void native::tensor_kernel_scan_innermost_dim<long, std::plus<long>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",267,0.0,0.0,0.0,0,0.0,0.0,0.0,176.0,16.0,0.9166666666666666,256.0,96.0,3.968,2603.0079999999994,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,8.0,3.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",268,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,256.0,256.0,3.168,2606.1759999999995,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,8.0,8.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",269,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,256.0,32.0,3.36,2609.5359999999996,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,8.0,1.0
"void native::vectorized_elementwise_kernel<2, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",270,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,288.0,0.0,3.584,2613.1199999999994,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,9.0,0.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",271,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,2.0,0.0,256.0,128.0,3.616,2616.7359999999994,0.0,0.0,0.0,16.0,0,0,0,0,0,0,0,0.0,0.0,0.0,8.0,4.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",272,768.0,0.0,1536.0,0,0.0,1536.0,1536.0,0.0,1152.0,0.0,43776.0,49152.0,13.6,2630.3359999999993,0.0,0.0,0.0,768.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1368.0,1536.0
"void native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long)",273,768.0,0.0,1536.0,0,0.0,1536.0,1536.0,0.0,1152.0,0.0,3840.0,49152.0,9.568,2639.9039999999995,0.0,0.0,0.0,768.0,0,0,0,0,0,0,0,0.0,0.0,0.0,120.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",274,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.328,2643.2319999999995,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",275,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,256.0,32.0,3.296,2646.5279999999993,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,8.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",276,32.0,0.0,64.0,0,0.0,64.0,64.0,0.0,2.0,0.0,32.0,32.0,4.928,2651.455999999999,0.0,0.0,0.0,32.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",277,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.336,2657.791999999999,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",278,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.336,2676.127999999999,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",279,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,5.888,2682.0159999999987,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",280,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,5.696,2687.7119999999986,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",281,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.448,2692.1599999999985,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",282,393216.0,13074432.0,0.0,0,115964116992.0,13074432.0,115977191424.0,102144.0,192.0,0.99812382739212,245760.0,49152.0,15.616,2707.7759999999985,9891840.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,7680.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),283,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.432,2718.2079999999983,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",284,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,4.928,2723.135999999998,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",285,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.52,2726.655999999998,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",286,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.304,2732.959999999998,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",287,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,18.88,2751.8399999999983,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",288,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.296,2755.135999999998,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",289,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.328,2758.463999999998,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",290,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.296,2761.759999999998,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",291,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.456,2765.215999999998,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",292,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.392,2768.607999999998,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",293,298135.0,731438.0,24576.0,0,0.0,756014.0,756014.0,0.0,768.0,0.0,196608.0,196608.0,3.552,2772.159999999998,49152.0,110592.0,285847.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",294,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.488,2775.647999999998,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",295,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.488,2779.1359999999977,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),296,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,621998.0,0.46713807679061886,14745056.0,795520.0,90.592,2869.727999999998,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,460783.0,24860.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",297,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.392,2873.1199999999976,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",298,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.24,2879.3599999999974,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",299,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.272,2897.6319999999973,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",300,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,5.888,2903.5199999999973,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",301,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,6.144,2909.663999999997,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",302,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.416,2914.079999999997,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",303,393216.0,13074432.0,0.0,0,115964116992.0,13074432.0,115977191424.0,102144.0,192.0,0.99812382739212,245760.0,49152.0,15.84,2929.9199999999973,9891840.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,7680.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),304,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.432,2940.351999999997,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",305,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,5.056,2945.407999999997,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",306,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.36,2948.7679999999973,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",307,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.304,2955.0719999999974,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",308,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,19.008,2974.079999999997,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",309,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.488,2977.567999999997,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",310,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.36,2980.927999999997,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",311,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.52,2984.447999999997,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",312,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.456,2987.9039999999973,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",313,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.392,2991.295999999997,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",314,298081.0,731330.0,24576.0,0,0.0,755906.0,755906.0,0.0,768.0,0.0,196608.0,196608.0,3.488,2994.783999999997,49152.0,110592.0,285793.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",315,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.488,2998.2719999999968,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",316,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.52,3001.7919999999967,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),317,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,631908.0,0.46320553726337677,14761440.0,806656.0,90.464,3092.2559999999967,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,461295.0,25208.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",318,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.616,3095.8719999999967,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",319,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.24,3102.1119999999964,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",320,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.624,3120.7359999999962,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",321,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,6.08,3126.815999999996,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",322,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,5.92,3132.7359999999962,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",323,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.448,3137.183999999996,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",324,393216.0,13074432.0,0.0,0,115964116992.0,13074432.0,115977191424.0,102144.0,192.0,0.99812382739212,245760.0,49152.0,15.264,3152.4479999999962,9891840.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,7680.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),325,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.528,3162.975999999996,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",326,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,5.056,3168.031999999996,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",327,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.296,3171.327999999996,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",328,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.112,3177.439999999996,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",329,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,18.56,3195.999999999996,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",330,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.456,3199.455999999996,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",331,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.456,3202.911999999996,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",332,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.52,3206.431999999996,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",333,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.552,3209.9839999999963,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",334,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.328,3213.3119999999963,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",335,298012.0,731192.0,24576.0,0,0.0,755768.0,755768.0,0.0,768.0,0.0,196608.0,196608.0,3.488,3216.799999999996,49152.0,110592.0,285724.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",336,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.328,3220.127999999996,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",337,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.328,3223.455999999996,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),338,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,621186.0,0.4674632608237188,14767488.0,819968.0,90.592,3314.047999999996,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,461484.0,25624.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",339,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.488,3317.535999999996,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",340,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.272,3323.807999999996,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",341,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.368,3342.175999999996,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",342,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,6.144,3348.3199999999956,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",343,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,5.696,3354.0159999999955,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",344,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.384,3358.3999999999955,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",345,393216.0,13074432.0,0.0,0,115964116992.0,13074432.0,115977191424.0,102144.0,192.0,0.99812382739212,245760.0,49152.0,15.84,3374.2399999999957,9891840.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,7680.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),346,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.464,3384.7039999999956,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",347,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,5.088,3389.791999999996,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",348,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.52,3393.311999999996,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",349,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.304,3399.615999999996,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",350,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,18.72,3418.3359999999957,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",351,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.488,3421.8239999999955,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",352,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.488,3425.3119999999954,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",353,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.296,3428.607999999995,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",354,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.52,3432.127999999995,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",355,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.424,3435.551999999995,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",356,298550.0,732268.0,24576.0,0,0.0,756844.0,756844.0,0.0,768.0,0.0,196608.0,196608.0,3.456,3439.0079999999953,49152.0,110592.0,286262.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",357,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.328,3442.3359999999952,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",358,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.552,3445.8879999999954,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),359,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,618292.0,0.4686259208712482,14761696.0,785536.0,89.632,3535.5199999999954,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,461303.0,24548.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",360,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.296,3538.8159999999953,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",361,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.208,3545.0239999999953,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",362,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.24,3563.263999999995,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",363,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,5.664,3568.9279999999953,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",364,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,5.76,3574.6879999999956,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",365,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.64,3579.3279999999954,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",366,393216.0,13074432.0,0.0,0,115964116992.0,13074432.0,115977191424.0,102144.0,192.0,0.99812382739212,245760.0,49152.0,15.616,3594.9439999999954,9891840.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,7680.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),367,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.432,3605.375999999995,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",368,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,4.928,3610.303999999995,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",369,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.328,3613.631999999995,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",370,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.4,3620.031999999995,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",371,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,19.136,3639.167999999995,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",372,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.456,3642.6239999999952,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",373,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.552,3646.1759999999954,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",374,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.52,3649.6959999999954,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",375,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.52,3653.2159999999953,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",376,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.424,3656.6399999999953,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",377,298051.0,731270.0,24576.0,0,0.0,755846.0,755846.0,0.0,768.0,0.0,196608.0,196608.0,3.584,3660.223999999995,49152.0,110592.0,285763.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",378,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.424,3663.647999999995,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",379,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.552,3667.1999999999953,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),380,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,620726.0,0.46764767934298795,14757056.0,807424.0,90.688,3757.8879999999954,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,461158.0,25232.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",381,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.52,3761.4079999999954,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",382,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.528,3767.935999999995,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",383,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.4,3786.3359999999952,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",384,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,5.632,3791.9679999999953,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",385,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,6.272,3798.2399999999952,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",386,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.352,3802.591999999995,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",387,393216.0,13074432.0,0.0,0,115964116992.0,13074432.0,115977191424.0,102144.0,192.0,0.99812382739212,245760.0,49152.0,15.456,3818.047999999995,9891840.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,7680.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),388,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.464,3828.511999999995,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",389,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,5.376,3833.8879999999954,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",390,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.52,3837.4079999999954,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",391,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.4,3843.8079999999954,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",392,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,18.56,3862.3679999999954,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",393,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.36,3865.7279999999955,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",394,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.552,3869.2799999999957,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",395,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.52,3872.7999999999956,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",396,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.584,3876.3839999999955,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",397,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.36,3879.7439999999956,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",398,298270.0,731708.0,24576.0,0,0.0,756284.0,756284.0,0.0,768.0,0.0,196608.0,196608.0,3.456,3883.1999999999957,49152.0,110592.0,285982.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",399,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.36,3886.559999999996,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",400,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.424,3889.983999999996,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),401,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,617162.0,0.46908146815066903,14760352.0,800128.0,91.2,3981.1839999999956,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,461261.0,25004.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",402,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.456,3984.639999999996,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",403,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.272,3990.9119999999957,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",404,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.656,4009.5679999999957,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",405,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,6.24,4015.8079999999954,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",406,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,6.208,4022.0159999999955,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",407,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.512,4026.5279999999957,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",408,393216.0,13074432.0,0.0,0,115964116992.0,13074432.0,115977191424.0,102144.0,192.0,0.99812382739212,245760.0,49152.0,15.488,4042.0159999999955,9891840.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,7680.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),409,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.464,4052.4799999999955,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",410,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,4.992,4057.4719999999957,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",411,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.456,4060.927999999996,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",412,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.432,4067.3599999999956,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",413,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,18.688,4086.0479999999957,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",414,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.488,4089.5359999999955,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",415,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.328,4092.8639999999955,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",416,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.424,4096.287999999996,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",417,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.488,4099.775999999996,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",418,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.264,4103.039999999996,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",419,297881.0,730930.0,24576.0,0,0.0,755506.0,755506.0,0.0,768.0,0.0,196608.0,196608.0,3.488,4106.527999999997,49152.0,110592.0,285593.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",420,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.488,4110.015999999997,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",421,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.552,4113.567999999997,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),422,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,620733.0,0.46764487188393267,14772320.0,828928.0,90.976,4204.543999999996,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,461635.0,25904.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",423,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.392,4207.935999999996,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",424,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.144,4214.079999999996,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",425,28495872.0,57507840.0,368640.0,0,0.0,57876480.0,57876480.0,258048.0,237312.0,0.5209302325581395,30105600.0,147456.0,18.304,4232.383999999996,442368.0,442368.0,28311552.0,184320.0,0,0,0,0,0,0,0,0.0,0.0,0.0,940800.0,4608.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",426,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,6.272,4238.655999999996,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::<unnamed>::CatArrayBatchedCopy<native::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",427,49152.0,0.0,98304.0,0,0.0,98304.0,98304.0,0.0,1536.0,0.0,98304.0,98304.0,5.728,4244.383999999996,0.0,0.0,0.0,49152.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,3072.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",428,12288.0,0.0,24576.0,0,0.0,24576.0,24576.0,0.0,768.0,0.0,49152.0,49152.0,4.352,4248.735999999996,0.0,0.0,0.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1536.0,1536.0
"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, arch::Sm80, 1, 64, 64, 64, 1, 1>::Params)",429,393216.0,13074432.0,0.0,0,115964116992.0,13074432.0,115977191424.0,102144.0,192.0,0.99812382739212,245760.0,49152.0,15.68,4264.4159999999965,9891840.0,2396160.0,393216.0,0.0,0,0,0,0,0,0,0,0.0,0.0,452984832.0,7680.0,1536.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),430,37748736.0,76087296.0,0.0,0,0.0,76087296.0,76087296.0,149184.0,4608.0,0.9700374531835206,2949120.0,589824.0,10.56,4274.975999999997,0.0,589824.0,37748736.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,92160.0,18432.0
"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T10 *, T9 *, T5 *, const T6 *, const T6 *, const T11 *, const T4 *, T11 *, void *, long, T6 *, int *, T6 *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *, const T6 *)",431,24576.0,172032.0,49152.0,0,0.0,221184.0,221184.0,0.0,6144.0,0.0,592896.0,49152.0,5.344,4280.319999999997,159744.0,12288.0,0.0,24576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,18528.0,1536.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",432,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.328,4283.647999999997,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",433,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.464,4290.111999999997,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
"void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, 1, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<T9, T10, T11, T1>)",434,37994496.0,76677120.0,491520.0,0,0.0,77168640.0,77168640.0,344064.0,316416.0,0.5209302325581395,40157184.0,196608.0,18.784,4308.895999999997,589824.0,589824.0,37748736.0,245760.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1254912.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",435,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.456,4312.351999999997,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, void native::<unnamed>::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase &, T2)::[lambda(float) (instance 2)], std::array<char *, 2>>(int, T2, T3)",436,0.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.648,4315.999999999997,0.0,98304.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",437,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.328,4319.327999999998,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",438,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,1152.0,0.0,393216.0,196608.0,3.52,4322.847999999998,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",439,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,768.0,0.0,196608.0,196608.0,3.424,4326.271999999998,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::tanh_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",440,297859.0,730886.0,24576.0,0,0.0,755462.0,755462.0,0.0,768.0,0.0,196608.0,196608.0,3.424,4329.695999999998,49152.0,110592.0,285571.0,12288.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctorOnSelf_add<float>, std::array<char *, 2>>(int, T2, T3)",441,49152.0,98304.0,0.0,0,0.0,98304.0,98304.0,0.0,768.0,0.0,196608.0,196608.0,3.392,4333.087999999998,0.0,0.0,49152.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,6144.0,6144.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",442,0.0,49152.0,0.0,0,0.0,49152.0,49152.0,0.0,1152.0,0.0,393216.0,196608.0,3.488,4336.575999999998,0.0,49152.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12288.0,6144.0
void cutlass::Kernel2<cutlass_80_simt_sgemm_64x64_8x5_nn_align1>(T1::Params),443,152567808.0,306708480.0,0.0,0,0.0,306708480.0,306708480.0,545280.0,618010.0,0.468739523248717,14777248.0,820864.0,90.016,4426.591999999998,0.0,1572864.0,152567808.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,461789.0,25652.0
"void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3)",444,12288.0,24576.0,0.0,0,0.0,24576.0,24576.0,0.0,288.0,0.0,98304.0,49152.0,3.392,4429.983999999998,0.0,0.0,12288.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,3072.0,1536.0
"void native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *)",445,121104.0,373712.0,36864.0,0,0.0,410576.0,410576.0,320.0,1088.0,0.22727272727272727,147456.0,50176.0,6.304,4436.287999999998,104640.0,63728.0,102672.0,18432.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4608.0,1568.0
sm80_xmma_gemm_f32f32_f32f32_f32_tn_n_tilesize32x128x8_stage3_warpsize1x2x1_ffma_aligna4_alignc4_execute_kernel__5x_cublas,446,1237931136.0,2475761664.0,100608.0,0,0.0,2475862272.0,2475862272.0,3085050.0,25136.0,0.9919181682381697,165528256.0,2598240.0,104.096,4540.383999999998,0.0,0.0,1237880832.0,50304.0,0,0,0,0,0,0,0,0.0,0.0,0.0,5172758.0,81195.0
"void native::vectorized_elementwise_kernel<2, native::FillFunctor<long>, std::array<char *, 1>>(int, T2, T3)",447,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,128.0,2.784,4543.167999999998,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,4.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",448,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,6.0,0.0,384.0,640.0,3.808,4546.975999999998,0.0,0.0,0.0,256.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12.0,20.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3)",449,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.264,4550.239999999998,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",450,128.0,804864.0,256.0,0,0.0,805120.0,805120.0,0.0,12578.0,0.0,3216448.0,3216448.0,5.216,4555.455999999998,0.0,804864.0,0.0,128.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100514.0,100514.0
"void mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)",451,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,64.0,3.136,4558.591999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,2.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",452,0.0,0.0,0.0,0,0.0,0.0,0.0,12800.0,37936.0,0.2522863450015768,3247296.0,231744.0,6.88,4565.471999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,101478.0,7242.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",453,384000.0,0.0,768000.0,0,0.0,768000.0,768000.0,52800.0,330432.0,0.1377755511022044,20522624.0,128.0,9.76,4575.231999999999,0.0,0.0,0.0,384000.0,0,0,0,0,0,0,0,0.0,0.0,0.0,641332.0,4.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",454,0.0,0.0,0.0,0,0.0,0.0,0.0,12800.0,37936.0,0.2522863450015768,3247296.0,231680.0,6.688,4581.919999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,101478.0,7240.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",455,230400.0,0.0,460800.0,0,0.0,460800.0,460800.0,52800.0,335232.0,0.13607125185551708,20522880.0,224.0,9.824,4591.743999999999,0.0,0.0,0.0,230400.0,0,0,0,0,0,0,0,0.0,0.0,0.0,641340.0,7.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",456,0.0,0.0,0.0,0,0.0,0.0,0.0,12800.0,37936.0,0.2522863450015768,3247296.0,232384.0,6.528,4598.271999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,101478.0,7262.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",457,313600.0,0.0,627200.0,0,0.0,627200.0,627200.0,52800.0,332632.0,0.13698914464808318,20522784.0,224.0,9.792,4608.063999999999,0.0,0.0,0.0,313600.0,0,0,0,0,0,0,0,0.0,0.0,0.0,641337.0,7.0
"void mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, unsigned int, unsigned int *, unsigned int, T2, int, int, unsigned int, T3, T3 *, short *)",458,0.0,0.0,0.0,0,0.0,0.0,0.0,12800.0,37936.0,0.2522863450015768,3247296.0,232000.0,6.496,4614.5599999999995,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,101478.0,7250.0
"void mbtopk::computeBlockwiseWithinKCounts<unsigned int, float>(T1 *, short *, unsigned int *, unsigned int, int, bool, unsigned int *, T2 *, unsigned int *, T1 *, unsigned int)",459,312000.0,0.0,624000.0,0,0.0,624000.0,624000.0,52800.0,332682.0,0.13697137609538187,20522624.0,512.0,9.664,4624.223999999999,0.0,0.0,0.0,312000.0,0,0,0,0,0,0,0,0.0,0.0,0.0,641332.0,16.0
"void mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsigned int, unsigned int *)",460,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,75.0,0.0,25664.0,3200.0,5.376,4629.599999999999,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,802.0,100.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",461,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,2.944,4632.544,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",462,0.0,0.0,0.0,0,0.0,0.0,0.0,640.0,59.0,0.9155937052932761,3200.0,0.0,5.728,4638.272,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100.0,0.0
"void at_cuda_detail::DeviceScanByKeyInitKernel<at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>>(T1, T2, std::iterator_traits<T2>::value_type *, unsigned int, int)",463,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,544.0,3.136,4641.408,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,17.0
"void at_cuda_detail::DeviceScanByKeyKernel<at_cuda_detail::DeviceScanByKeyPolicy<at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int, unsigned int, __4::plus<void>>::Policy900, at_cuda_detail::TransformInputIterator<unsigned int, mbtopk::BlockIdxToKey, at_cuda_detail::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int *, unsigned int *, at_cuda_detail::ReduceByKeyScanTileState<unsigned int, int, 1>, __4::equal_to<void>, __4::plus<void>, at_cuda_detail::NullType, int, unsigned int, unsigned int>(T2, T11 *, T3, T4, T5, int, T6, T7, T8, T9)",464,0.0,0.0,0.0,0,0.0,0.0,0.0,640.0,59.0,0.9155937052932761,3200.0,0.0,5.984,4647.392000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100.0,0.0
"void mbtopk::gatherTopK<float, unsigned int, 2>(cuda::TensorInfo<const T1, T2>, T2, T2, bool, unsigned int, T2, cuda::TensorInfo<T1, T2>, T2, cuda::TensorInfo<long, T2>, T2, unsigned int, unsigned int, T1 *, unsigned int *, unsigned int *, unsigned int)",465,204800.0,0.0,409600.0,0,0.0,409600.0,409600.0,205328.0,52062.0,0.7977310695831229,3321024.0,34688.0,10.56,4657.952000000001,0.0,0.0,0.0,204800.0,0,0,0,0,0,0,0,0.0,0.0,0.0,103782.0,1084.0
"void native::radixSortKVInPlace<2, (int)-1, 32, 4, float, long, unsigned int>(cuda::TensorInfo<T5, T7>, T7, T7, T7, cuda::TensorInfo<T6, T7>, T7, bool)",466,0.0,0.0,0.0,0,0.0,0.0,0.0,7328.0,128.0,0.9828326180257511,10240.0,0.0,8.64,4666.5920000000015,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,320.0,0.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::<unnamed>::CompareFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",467,1608224.0,0.0,3216448.0,0,0.0,3216448.0,3216448.0,0.0,75387.0,0.0,3266912.0,251808.0,6.624,4673.216000000001,0.0,0.0,0.0,1608224.0,0,0,0,0,0,0,0,0.0,0.0,0.0,102091.0,7869.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",468,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,18867.0,0.0,4020576.0,0.0,5.12,4678.336000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,125643.0,0.0
"native::<unnamed>::fill_index_and_segment_kernel(int2 *, int, cuda::IntDivider<unsigned int>)",469,804112.0,0.0,1608224.0,0,0.0,1608224.0,1608224.0,0.0,25129.0,0.0,0.0,6432896.0,6.208,4684.544000000001,0.0,0.0,0.0,804112.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,201028.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",470,0.0,0.0,0.0,0,0.0,0.0,0.0,135168.0,25129.0,0.8432347455036588,3216448.0,0.0,7.392,4691.936000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100514.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, unsigned long long>(T2 *)",471,0.0,0.0,0.0,0,0.0,0.0,0.0,128.0,64.0,0.6666666666666666,8192.0,0.0,3.52,4695.456000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,256.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",472,0.0,0.0,0.0,0,0.0,0.0,0.0,259746.0,118817.0,0.6861367856869266,11687360.0,7978048.0,24.256,4719.712000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,365230.0,249314.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",473,0.0,0.0,0.0,0,0.0,0.0,0.0,57570.0,125219.0,0.31495330681824396,11657920.0,6633696.0,18.08,4737.792000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,364310.0,207303.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",474,0.0,0.0,0.0,0,0.0,0.0,0.0,53145.0,126457.0,0.2959042772352201,11632192.0,9802944.0,20.224,4758.016000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,363506.0,306342.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<float, cuda::OpaqueType<8>, unsigned long long>::Policy900, 0, float, cuda::OpaqueType<8>, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",475,0.0,0.0,0.0,0,0.0,0.0,0.0,53145.0,127611.0,0.29401513642700655,11662784.0,9801536.0,20.416,4778.432000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,364462.0,306298.0
"void at_cuda_detail::DeviceRadixSortHistogramKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, unsigned long long, at_cuda_detail::identity_decomposer_t>(T4 *, const T3 *, T4, int, int, T5)",476,0.0,0.0,0.0,0,0.0,0.0,0.0,14784.0,25129.0,0.37040563225014406,6432896.0,0.0,6.656,4785.088000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,201028.0,0.0
"void at_cuda_detail::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, unsigned long long>(T2 *)",477,0.0,0.0,0.0,0,0.0,0.0,0.0,32.0,16.0,0.6666666666666666,2048.0,0.0,3.36,4788.448000000001,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,64.0,0.0
"void at_cuda_detail::DeviceRadixSortOnesweepKernel<at_cuda_detail::DeviceRadixSortPolicy<long, at_cuda_detail::NullType, unsigned long long>::Policy900, 0, long, at_cuda_detail::NullType, unsigned long long, int, int, at_cuda_detail::identity_decomposer_t>(T7 *, T7 *, T5 *, const T5 *, T3 *, const T3 *, T4 *, const T4 *, T6, int, int, T8)",478,0.0,0.0,0.0,0,0.0,0.0,0.0,59326.0,70063.0,0.4585088376909938,8369664.0,5557728.0,14.368,4802.816000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,261552.0,173679.0
"void native::<unnamed>::sort_postprocess_kernel<float>(const T1 *, T1 *, long *, const int2 *, int, int)",479,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,100516.0,0.0,9719296.0,9649344.0,9.728,4812.544000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,303728.0,301542.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",480,12488160.0,26620176.0,2461184.0,0,0.0,29081360.0,29081360.0,2112.0,26816.0,0.07300884955752213,4443648.0,2914240.0,32.224,4844.768000000002,3300928.0,804112.0,11257568.0,1230592.0,0,0,0,0,0,0,0,0.0,0.0,0.0,138864.0,91070.0
"void native::tensor_kernel_scan_innermost_dim<float, std::plus<float>>(T1 *, const T1 *, unsigned int, unsigned int, unsigned int, T1, T2)",481,0.0,4096800.0,0.0,0,0.0,4096800.0,4096800.0,449136.0,50272.0,0.8993368147887099,3217376.0,2248480.0,98.432,4943.200000000002,4096800.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100543.0,70265.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",482,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,12578.0,0.0,3216448.0,803968.0,4.576,4947.776000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100514.0,25124.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::FillFunctor<bool>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",483,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,1.0,0.0,0.0,512.0,3.072,4950.848000000002,0.0,0.0,0.0,16.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,16.0
"void native::_scatter_gather_elementwise_kernel<128, 8, void native::_cuda_scatter_gather_internal_kernel<1, native::OpaqueType<1>>::operator ()<native::TensorAssign>(at::TensorIterator &, long, long, long, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",484,1608224.0,0.0,3216448.0,0,0.0,3216448.0,3216448.0,0.0,75387.0,0.0,7237024.0,400128.0,11.872,4962.720000000002,0.0,0.0,0.0,1608224.0,0,0,0,0,0,0,0,0.0,0.0,0.0,226157.0,12504.0
"void native::vectorized_elementwise_kernel<4, native::<unnamed>::masked_fill_kernel(at::TensorIterator &, const c10::Scalar &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float, bool) (instance 1)], std::array<char *, 3>>(int, T2, T3)",485,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,18867.0,0.0,4020576.0,0.0,5.952,4968.672000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,125643.0,0.0
"void native::<unnamed>::cunn_SoftMaxForward<4, float, float, float, native::<unnamed>::SoftMaxForwardEpilogue>(T4 *, const T2 *, int)",486,12488207.0,26620176.0,2461278.0,0,0.0,29081454.0,29081454.0,2112.0,26816.0,0.07300884955752213,4485664.0,2917632.0,32.288,5000.960000000002,3300928.0,804112.0,11257568.0,1230639.0,0,0,0,0,0,0,0,0.0,0.0,0.0,140177.0,91176.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MinNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",487,152576.0,0.0,305152.0,0,0.0,305152.0,305152.0,7883.0,6387.0,0.5524176594253679,3216864.0,3200.0,9.984,5010.944000000002,0.0,0.0,0.0,152576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100527.0,100.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",488,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.328,5014.272000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::MaxNanFunctor<float>>, unsigned int, float, 4, 4>>(T3)",489,152576.0,0.0,305152.0,0,0.0,305152.0,305152.0,7883.0,6387.0,0.5524176594253679,3216864.0,3200.0,9.696,5023.968000000003,0.0,0.0,0.0,152576.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100527.0,100.0
"void native::vectorized_elementwise_kernel<4, void native::compare_scalar_kernel<float>(at::TensorIteratorBase &, native::<unnamed>::OpType, T1)::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3)",490,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.456,5027.424000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseAndFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",491,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.424,5030.848000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",492,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,4.576,5035.424000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::func_wrapper_t<float, native::sum_functor<float, float, float>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, float, 4, 4>>(T3)",493,16384.0,877328.0,32768.0,0,0.0,910096.0,910096.0,736.0,6328.0,0.10419026047565119,3216896.0,512.0,16.768,5052.192000000003,877328.0,0.0,0.0,16384.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100528.0,16.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<float, float, bool, native::<unnamed>::CompareEqFunctor<float>>, std::array<char *, 2>>(int, T2, T3)",494,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,64.0,32.0,3.552,5055.744000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::reduce_kernel<512, 1, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",495,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,2.0,0.0,32.0,32.0,5.312,5061.056000000002,0.0,0.0,0.0,16.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",496,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.264,5064.320000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::_assert_async_cuda_kernel<bool>(const T1 *, native::Msg)",497,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,32.0,0.0,4.608,5068.928000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,0.0
"void native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void native::<unnamed>::distribution_nullary_kernel<float, float, float4, at::CUDAGeneratorImpl *, void templates::uniform_and_transform<float, float, at::CUDAGeneratorImpl *, void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::exponential_kernel<at::CUDAGeneratorImpl *>(at::TensorIteratorBase &, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 2)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4)",498,2973696.0,6474272.0,1081344.0,0,0.0,7555616.0,7555616.0,0.0,25129.0,0.0,0.0,3216448.0,7.232,5076.160000000003,0.0,1608224.0,2433024.0,540672.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,100514.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<float, float, float, binary_internal::DivFunctor<float>>, std::array<char *, 3>>(int, T2, T3)",499,5025135.0,8041120.0,2009150.0,0,0.0,10050270.0,10050270.0,0.0,18867.0,0.0,6432896.0,0.0,7.264,5083.424000000003,0.0,0.0,4020560.0,1004575.0,0,0,0,0,0,0,0,0.0,0.0,0.0,201028.0,0.0
"void native::reduce_kernel<512, 1, native::ReduceOp<float, native::ArgMaxOps<float>, unsigned int, long, 4, 4>>(T3)",500,8704.0,0.0,17408.0,0,0.0,17408.0,17408.0,1472.0,6328.0,0.18871794871794872,3216896.0,512.0,22.496,5105.920000000003,0.0,0.0,0.0,8704.0,0,0,0,0,0,0,0,0.0,0.0,0.0,100528.0,16.0
"void native::vectorized_elementwise_kernel<2, native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3)",501,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,256.0,128.0,3.264,5109.184000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,8.0,4.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctorOnOther_add<long>, std::array<char *, 2>>(int, T2, T3)",502,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,128.0,128.0,3.36,5112.544000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4.0,4.0
"void native::elementwise_kernel<128, 2, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",503,16.0,0.0,32.0,0,0.0,32.0,32.0,0.0,3.0,0.0,160.0,128.0,4.064,5116.608000000003,0.0,0.0,0.0,16.0,0,0,0,0,0,0,0,0.0,0.0,0.0,5.0,4.0
"void native::vectorized_elementwise_kernel<2, native::CUDAFunctor_add<long>, std::array<char *, 3>>(int, T2, T3)",504,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,256.0,128.0,3.52,5120.128000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,8.0,4.0
"void native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<native::<unnamed>::OpaqueType<8>, unsigned int, 2, 128, 1>(T1 *, native::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, native::<unnamed>::TensorSizeStride<T2, 4>, int, T2)",505,256.0,0.0,512.0,0,0.0,512.0,512.0,0.0,6.0,0.0,384.0,640.0,3.936,5124.064000000003,0.0,0.0,0.0,256.0,0,0,0,0,0,0,0,0.0,0.0,0.0,12.0,20.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",506,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,3.168,5127.232000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",507,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.784,5130.016000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",508,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.392,5133.408000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::FillFunctor<bool>, std::array<char *, 1>>(int, T2, T3)",509,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,32.0,2.72,5136.128000000002,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,0.0,1.0
"void native::elementwise_kernel<128, 4, void native::gpu_kernel_impl_nocast<native::BinaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)",510,32.0,0.0,64.0,0,0.0,64.0,64.0,0.0,3.0,0.0,416.0,32.0,3.808,5139.936000000002,0.0,0.0,0.0,32.0,0,0,0,0,0,0,0,0.0,0.0,0.0,13.0,1.0
"void native::reduce_kernel<128, 4, native::ReduceOp<bool, native::func_wrapper_t<bool, native::or_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3)",511,8.0,0.0,16.0,0,0.0,16.0,16.0,0.0,5.0,0.0,32.0,32.0,7.52,5147.456000000003,0.0,0.0,0.0,8.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::vectorized_elementwise_kernel<4, native::BinaryFunctor<bool, bool, bool, native::BitwiseOrFunctor<bool>>, std::array<char *, 3>>(int, T2, T3)",512,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,64.0,32.0,3.424,5150.880000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,2.0,1.0
"void native::vectorized_elementwise_kernel<4, native::bitwise_not_kernel_cuda(at::TensorIteratorBase &)::[lambda(bool) (instance 1)], std::array<char *, 2>>(int, T2, T3)",513,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.328,5154.208000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
"void native::unrolled_elementwise_kernel<native::BinaryFunctor<long, long, long, native::BitwiseAndFunctor<long>>, std::array<char *, 3>, 8, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>(int, T1, T2, T4, T5, T6, T7)",514,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,3.0,0.0,160.0,128.0,4.48,5158.688000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,5.0,4.0
"void native::reduce_kernel<512, 1, native::ReduceOp<long, native::func_wrapper_t<long, native::MaxNanFunctor<long>>, unsigned int, long, 4, 4>>(T3)",515,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,128.0,32.0,4.928,5163.616000000003,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,4.0,1.0
"void native::vectorized_elementwise_kernel<4, native::AUnaryFunctor<long, long, bool, native::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3)",516,0.0,0.0,0.0,0,0.0,0.0,0.0,0.0,2.0,0.0,32.0,32.0,3.2,5166.8160000000025,0.0,0.0,0.0,0.0,0,0,0,0,0,0,0,0.0,0.0,0.0,1.0,1.0
